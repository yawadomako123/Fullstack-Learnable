## √úberpr√ºfung der Strategie

Da die Q-Tabelle die "Attraktivit√§t" jeder Aktion in jedem Zustand auflistet, ist es ziemlich einfach, sie zu verwenden, um die effiziente Navigation in unserer Welt zu definieren. Im einfachsten Fall k√∂nnen wir die Aktion ausw√§hlen, die dem h√∂chsten Q-Tabelle-Wert entspricht: (Codeblock 9)

```python
def qpolicy_strict(m):
        x,y = m.human
        v = probs(Q[x,y])
        a = list(actions)[np.argmax(v)]
        return a

walk(m,qpolicy_strict)
```

> Wenn Sie den obigen Code mehrmals ausprobieren, k√∂nnten Sie feststellen, dass er manchmal "h√§ngt" und Sie die STOP-Taste im Notizbuch dr√ºcken m√ºssen, um ihn zu unterbrechen. Dies geschieht, weil es Situationen geben k√∂nnte, in denen zwei Zust√§nde in Bezug auf den optimalen Q-Wert "aufeinander zeigen", in diesem Fall bewegt sich der Agent unendlich zwischen diesen Zust√§nden hin und her.

## üöÄHerausforderung

> **Aufgabe 1:** √Ñndern Sie die `walk` function to limit the maximum length of path by a certain number of steps (say, 100), and watch the code above return this value from time to time.

> **Task 2:** Modify the `walk` function so that it does not go back to the places where it has already been previously. This will prevent `walk` from looping, however, the agent can still end up being "trapped" in a location from which it is unable to escape.

## Navigation

A better navigation policy would be the one that we used during training, which combines exploitation and exploration. In this policy, we will select each action with a certain probability, proportional to the values in the Q-Table. This strategy may still result in the agent returning back to a position it has already explored, but, as you can see from the code below, it results in a very short average path to the desired location (remember that `print_statistics`, die die Simulation 100 Mal ausf√ºhrt: (Codeblock 10)

```python
def qpolicy(m):
        x,y = m.human
        v = probs(Q[x,y])
        a = random.choices(list(actions),weights=v)[0]
        return a

print_statistics(qpolicy)
```

Nach dem Ausf√ºhren dieses Codes sollten Sie eine viel k√ºrzere durchschnittliche Pfadl√§nge als zuvor erhalten, im Bereich von 3-6.

## Untersuchung des Lernprozesses

Wie bereits erw√§hnt, ist der Lernprozess ein Gleichgewicht zwischen Exploration und der Erkundung des erlangten Wissens √ºber die Struktur des Problembereichs. Wir haben gesehen, dass sich die Ergebnisse des Lernens (die F√§higkeit, einem Agenten zu helfen, einen kurzen Weg zum Ziel zu finden) verbessert haben, aber es ist auch interessant zu beobachten, wie sich die durchschnittliche Pfadl√§nge w√§hrend des Lernprozesses verh√§lt:

Die Erkenntnisse k√∂nnen zusammengefasst werden als:

- **Durchschnittliche Pfadl√§nge steigt**. Was wir hier sehen, ist, dass die durchschnittliche Pfadl√§nge zun√§chst zunimmt. Dies liegt wahrscheinlich daran, dass wir, wenn wir nichts √ºber die Umgebung wissen, wahrscheinlich in schlechten Zust√§nden, Wasser oder dem Wolf, gefangen werden. W√§hrend wir mehr lernen und dieses Wissen nutzen, k√∂nnen wir die Umgebung l√§nger erkunden, aber wir wissen immer noch nicht genau, wo die √Ñpfel sind.

- **Pfadl√§nge verringert sich, w√§hrend wir mehr lernen**. Sobald wir genug gelernt haben, wird es f√ºr den Agenten einfacher, das Ziel zu erreichen, und die Pfadl√§nge beginnt zu sinken. Wir sind jedoch weiterhin offen f√ºr Erkundungen, sodass wir oft vom besten Pfad abweichen und neue Optionen erkunden, was den Pfad l√§nger macht als optimal.

- **L√§ngensteigerung abrupt**. Was wir auch in diesem Diagramm beobachten, ist, dass die L√§nge an einem Punkt abrupt anstieg. Dies zeigt die stochastische Natur des Prozesses an und dass wir zu einem bestimmten Zeitpunkt die Q-Tabellen-Koeffizienten "verderben" k√∂nnen, indem wir sie mit neuen Werten √ºberschreiben. Dies sollte idealerweise minimiert werden, indem die Lernrate verringert wird (zum Beispiel passen wir gegen Ende des Trainings die Q-Tabellen-Werte nur um einen kleinen Wert an).

Insgesamt ist es wichtig, sich daran zu erinnern, dass der Erfolg und die Qualit√§t des Lernprozesses erheblich von Parametern wie Lernrate, Lernratenverringerung und Abzinsungsfaktor abh√§ngen. Diese werden oft als **Hyperparameter** bezeichnet, um sie von **Parametern** zu unterscheiden, die wir w√§hrend des Trainings optimieren (zum Beispiel die Q-Tabellen-Koeffizienten). Der Prozess, die besten Hyperparameterwerte zu finden, wird als **Hyperparameter-Optimierung** bezeichnet und verdient ein eigenes Thema.

## [Nachlese-Quiz](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/46/)

## Aufgabe 
[Eine realistischere Welt](assignment.md)

**Haftungsausschluss**:  
Dieses Dokument wurde mithilfe von KI-√úbersetzungsdiensten √ºbersetzt. Obwohl wir uns um Genauigkeit bem√ºhen, beachten Sie bitte, dass automatisierte √úbersetzungen Fehler oder Ungenauigkeiten enthalten k√∂nnen. Das Originaldokument in seiner Ausgangssprache sollte als die ma√ügebliche Quelle betrachtet werden. F√ºr wichtige Informationen wird eine professionelle menschliche √úbersetzung empfohlen. Wir √ºbernehmen keine Haftung f√ºr Missverst√§ndnisse oder Fehlinterpretationen, die aus der Verwendung dieser √úbersetzung entstehen.