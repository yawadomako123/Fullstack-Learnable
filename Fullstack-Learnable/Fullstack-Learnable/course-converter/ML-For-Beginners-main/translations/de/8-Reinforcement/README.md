# Einf√ºhrung in das Reinforcement Learning

Reinforcement Learning (RL) wird als eines der grundlegenden Paradigmen des maschinellen Lernens angesehen, neben dem √ºberwachten und un√ºberwachten Lernen. RL dreht sich um Entscheidungen: die richtigen Entscheidungen zu treffen oder zumindest aus ihnen zu lernen.

Stellen Sie sich vor, Sie haben eine simulierte Umgebung wie den Aktienmarkt. Was passiert, wenn Sie eine bestimmte Regelung auferlegen? Hat sie einen positiven oder negativen Effekt? Wenn etwas Negatives passiert, m√ºssen Sie diese _negative Verst√§rkung_ annehmen, daraus lernen und den Kurs √§ndern. Wenn das Ergebnis positiv ist, sollten Sie auf dieser _positiven Verst√§rkung_ aufbauen.

![peter und der wolf](../../../translated_images/peter.779730f9ba3a8a8d9290600dcf55f2e491c0640c785af7ac0d64f583c49b8864.de.png)

> Peter und seine Freunde m√ºssen dem hungrigen Wolf entkommen! Bild von [Jen Looper](https://twitter.com/jenlooper)

## Regionales Thema: Peter und der Wolf (Russland)

[Peter und der Wolf](https://en.wikipedia.org/wiki/Peter_and_the_Wolf) ist ein musikalisches M√§rchen, das von dem russischen Komponisten [Sergei Prokofiev](https://en.wikipedia.org/wiki/Sergei_Prokofiev) geschrieben wurde. Es ist die Geschichte des jungen Pioniers Peter, der mutig aus seinem Haus auf die Lichtung im Wald geht, um den Wolf zu jagen. In diesem Abschnitt werden wir Algorithmen des maschinellen Lernens trainieren, die Peter helfen werden:

- **Die Umgebung** zu erkunden und eine optimale Navigationskarte zu erstellen.
- **Zu lernen**, wie man ein Skateboard benutzt und darauf balanciert, um schneller voranzukommen.

[![Peter und der Wolf](https://img.youtube.com/vi/Fmi5zHg4QSM/0.jpg)](https://www.youtube.com/watch?v=Fmi5zHg4QSM)

> üé• Klicken Sie auf das Bild oben, um Peter und den Wolf von Prokofiev zu h√∂ren.

## Reinforcement Learning

In den vorherigen Abschnitten haben Sie zwei Beispiele f√ºr Probleme des maschinellen Lernens gesehen:

- **√úberwachtes Lernen**, bei dem wir Datens√§tze haben, die Beispiel-L√∂sungen f√ºr das Problem vorschlagen, das wir l√∂sen m√∂chten. [Klassifikation](../4-Classification/README.md) und [Regression](../2-Regression/README.md) sind Aufgaben des √ºberwachten Lernens.
- **Un√ºberwachtes Lernen**, bei dem wir keine beschrifteten Trainingsdaten haben. Das Hauptbeispiel f√ºr un√ºberwachtes Lernen ist [Clustering](../5-Clustering/README.md).

In diesem Abschnitt werden wir Ihnen eine neue Art von Lernproblem vorstellen, das keine beschrifteten Trainingsdaten ben√∂tigt. Es gibt mehrere Arten solcher Probleme:

- **[Semi-√ºberwachtes Lernen](https://wikipedia.org/wiki/Semi-supervised_learning)**, bei dem wir eine gro√üe Menge an unbeschrifteten Daten haben, die verwendet werden k√∂nnen, um das Modell vorzutrainieren.
- **[Reinforcement Learning](https://wikipedia.org/wiki/Reinforcement_learning)**, bei dem ein Agent lernt, wie er sich verhalten soll, indem er Experimente in einer simulierten Umgebung durchf√ºhrt.

### Beispiel - Computerspiel

Angenommen, Sie m√∂chten einem Computer beibringen, ein Spiel zu spielen, wie Schach oder [Super Mario](https://wikipedia.org/wiki/Super_Mario). Damit der Computer ein Spiel spielen kann, muss er vorhersagen, welchen Zug er in jedem der Spielzust√§nde machen soll. Auch wenn dies wie ein Klassifikationsproblem erscheinen mag, ist es das nicht - weil wir keinen Datensatz mit Zust√§nden und entsprechenden Aktionen haben. Auch wenn wir einige Daten wie bestehende Schachpartien oder Aufzeichnungen von Spielern, die Super Mario spielen, haben, ist es wahrscheinlich, dass diese Daten nicht ausreichend eine gro√üe Anzahl m√∂glicher Zust√§nde abdecken.

Anstatt nach vorhandenen Spieldaten zu suchen, basiert **Reinforcement Learning** (RL) auf der Idee, *den Computer viele Male spielen zu lassen* und das Ergebnis zu beobachten. Um Reinforcement Learning anzuwenden, ben√∂tigen wir daher zwei Dinge:

- **Eine Umgebung** und **einen Simulator**, die es uns erm√∂glichen, ein Spiel viele Male zu spielen. Dieser Simulator w√ºrde alle Spielregeln sowie m√∂gliche Zust√§nde und Aktionen definieren.

- **Eine Belohnungsfunktion**, die uns sagt, wie gut wir w√§hrend jedes Zuges oder Spiels abgeschnitten haben.

Der Hauptunterschied zwischen anderen Arten des maschinellen Lernens und RL besteht darin, dass wir im RL typischerweise nicht wissen, ob wir gewinnen oder verlieren, bis wir das Spiel beendet haben. Daher k√∂nnen wir nicht sagen, ob ein bestimmter Zug allein gut oder schlecht ist - wir erhalten erst am Ende des Spiels eine Belohnung. Unser Ziel ist es, Algorithmen zu entwerfen, die es uns erm√∂glichen, ein Modell unter unsicheren Bedingungen zu trainieren. Wir werden √ºber einen RL-Algorithmus namens **Q-Learning** lernen.

## Lektionen

1. [Einf√ºhrung in Reinforcement Learning und Q-Learning](1-QLearning/README.md)
2. [Verwendung einer Gym-Simulationsumgebung](2-Gym/README.md)

## Danksagungen

"Einf√ºhrung in Reinforcement Learning" wurde mit ‚ô•Ô∏è von [Dmitry Soshnikov](http://soshnikov.com) geschrieben.

**Haftungsausschluss**:  
Dieses Dokument wurde mit maschinellen KI-√úbersetzungsdiensten √ºbersetzt. Obwohl wir uns um Genauigkeit bem√ºhen, sollten Sie sich bewusst sein, dass automatisierte √úbersetzungen Fehler oder Ungenauigkeiten enthalten k√∂nnen. Das Originaldokument in seiner urspr√ºnglichen Sprache sollte als die ma√ügebliche Quelle betrachtet werden. F√ºr wichtige Informationen wird eine professionelle menschliche √úbersetzung empfohlen. Wir √ºbernehmen keine Haftung f√ºr Missverst√§ndnisse oder Fehlinterpretationen, die aus der Nutzung dieser √úbersetzung entstehen.