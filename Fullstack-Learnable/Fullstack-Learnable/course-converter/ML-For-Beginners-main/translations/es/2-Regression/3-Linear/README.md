# Construye un modelo de regresi√≥n usando Scikit-learn: regresi√≥n de cuatro maneras

![Infograf√≠a de regresi√≥n lineal vs polin√≥mica](../../../../translated_images/linear-polynomial.5523c7cb6576ccab0fecbd0e3505986eb2d191d9378e785f82befcf3a578a6e7.es.png)
> Infograf√≠a por [Dasani Madipalli](https://twitter.com/dasani_decoded)
## [Cuestionario previo a la lecci√≥n](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/13/)

> ### [¬°Esta lecci√≥n est√° disponible en R!](../../../../2-Regression/3-Linear/solution/R/lesson_3.html)
### Introducci√≥n 

Hasta ahora has explorado qu√© es la regresi√≥n con datos de muestra recopilados del conjunto de datos de precios de calabazas que utilizaremos a lo largo de esta lecci√≥n. Tambi√©n lo has visualizado usando Matplotlib.

Ahora est√°s listo para profundizar en la regresi√≥n para ML. Mientras que la visualizaci√≥n te permite entender los datos, el verdadero poder del Aprendizaje Autom√°tico proviene del _entrenamiento de modelos_. Los modelos se entrenan con datos hist√≥ricos para capturar autom√°ticamente las dependencias de los datos, y te permiten predecir resultados para nuevos datos, que el modelo no ha visto antes.

En esta lecci√≥n, aprender√°s m√°s sobre dos tipos de regresi√≥n: _regresi√≥n lineal b√°sica_ y _regresi√≥n polin√≥mica_, junto con algunas de las matem√°ticas subyacentes a estas t√©cnicas. Esos modelos nos permitir√°n predecir los precios de las calabazas dependiendo de diferentes datos de entrada. 

[![ML para principiantes - Entendiendo la Regresi√≥n Lineal](https://img.youtube.com/vi/CRxFT8oTDMg/0.jpg)](https://youtu.be/CRxFT8oTDMg "ML para principiantes - Entendiendo la Regresi√≥n Lineal")

> üé• Haz clic en la imagen de arriba para una breve visi√≥n general de la regresi√≥n lineal.

> A lo largo de este plan de estudios, asumimos un conocimiento m√≠nimo de matem√°ticas, y buscamos hacerlo accesible para estudiantes que vienen de otros campos, as√≠ que estate atento a notas, üßÆ llamadas, diagramas y otras herramientas de aprendizaje para ayudar en la comprensi√≥n.

### Prerrequisitos

Deber√≠as estar familiarizado con la estructura de los datos de calabazas que estamos examinando. Puedes encontrarlo precargado y pre-limpiado en el archivo _notebook.ipynb_ de esta lecci√≥n. En el archivo, el precio de la calabaza se muestra por fanega en un nuevo marco de datos. Aseg√∫rate de poder ejecutar estos cuadernos en kernels en Visual Studio Code.

### Preparaci√≥n

Como recordatorio, est√°s cargando estos datos para hacerles preguntas.

- ¬øCu√°ndo es el mejor momento para comprar calabazas?
- ¬øQu√© precio puedo esperar de una caja de calabazas miniatura?
- ¬øDeber√≠a comprarlas en cestas de media fanega o en cajas de 1 1/9 fanega?
Sigamos profundizando en estos datos.

En la lecci√≥n anterior, creaste un marco de datos de Pandas y lo llenaste con parte del conjunto de datos original, estandarizando los precios por fanega. Sin embargo, al hacerlo, solo pudiste reunir alrededor de 400 puntos de datos y solo para los meses de oto√±o.

Echa un vistazo a los datos que precargamos en el cuaderno acompa√±ante de esta lecci√≥n. Los datos est√°n precargados y se ha graficado un gr√°fico de dispersi√≥n inicial para mostrar los datos por mes. Tal vez podamos obtener un poco m√°s de detalle sobre la naturaleza de los datos limpi√°ndolos m√°s.

## Una l√≠nea de regresi√≥n lineal

Como aprendiste en la Lecci√≥n 1, el objetivo de un ejercicio de regresi√≥n lineal es poder trazar una l√≠nea para:

- **Mostrar relaciones entre variables**. Mostrar la relaci√≥n entre variables.
- **Hacer predicciones**. Hacer predicciones precisas sobre d√≥nde caer√≠a un nuevo punto de datos en relaci√≥n con esa l√≠nea.
 
Es t√≠pico de la **Regresi√≥n de M√≠nimos Cuadrados** dibujar este tipo de l√≠nea. El t√©rmino 'm√≠nimos cuadrados' significa que todos los puntos de datos que rodean la l√≠nea de regresi√≥n se elevan al cuadrado y luego se suman. Idealmente, esa suma final es lo m√°s peque√±a posible, porque queremos un n√∫mero bajo de errores, o `least-squares`. 

Hacemos esto ya que queremos modelar una l√≠nea que tenga la menor distancia acumulada de todos nuestros puntos de datos. Tambi√©n elevamos al cuadrado los t√©rminos antes de sumarlos, ya que nos preocupa su magnitud m√°s que su direcci√≥n.

> **üßÆ Mu√©strame las matem√°ticas** 
> 
> Esta l√≠nea, llamada la _l√≠nea de mejor ajuste_ puede expresarse por [una ecuaci√≥n](https://es.wikipedia.org/wiki/Regresi√≥n_lineal_simple): 
> 
> ```
> Y = a + bX
> ```
>
> `X` is the 'explanatory variable'. `Y` is the 'dependent variable'. The slope of the line is `b` and `a` is the y-intercept, which refers to the value of `Y` when `X = 0`. 
>
>![calculate the slope](../../../../translated_images/slope.f3c9d5910ddbfcf9096eb5564254ba22c9a32d7acd7694cab905d29ad8261db3.es.png)
>
> First, calculate the slope `b`. Infographic by [Jen Looper](https://twitter.com/jenlooper)
>
> In other words, and referring to our pumpkin data's original question: "predict the price of a pumpkin per bushel by month", `X` would refer to the price and `Y` would refer to the month of sale. 
>
>![complete the equation](../../../../translated_images/calculation.a209813050a1ddb141cdc4bc56f3af31e67157ed499e16a2ecf9837542704c94.es.png)
>
> Calculate the value of Y. If you're paying around $4, it must be April! Infographic by [Jen Looper](https://twitter.com/jenlooper)
>
> The math that calculates the line must demonstrate the slope of the line, which is also dependent on the intercept, or where `Y` is situated when `X = 0`.
>
> You can observe the method of calculation for these values on the [Math is Fun](https://www.mathsisfun.com/data/least-squares-regression.html) web site. Also visit [this Least-squares calculator](https://www.mathsisfun.com/data/least-squares-calculator.html) to watch how the numbers' values impact the line.

## Correlation

One more term to understand is the **Correlation Coefficient** between given X and Y variables. Using a scatterplot, you can quickly visualize this coefficient. A plot with datapoints scattered in a neat line have high correlation, but a plot with datapoints scattered everywhere between X and Y have a low correlation.

A good linear regression model will be one that has a high (nearer to 1 than 0) Correlation Coefficient using the Least-Squares Regression method with a line of regression.

‚úÖ Run the notebook accompanying this lesson and look at the Month to Price scatterplot. Does the data associating Month to Price for pumpkin sales seem to have high or low correlation, according to your visual interpretation of the scatterplot? Does that change if you use more fine-grained measure instead of `Month`, eg. *day of the year* (i.e. number of days since the beginning of the year)?

In the code below, we will assume that we have cleaned up the data, and obtained a data frame called `new_pumpkins`, similar to the following:

ID | Month | DayOfYear | Variety | City | Package | Low Price | High Price | Price
---|-------|-----------|---------|------|---------|-----------|------------|-------
70 | 9 | 267 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 15.0 | 15.0 | 13.636364
71 | 9 | 267 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 18.0 | 18.0 | 16.363636
72 | 10 | 274 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 18.0 | 18.0 | 16.363636
73 | 10 | 274 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 17.0 | 17.0 | 15.454545
74 | 10 | 281 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 15.0 | 15.0 | 13.636364

> The code to clean the data is available in [`notebook.ipynb`](../../../../2-Regression/3-Linear/notebook.ipynb). We have performed the same cleaning steps as in the previous lesson, and have calculated `DayOfYear` columna usando la siguiente expresi√≥n: 

```python
day_of_year = pd.to_datetime(pumpkins['Date']).apply(lambda dt: (dt-datetime(dt.year,1,1)).days)
```

Ahora que tienes una comprensi√≥n de las matem√°ticas detr√°s de la regresi√≥n lineal, vamos a crear un modelo de Regresi√≥n para ver si podemos predecir qu√© paquete de calabazas tendr√° los mejores precios de calabaza. Alguien que compra calabazas para un huerto de calabazas festivo podr√≠a querer esta informaci√≥n para poder optimizar sus compras de paquetes de calabazas para el huerto.

## Buscando correlaci√≥n

[![ML para principiantes - Buscando correlaci√≥n: La clave para la regresi√≥n lineal](https://img.youtube.com/vi/uoRq-lW2eQo/0.jpg)](https://youtu.be/uoRq-lW2eQo "ML para principiantes - Buscando correlaci√≥n: La clave para la regresi√≥n lineal")

> üé• Haz clic en la imagen de arriba para una breve visi√≥n general de la correlaci√≥n.

De la lecci√≥n anterior, probablemente hayas visto que el precio promedio para diferentes meses se ve as√≠:

<img alt="Precio promedio por mes" src="../2-Data/images/barchart.png" width="50%"/>

Esto sugiere que deber√≠a haber alguna correlaci√≥n, y podemos intentar entrenar un modelo de regresi√≥n lineal para predecir la relaci√≥n entre `Month` and `Price`, or between `DayOfYear` and `Price`. Here is the scatter plot that shows the latter relationship:

<img alt="Scatter plot of Price vs. Day of Year" src="images/scatter-dayofyear.png" width="50%" /> 

Let's see if there is a correlation using the `corr` funci√≥n:

```python
print(new_pumpkins['Month'].corr(new_pumpkins['Price']))
print(new_pumpkins['DayOfYear'].corr(new_pumpkins['Price']))
```

Parece que la correlaci√≥n es bastante peque√±a, -0.15 por `Month` and -0.17 by the `DayOfMonth`, but there could be another important relationship. It looks like there are different clusters of prices corresponding to different pumpkin varieties. To confirm this hypothesis, let's plot each pumpkin category using a different color. By passing an `ax` parameter to the `scatter` funci√≥n de trazado de dispersi√≥n podemos trazar todos los puntos en el mismo gr√°fico:

```python
ax=None
colors = ['red','blue','green','yellow']
for i,var in enumerate(new_pumpkins['Variety'].unique()):
    df = new_pumpkins[new_pumpkins['Variety']==var]
    ax = df.plot.scatter('DayOfYear','Price',ax=ax,c=colors[i],label=var)
```

<img alt="Gr√°fico de dispersi√≥n de Precio vs. D√≠a del A√±o" src="images/scatter-dayofyear-color.png" width="50%" /> 

Nuestra investigaci√≥n sugiere que la variedad tiene m√°s efecto en el precio general que la fecha de venta real. Podemos ver esto con un gr√°fico de barras:

```python
new_pumpkins.groupby('Variety')['Price'].mean().plot(kind='bar')
```

<img alt="Gr√°fico de barras de precio vs variedad" src="images/price-by-variety.png" width="50%" /> 

Centr√©monos por el momento solo en una variedad de calabaza, el 'tipo para tarta', y veamos qu√© efecto tiene la fecha en el precio:

```python
pie_pumpkins = new_pumpkins[new_pumpkins['Variety']=='PIE TYPE']
pie_pumpkins.plot.scatter('DayOfYear','Price') 
```
<img alt="Gr√°fico de dispersi√≥n de Precio vs. D√≠a del A√±o" src="images/pie-pumpkins-scatter.png" width="50%" /> 

Si ahora calculamos la correlaci√≥n entre `Price` and `DayOfYear` using `corr` function, we will get something like `-0.27` - lo que significa que tiene sentido entrenar un modelo predictivo.

> Antes de entrenar un modelo de regresi√≥n lineal, es importante asegurarse de que nuestros datos est√©n limpios. La regresi√≥n lineal no funciona bien con valores faltantes, por lo que tiene sentido deshacerse de todas las celdas vac√≠as:

```python
pie_pumpkins.dropna(inplace=True)
pie_pumpkins.info()
```

Otro enfoque ser√≠a llenar esos valores vac√≠os con valores medios de la columna correspondiente.

## Regresi√≥n Lineal Simple

[![ML para principiantes - Regresi√≥n Lineal y Polin√≥mica usando Scikit-learn](https://img.youtube.com/vi/e4c_UP2fSjg/0.jpg)](https://youtu.be/e4c_UP2fSjg "ML para principiantes - Regresi√≥n Lineal y Polin√≥mica usando Scikit-learn")

> üé• Haz clic en la imagen de arriba para una breve visi√≥n general de la regresi√≥n lineal y polin√≥mica.

Para entrenar nuestro modelo de Regresi√≥n Lineal, utilizaremos la biblioteca **Scikit-learn**.

```python
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
```

Comenzamos separando los valores de entrada (caracter√≠sticas) y la salida esperada (etiqueta) en matrices numpy separadas:

```python
X = pie_pumpkins['DayOfYear'].to_numpy().reshape(-1,1)
y = pie_pumpkins['Price']
```

> Nota que tuvimos que realizar `reshape` en los datos de entrada para que el paquete de Regresi√≥n Lineal los entienda correctamente. La Regresi√≥n Lineal espera una matriz 2D como entrada, donde cada fila de la matriz corresponde a un vector de caracter√≠sticas de entrada. En nuestro caso, como solo tenemos una entrada, necesitamos una matriz con forma N√ó1, donde N es el tama√±o del conjunto de datos.

Luego, necesitamos dividir los datos en conjuntos de entrenamiento y prueba, para que podamos validar nuestro modelo despu√©s del entrenamiento:

```python
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
```

Finalmente, entrenar el modelo de Regresi√≥n Lineal real toma solo dos l√≠neas de c√≥digo. Definimos el m√©todo `LinearRegression` object, and fit it to our data using the `fit`:

```python
lin_reg = LinearRegression()
lin_reg.fit(X_train,y_train)
```

El `LinearRegression` object after `fit`-ting contains all the coefficients of the regression, which can be accessed using `.coef_` property. In our case, there is just one coefficient, which should be around `-0.017`. It means that prices seem to drop a bit with time, but not too much, around 2 cents per day. We can also access the intersection point of the regression with Y-axis using `lin_reg.intercept_` - it will be around `21` en nuestro caso, indicando el precio al comienzo del a√±o.

Para ver qu√© tan preciso es nuestro modelo, podemos predecir precios en un conjunto de datos de prueba, y luego medir qu√© tan cerca est√°n nuestras predicciones de los valores esperados. Esto se puede hacer usando la m√©trica de error cuadr√°tico medio (MSE), que es la media de todas las diferencias al cuadrado entre el valor esperado y el valor predicho.

```python
pred = lin_reg.predict(X_test)

mse = np.sqrt(mean_squared_error(y_test,pred))
print(f'Mean error: {mse:3.3} ({mse/np.mean(pred)*100:3.3}%)')
```

Nuestro error parece ser de alrededor de 2 puntos, lo que es ~17%. No es muy bueno. Otro indicador de la calidad del modelo es el **coeficiente de determinaci√≥n**, que se puede obtener as√≠:

```python
score = lin_reg.score(X_train,y_train)
print('Model determination: ', score)
```
Si el valor es 0, significa que el modelo no toma en cuenta los datos de entrada, y act√∫a como el *peor predictor lineal*, que es simplemente un valor medio del resultado. El valor de 1 significa que podemos predecir perfectamente todos los resultados esperados. En nuestro caso, el coeficiente es alrededor de 0.06, lo cual es bastante bajo.

Tambi√©n podemos graficar los datos de prueba junto con la l√≠nea de regresi√≥n para ver mejor c√≥mo funciona la regresi√≥n en nuestro caso:

```python
plt.scatter(X_test,y_test)
plt.plot(X_test,pred)
```

<img alt="Regresi√≥n lineal" src="images/linear-results.png" width="50%" />

## Regresi√≥n Polin√≥mica

Otro tipo de Regresi√≥n Lineal es la Regresi√≥n Polin√≥mica. Aunque a veces hay una relaci√≥n lineal entre variables, como que cuanto mayor es el volumen de la calabaza, mayor es el precio, a veces estas relaciones no se pueden trazar como un plano o una l√≠nea recta.

‚úÖ Aqu√≠ hay [algunos ejemplos m√°s](https://online.stat.psu.edu/stat501/lesson/9/9.8) de datos que podr√≠an usar Regresi√≥n Polin√≥mica

Mira nuevamente la relaci√≥n entre Fecha y Precio. ¬øParece que este gr√°fico de dispersi√≥n deber√≠a necesariamente ser analizado por una l√≠nea recta? ¬øNo pueden fluctuar los precios? En este caso, puedes intentar la regresi√≥n polin√≥mica.

‚úÖ Los polinomios son expresiones matem√°ticas que pueden consistir en una o m√°s variables y coeficientes

La regresi√≥n polin√≥mica crea una l√≠nea curva para ajustar mejor los datos no lineales. En nuestro caso, si incluimos una variable `DayOfYear` al cuadrado en los datos de entrada, deber√≠amos poder ajustar nuestros datos con una curva parab√≥lica, que tendr√° un m√≠nimo en un cierto punto dentro del a√±o.

Scikit-learn incluye una √∫til [API de pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html?highlight=pipeline#sklearn.pipeline.make_pipeline) para combinar diferentes pasos de procesamiento de datos juntos. Un **pipeline** es una cadena de **estimadores**. En nuestro caso, crearemos un pipeline que primero agregue caracter√≠sticas polin√≥micas a nuestro modelo, y luego entrene la regresi√≥n:

```python
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline

pipeline = make_pipeline(PolynomialFeatures(2), LinearRegression())

pipeline.fit(X_train,y_train)
```

Usando `PolynomialFeatures(2)` means that we will include all second-degree polynomials from the input data. In our case it will just mean `DayOfYear`<sup>2</sup>, but given two input variables X and Y, this will add X<sup>2</sup>, XY and Y<sup>2</sup>. We may also use higher degree polynomials if we want.

Pipelines can be used in the same manner as the original `LinearRegression` object, i.e. we can `fit` the pipeline, and then use `predict` to get the prediction results. Here is the graph showing test data, and the approximation curve:

<img alt="Polynomial regression" src="images/poly-results.png" width="50%" />

Using Polynomial Regression, we can get slightly lower MSE and higher determination, but not significantly. We need to take into account other features!

> You can see that the minimal pumpkin prices are observed somewhere around Halloween. How can you explain this? 

üéÉ Congratulations, you just created a model that can help predict the price of pie pumpkins. You can probably repeat the same procedure for all pumpkin types, but that would be tedious. Let's learn now how to take pumpkin variety into account in our model!

## Categorical Features

In the ideal world, we want to be able to predict prices for different pumpkin varieties using the same model. However, the `Variety` column is somewhat different from columns like `Month`, because it contains non-numeric values. Such columns are called **categorical**.

[![ML for beginners - Categorical Feature Predictions with Linear Regression](https://img.youtube.com/vi/DYGliioIAE0/0.jpg)](https://youtu.be/DYGliioIAE0 "ML for beginners - Categorical Feature Predictions with Linear Regression")

> üé• Click the image above for a short video overview of using categorical features.

Here you can see how average price depends on variety:

<img alt="Average price by variety" src="images/price-by-variety.png" width="50%" />

To take variety into account, we first need to convert it to numeric form, or **encode** it. There are several way we can do it:

* Simple **numeric encoding** will build a table of different varieties, and then replace the variety name by an index in that table. This is not the best idea for linear regression, because linear regression takes the actual numeric value of the index, and adds it to the result, multiplying by some coefficient. In our case, the relationship between the index number and the price is clearly non-linear, even if we make sure that indices are ordered in some specific way.
* **One-hot encoding** will replace the `Variety` column by 4 different columns, one for each variety. Each column will contain `1` if the corresponding row is of a given variety, and `0` de lo contrario. Esto significa que habr√° cuatro coeficientes en la regresi√≥n lineal, uno para cada variedad de calabaza, responsable del "precio inicial" (o m√°s bien "precio adicional") para esa variedad en particular.

El c√≥digo a continuaci√≥n muestra c√≥mo podemos codificar una variedad en una sola columna:

```python
pd.get_dummies(new_pumpkins['Variety'])
```

 ID | FAIRYTALE | MINIATURE | MIXED HEIRLOOM VARIETIES | PIE TYPE
----|-----------|-----------|--------------------------|----------
70 | 0 | 0 | 0 | 1
71 | 0 | 0 | 0 | 1
... | ... | ... | ... | ...
1738 | 0 | 1 | 0 | 0
1739 | 0 | 1 | 0 | 0
1740 | 0 | 1 | 0 | 0
1741 | 0 | 1 | 0 | 0
1742 | 0 | 1 | 0 | 0

Para entrenar la regresi√≥n lineal usando la variedad codificada en una sola columna como entrada, solo necesitamos inicializar los datos `X` and `y` correctamente:

```python
X = pd.get_dummies(new_pumpkins['Variety'])
y = new_pumpkins['Price']
```

El resto del c√≥digo es el mismo que usamos arriba para entrenar la Regresi√≥n Lineal. Si lo pruebas, ver√°s que el error cuadr√°tico medio es aproximadamente el mismo, pero obtenemos un coeficiente de determinaci√≥n mucho m√°s alto (~77%). Para obtener predicciones a√∫n m√°s precisas, podemos tener en cuenta m√°s caracter√≠sticas categ√≥ricas, as√≠ como caracter√≠sticas num√©ricas, como `Month` or `DayOfYear`. To get one large array of features, we can use `join`:

```python
X = pd.get_dummies(new_pumpkins['Variety']) \
        .join(new_pumpkins['Month']) \
        .join(pd.get_dummies(new_pumpkins['City'])) \
        .join(pd.get_dummies(new_pumpkins['Package']))
y = new_pumpkins['Price']
```

Aqu√≠ tambi√©n tenemos en cuenta `City` and `Package` tipo, lo que nos da un MSE de 2.84 (10%), y una determinaci√≥n de 0.94!

## Poni√©ndolo todo junto

Para hacer el mejor modelo, podemos usar datos combinados (codificados en una sola columna categ√≥rica + num√©rica) del ejemplo anterior junto con la Regresi√≥n Polin√≥mica. Aqu√≠ est√° el c√≥digo completo para tu conveniencia:

```python
# set up training data
X = pd.get_dummies(new_pumpkins['Variety']) \
        .join(new_pumpkins['Month']) \
        .join(pd.get_dummies(new_pumpkins['City'])) \
        .join(pd.get_dummies(new_pumpkins['Package']))
y = new_pumpkins['Price']

# make train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# setup and train the pipeline
pipeline = make_pipeline(PolynomialFeatures(2), LinearRegression())
pipeline.fit(X_train,y_train)

# predict results for test data
pred = pipeline.predict(X_test)

# calculate MSE and determination
mse = np.sqrt(mean_squared_error(y_test,pred))
print(f'Mean error: {mse:3.3} ({mse/np.mean(pred)*100:3.3}%)')

score = pipeline.score(X_train,y_train)
print('Model determination: ', score)
```

Esto deber√≠a darnos el mejor coeficiente de determinaci√≥n de casi 97%, y MSE=2.23 (~8% de error de predicci√≥n).

| Modelo | MSE | Determinaci√≥n |
|-------|-----|---------------|
| `DayOfYear` Linear | 2.77 (17.2%) | 0.07 |
| `DayOfYear` Polynomial | 2.73 (17.0%) | 0.08 |
| `Variety` Lineal | 5.24 (19.7%) | 0.77 |
| Todas las caracter√≠sticas Lineal | 2.84 (10.5%) | 0.94 |
| Todas las caracter√≠sticas Polin√≥mica | 2.23 (8.25%) | 0.97 |

üèÜ ¬°Bien hecho! Creaste cuatro modelos de Regresi√≥n en una lecci√≥n, y mejoraste la calidad del modelo al 97%. En la secci√≥n final sobre Regresi√≥n, aprender√°s sobre la Regresi√≥n Log√≠stica para determinar categor√≠as. 

---
## üöÄDesaf√≠o

Prueba varias variables diferentes en este cuaderno para ver c√≥mo la correlaci√≥n corresponde a la precisi√≥n del modelo.

## [Cuestionario posterior a la lecci√≥n](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/14/)

## Revisi√≥n y Autoestudio

En esta lecci√≥n aprendimos sobre la Regresi√≥n Lineal. Hay otros tipos importantes de Regresi√≥n. Lee sobre las t√©cnicas de Stepwise, Ridge, Lasso y Elasticnet. Un buen curso para estudiar y aprender m√°s es el [curso de Stanford Statistical Learning](https://online.stanford.edu/courses/sohs-ystatslearning-statistical-learning)

## Asignaci√≥n 

[Construye un Modelo](assignment.md)

**Descargo de responsabilidad**:
Este documento ha sido traducido utilizando servicios de traducci√≥n autom√°tica basados en IA. Si bien nos esforzamos por lograr precisi√≥n, tenga en cuenta que las traducciones autom√°ticas pueden contener errores o imprecisiones. El documento original en su idioma nativo debe considerarse la fuente autorizada. Para informaci√≥n cr√≠tica, se recomienda una traducci√≥n humana profesional. No nos hacemos responsables de ning√∫n malentendido o interpretaci√≥n err√≥nea que surja del uso de esta traducci√≥n.