# Introducci√≥n a la agrupaci√≥n

La agrupaci√≥n es un tipo de [Aprendizaje No Supervisado](https://wikipedia.org/wiki/Unsupervised_learning) que supone que un conjunto de datos no est√° etiquetado o que sus entradas no est√°n emparejadas con salidas predefinidas. Utiliza varios algoritmos para clasificar datos no etiquetados y proporcionar agrupaciones seg√∫n los patrones que discierne en los datos.

[![No One Like You by PSquare](https://img.youtube.com/vi/ty2advRiWJM/0.jpg)](https://youtu.be/ty2advRiWJM "No One Like You by PSquare")

> üé• Haz clic en la imagen de arriba para ver un video. Mientras estudias el aprendizaje autom√°tico con agrupaci√≥n, disfruta de algunas pistas de Dance Hall nigeriano: esta es una canci√≥n muy valorada de 2014 por PSquare.

## [Cuestionario antes de la lecci√≥n](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/27/)
### Introducci√≥n

[La agrupaci√≥n](https://link.springer.com/referenceworkentry/10.1007%2F978-0-387-30164-8_124) es muy √∫til para la exploraci√≥n de datos. Veamos si puede ayudar a descubrir tendencias y patrones en la forma en que el p√∫blico nigeriano consume m√∫sica.

‚úÖ T√≥mate un minuto para pensar en los usos de la agrupaci√≥n. En la vida real, la agrupaci√≥n ocurre cada vez que tienes una pila de ropa y necesitas separar la ropa de los miembros de tu familia üß¶üëïüëñü©≤. En ciencia de datos, la agrupaci√≥n ocurre cuando se intenta analizar las preferencias de un usuario o determinar las caracter√≠sticas de cualquier conjunto de datos no etiquetado. La agrupaci√≥n, de alguna manera, ayuda a darle sentido al caos, como un caj√≥n de calcetines.

[![Introduction to ML](https://img.youtube.com/vi/esmzYhuFnds/0.jpg)](https://youtu.be/esmzYhuFnds "Introduction to Clustering")

> üé• Haz clic en la imagen de arriba para ver un video: John Guttag del MIT introduce la agrupaci√≥n

En un entorno profesional, la agrupaci√≥n puede usarse para determinar cosas como la segmentaci√≥n del mercado, determinando qu√© grupos de edad compran qu√© art√≠culos, por ejemplo. Otro uso ser√≠a la detecci√≥n de anomal√≠as, tal vez para detectar fraudes en un conjunto de datos de transacciones con tarjetas de cr√©dito. O podr√≠as usar la agrupaci√≥n para determinar tumores en un lote de escaneos m√©dicos.

‚úÖ Piensa un minuto sobre c√≥mo podr√≠as haber encontrado la agrupaci√≥n 'en la naturaleza', en un entorno bancario, de comercio electr√≥nico o de negocios.

> üéì Curiosamente, el an√°lisis de agrupaciones se origin√≥ en los campos de la Antropolog√≠a y la Psicolog√≠a en la d√©cada de 1930. ¬øPuedes imaginar c√≥mo podr√≠a haberse utilizado?

Alternativamente, podr√≠as usarlo para agrupar resultados de b√∫squeda: por enlaces de compras, im√°genes o rese√±as, por ejemplo. La agrupaci√≥n es √∫til cuando tienes un conjunto de datos grande que deseas reducir y sobre el cual deseas realizar un an√°lisis m√°s granular, por lo que la t√©cnica puede usarse para aprender sobre los datos antes de que se construyan otros modelos.

‚úÖ Una vez que tus datos est√©n organizados en grupos, les asignas un Id de grupo, y esta t√©cnica puede ser √∫til para preservar la privacidad del conjunto de datos; en lugar de referirse a un punto de datos por su id de grupo, en lugar de por datos m√°s reveladores e identificables. ¬øPuedes pensar en otras razones por las que preferir√≠as referirte a un Id de grupo en lugar de a otros elementos del grupo para identificarlo?

Profundiza tu comprensi√≥n de las t√©cnicas de agrupaci√≥n en este [m√≥dulo de aprendizaje](https://docs.microsoft.com/learn/modules/train-evaluate-cluster-models?WT.mc_id=academic-77952-leestott)
## Empezando con la agrupaci√≥n

[Scikit-learn ofrece una amplia gama](https://scikit-learn.org/stable/modules/clustering.html) de m√©todos para realizar agrupaciones. El tipo que elijas depender√° de tu caso de uso. Seg√∫n la documentaci√≥n, cada m√©todo tiene varios beneficios. Aqu√≠ hay una tabla simplificada de los m√©todos compatibles con Scikit-learn y sus casos de uso apropiados:

| Nombre del m√©todo            | Caso de uso                                                                |
| :--------------------------- | :------------------------------------------------------------------------- |
| K-Means                      | prop√≥sito general, inductivo                                               |
| Affinity propagation         | muchos, grupos desiguales, inductivo                                       |
| Mean-shift                   | muchos, grupos desiguales, inductivo                                       |
| Spectral clustering          | pocos, grupos uniformes, transductivo                                      |
| Ward hierarchical clustering | muchos, grupos restringidos, transductivo                                  |
| Agglomerative clustering     | muchos, restringidos, distancias no euclidianas, transductivo              |
| DBSCAN                       | geometr√≠a no plana, grupos desiguales, transductivo                        |
| OPTICS                       | geometr√≠a no plana, grupos desiguales con densidad variable, transductivo  |
| Gaussian mixtures            | geometr√≠a plana, inductivo                                                 |
| BIRCH                        | conjunto de datos grande con valores at√≠picos, inductivo                   |

> üéì C√≥mo creamos grupos tiene mucho que ver con c√≥mo reunimos los puntos de datos en grupos. Desempaquemos algo de vocabulario:
>
> üéì ['Transductivo' vs. 'inductivo'](https://wikipedia.org/wiki/Transduction_(machine_learning))
> 
> La inferencia transductiva se deriva de casos de entrenamiento observados que se asignan a casos de prueba espec√≠ficos. La inferencia inductiva se deriva de casos de entrenamiento que se asignan a reglas generales que solo entonces se aplican a casos de prueba.
> 
> Un ejemplo: Imagina que tienes un conjunto de datos que solo est√° parcialmente etiquetado. Algunas cosas son 'discos', algunas 'CDs' y algunas est√°n en blanco. Tu trabajo es proporcionar etiquetas para los vac√≠os. Si eliges un enfoque inductivo, entrenar√≠as un modelo buscando 'discos' y 'CDs', y aplicar√≠as esas etiquetas a tus datos no etiquetados. Este enfoque tendr√° problemas para clasificar cosas que en realidad son 'cassettes'. Un enfoque transductivo, por otro lado, maneja estos datos desconocidos de manera m√°s efectiva, ya que trabaja para agrupar elementos similares y luego aplica una etiqueta a un grupo. En este caso, los grupos podr√≠an reflejar 'cosas musicales redondas' y 'cosas musicales cuadradas'.
> 
> üéì ['Geometr√≠a no plana' vs. 'plana'](https://datascience.stackexchange.com/questions/52260/terminology-flat-geometry-in-the-context-of-clustering)
> 
> Derivado de la terminolog√≠a matem√°tica, la geometr√≠a no plana vs. plana se refiere a la medida de distancias entre puntos mediante m√©todos geom√©tricos 'planos' ([Euclidianos](https://wikipedia.org/wiki/Euclidean_geometry)) o 'no planos' (no euclidianos).
>
>'Plana' en este contexto se refiere a la geometr√≠a euclidiana (partes de la cual se ense√±an como geometr√≠a 'plana'), y no plana se refiere a la geometr√≠a no euclidiana. ¬øQu√© tiene que ver la geometr√≠a con el aprendizaje autom√°tico? Bueno, como dos campos que est√°n arraigados en las matem√°ticas, debe haber una forma com√∫n de medir las distancias entre puntos en grupos, y eso se puede hacer de manera 'plana' o 'no plana', dependiendo de la naturaleza de los datos. Las distancias [euclidianas](https://wikipedia.org/wiki/Euclidean_distance) se miden como la longitud de un segmento de l√≠nea entre dos puntos. Las distancias [no euclidianas](https://wikipedia.org/wiki/Non-Euclidean_geometry) se miden a lo largo de una curva. Si tus datos, visualizados, parecen no existir en un plano, es posible que necesites usar un algoritmo especializado para manejarlos.
>
![Flat vs Nonflat Geometry Infographic](../../../../translated_images/flat-nonflat.d1c8c6e2a96110c1d57fa0b72913f6aab3c245478524d25baf7f4a18efcde224.es.png)
> Infograf√≠a por [Dasani Madipalli](https://twitter.com/dasani_decoded)
> 
> üéì ['Distancias'](https://web.stanford.edu/class/cs345a/slides/12-clustering.pdf)
> 
> Los grupos se definen por su matriz de distancias, es decir, las distancias entre puntos. Esta distancia se puede medir de varias maneras. Los grupos euclidianos se definen por el promedio de los valores de los puntos y contienen un 'centroide' o punto central. Las distancias, por lo tanto, se miden por la distancia a ese centroide. Las distancias no euclidianas se refieren a 'clustroides', el punto m√°s cercano a otros puntos. Los clustroides, a su vez, pueden definirse de varias maneras.
> 
> üéì ['Restringido'](https://wikipedia.org/wiki/Constrained_clustering)
> 
> [La Agrupaci√≥n Restringida](https://web.cs.ucdavis.edu/~davidson/Publications/ICDMTutorial.pdf) introduce el aprendizaje 'semi-supervisado' en este m√©todo no supervisado. Las relaciones entre puntos se marcan como 'no pueden enlazarse' o 'deben enlazarse', por lo que se imponen algunas reglas al conjunto de datos.
>
>Un ejemplo: Si un algoritmo se libera en un lote de datos no etiquetados o semi-etiquetados, los grupos que produce pueden ser de baja calidad. En el ejemplo anterior, los grupos podr√≠an agrupar 'cosas musicales redondas' y 'cosas musicales cuadradas' y 'cosas triangulares' y 'galletas'. Si se le dan algunas restricciones o reglas a seguir ("el √≠tem debe estar hecho de pl√°stico", "el √≠tem debe poder producir m√∫sica"), esto puede ayudar a 'restringir' el algoritmo para tomar mejores decisiones.
> 
> üéì 'Densidad'
> 
> Los datos que son 'ruidosos' se consideran 'densos'. Las distancias entre puntos en cada uno de sus grupos pueden demostrar, al examinarlas, ser m√°s o menos densas, o 'abarrotadas', y por lo tanto, estos datos deben analizarse con el m√©todo de agrupaci√≥n apropiado. [Este art√≠culo](https://www.kdnuggets.com/2020/02/understanding-density-based-clustering.html) demuestra la diferencia entre usar el algoritmo de agrupaci√≥n K-Means vs. HDBSCAN para explorar un conjunto de datos ruidosos con densidad de grupo desigual.

## Algoritmos de agrupaci√≥n

Existen m√°s de 100 algoritmos de agrupaci√≥n, y su uso depende de la naturaleza de los datos disponibles. Hablemos de algunos de los principales:

- **Agrupaci√≥n jer√°rquica**. Si un objeto se clasifica por su proximidad a un objeto cercano, en lugar de a uno m√°s lejano, los grupos se forman en funci√≥n de la distancia de sus miembros a y desde otros objetos. La agrupaci√≥n aglomerativa de Scikit-learn es jer√°rquica.

   ![Hierarchical clustering Infographic](../../../../translated_images/hierarchical.bf59403aa43c8c47493bfdf1cc25230f26e45f4e38a3d62e8769cd324129ac15.es.png)
   > Infograf√≠a por [Dasani Madipalli](https://twitter.com/dasani_decoded)

- **Agrupaci√≥n de centroides**. Este algoritmo popular requiere la elecci√≥n de 'k', o el n√∫mero de grupos a formar, despu√©s de lo cual el algoritmo determina el punto central de un grupo y re√∫ne datos alrededor de ese punto. [La agrupaci√≥n K-means](https://wikipedia.org/wiki/K-means_clustering) es una versi√≥n popular de la agrupaci√≥n de centroides. El centro se determina por la media m√°s cercana, de ah√≠ el nombre. La distancia cuadrada desde el grupo se minimiza.

   ![Centroid clustering Infographic](../../../../translated_images/centroid.097fde836cf6c9187d0b2033e9f94441829f9d86f4f0b1604dd4b3d1931aee34.es.png)
   > Infograf√≠a por [Dasani Madipalli](https://twitter.com/dasani_decoded)

- **Agrupaci√≥n basada en distribuci√≥n**. Basada en el modelado estad√≠stico, la agrupaci√≥n basada en distribuci√≥n se centra en determinar la probabilidad de que un punto de datos pertenezca a un grupo y asignarlo en consecuencia. Los m√©todos de mezcla gaussiana pertenecen a este tipo.

- **Agrupaci√≥n basada en densidad**. Los puntos de datos se asignan a grupos en funci√≥n de su densidad o su agrupaci√≥n entre s√≠. Los puntos de datos alejados del grupo se consideran valores at√≠picos o ruido. DBSCAN, Mean-shift y OPTICS pertenecen a este tipo de agrupaci√≥n.

- **Agrupaci√≥n basada en cuadr√≠cula**. Para conjuntos de datos multidimensionales, se crea una cuadr√≠cula y los datos se dividen entre las celdas de la cuadr√≠cula, creando as√≠ grupos.

## Ejercicio - agrupa tus datos

La t√©cnica de agrupaci√≥n se ve muy beneficiada por la visualizaci√≥n adecuada, as√≠ que comencemos visualizando nuestros datos musicales. Este ejercicio nos ayudar√° a decidir cu√°l de los m√©todos de agrupaci√≥n deber√≠amos usar m√°s eficazmente para la naturaleza de estos datos.

1. Abre el archivo [_notebook.ipynb_](https://github.com/microsoft/ML-For-Beginners/blob/main/5-Clustering/1-Visualize/notebook.ipynb) en esta carpeta.

1. Importa el paquete `Seaborn` para una buena visualizaci√≥n de datos.

    ```python
    !pip install seaborn
    ```

1. Agrega los datos de las canciones desde [_nigerian-songs.csv_](https://github.com/microsoft/ML-For-Beginners/blob/main/5-Clustering/data/nigerian-songs.csv). Carga un dataframe con algunos datos sobre las canciones. Prep√°rate para explorar estos datos importando las bibliotecas y descargando los datos:

    ```python
    import matplotlib.pyplot as plt
    import pandas as pd
    
    df = pd.read_csv("../data/nigerian-songs.csv")
    df.head()
    ```

    Revisa las primeras l√≠neas de datos:

    |     | nombre                   | √°lbum                        | artista             | g√©nero_principal_artista | fecha_lanzamiento | duraci√≥n | popularidad | bailabilidad | acusticidad | energ√≠a | instrumentalidad | vivacidad | volumen | locuacidad | tempo   | comp√°s |
    | --- | ------------------------ | ---------------------------- | ------------------- | ------------------------ | ----------------- | -------- | ----------- | ------------ | ------------ | ------- | ----------------- | --------- | ------- | ----------- | ------- | ------ |
    | 0   | Sparky                   | Mandy & The Jungle           | Cruel Santino       | alternative r&b          | 2019              | 144000   | 48          | 0.666        | 0.851        | 0.42    | 0.534             | 0.11      | -6.699  | 0.0829      | 133.015 | 5      |
    | 1   | shuga rush               | EVERYTHING YOU HEARD IS TRUE | Odunsi (The Engine) | afropop                  | 2020              | 89488    | 30          | 0.71         | 0.0822       | 0.683   | 0.000169          | 0.101     | -5.64   | 0.36        | 129.993 | 3      |
    | 2   | LITT!                    | LITT!                        | AYL√ò                | indie r&b                | 2018              | 207758   | 40          | 0.836        | 0.272        | 0.564   | 0.000537          | 0.11      | -7.127  | 0.0424      | 130.005 | 4      |
    | 3   | Confident / Feeling Cool | Enjoy Your Life              | Lady Donli          | nigerian pop             | 2019              | 175135   | 14          | 0.894        | 0.798        | 0.611   | 0.000187          | 0.0964    | -4.961  | 0.113       | 111.087 | 4      |
    | 4   | wanted you               | rare.                        | Odunsi (The Engine) | afropop                  | 2018              | 152049   | 25          | 0.702        | 0.116        | 0.833   | 0.91              | 0.348     | -6.044  | 0.0447      | 105.115 | 4      |

1. Obt√©n informaci√≥n sobre el dataframe, llamando a `info()`:

    ```python
    df.info()
    ```

   La salida se ver√° as√≠:

    ```output
    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 530 entries, 0 to 529
    Data columns (total 16 columns):
     #   Column            Non-Null Count  Dtype  
    ---  ------            --------------  -----  
     0   name              530 non-null    object 
     1   album             530 non-null    object 
     2   artist            530 non-null    object 
     3   artist_top_genre  530 non-null    object 
     4   release_date      530 non-null    int64  
     5   length            530 non-null    int64  
     6   popularity        530 non-null    int64  
     7   danceability      530 non-null    float64
     8   acousticness      530 non-null    float64
     9   energy            530 non-null    float64
     10  instrumentalness  530 non-null    float64
     11  liveness          530 non-null    float64
     12  loudness          530 non-null    float64
     13  speechiness       530 non-null    float64
     14  tempo             530 non-null    float64
     15  time_signature    530 non-null    int64  
    dtypes: float64(8), int64(4), object(4)
    memory usage: 66.4+ KB
    ```

1. Verifica nuevamente si hay valores nulos, llamando a `isnull()` y verificando que la suma sea 0:

    ```python
    df.isnull().sum()
    ```

    Se ve bien:

    ```output
    name                0
    album               0
    artist              0
    artist_top_genre    0
    release_date        0
    length              0
    popularity          0
    danceability        0
    acousticness        0
    energy              0
    instrumentalness    0
    liveness            0
    loudness            0
    speechiness         0
    tempo               0
    time_signature      0
    dtype: int64
    ```

1. Describe los datos:

    ```python
    df.describe()
    ```

    |       | fecha_lanzamiento | duraci√≥n   | popularidad | bailabilidad | acusticidad | energ√≠a   | instrumentalidad | vivacidad | volumen   | locuacidad | tempo      | comp√°s  |
    | ----- | ----------------- | ---------- | ----------- | ------------ | ------------ | --------- | ----------------- | --------- | --------- | ----------- | ---------- | ------- |
    | count | 530               | 530        | 530         | 530          | 530          | 530       | 530               | 530       | 530       | 530         | 530        | 530     |
    | mean  | 2015.390566       | 222298.1698| 17.507547   | 0.741619     | 0.265412     | 0.760623  | 0.016305          | 0.147308  | -4.953011 | 0.130748    | 116.487864 | 3.986792|
    | std   | 3.131688          | 39696.82226| 18.992212   | 0.117522     | 0.208342     | 0.148533  | 0.090321          | 0.123588  | 2.464186  | 0.092939    | 23.518601  | 0.333701|
    | min   | 1998              | 89488      | 0           | 0.255        | 0.000665     | 0.111     | 0                 | 0.0283    | -19.362   | 0.0278      | 61.695     | 3       |
    | 25%   | 2014              | 199305     | 0           | 0.681        | 0.089525     | 0.669     | 0                
## [Cuestionario posterior a la lecci√≥n](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/28/)

## Revisi√≥n y Autoestudio

Antes de aplicar algoritmos de clustering, como hemos aprendido, es una buena idea entender la naturaleza de tu conjunto de datos. Lee m√°s sobre este tema [aqu√≠](https://www.kdnuggets.com/2019/10/right-clustering-algorithm.html)

[Este art√≠culo √∫til](https://www.freecodecamp.org/news/8-clustering-algorithms-in-machine-learning-that-all-data-scientists-should-know/) te gu√≠a a trav√©s de las diferentes formas en que se comportan varios algoritmos de clustering, dadas diferentes formas de datos.

## Tarea

[Investiga otras visualizaciones para clustering](assignment.md)

        **Descargo de responsabilidad**:
        Este documento ha sido traducido utilizando servicios de traducci√≥n autom√°tica basados en inteligencia artificial. Aunque nos esforzamos por lograr precisi√≥n, tenga en cuenta que las traducciones autom√°ticas pueden contener errores o imprecisiones. El documento original en su idioma nativo debe considerarse la fuente autorizada. Para informaci√≥n cr√≠tica, se recomienda una traducci√≥n humana profesional. No somos responsables de ning√∫n malentendido o interpretaci√≥n err√≥nea que surja del uso de esta traducci√≥n.