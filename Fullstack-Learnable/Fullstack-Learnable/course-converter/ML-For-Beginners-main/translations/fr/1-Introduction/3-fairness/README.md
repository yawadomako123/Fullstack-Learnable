# Construire des solutions d'apprentissage automatique avec une IA responsable

![R√©sum√© de l'IA responsable dans l'apprentissage automatique dans une sketchnote](../../../../translated_images/ml-fairness.ef296ebec6afc98a44566d7b6c1ed18dc2bf1115c13ec679bb626028e852fa1d.fr.png)
> Sketchnote par [Tomomi Imura](https://www.twitter.com/girlie_mac)

## [Quiz pr√©-conf√©rence](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/5/)

## Introduction

Dans ce programme, vous commencerez √† d√©couvrir comment l'apprentissage automatique impacte notre vie quotidienne. M√™me maintenant, des syst√®mes et des mod√®les sont impliqu√©s dans des t√¢ches de prise de d√©cision quotidiennes, telles que les diagnostics m√©dicaux, les approbations de pr√™ts ou la d√©tection de fraudes. Il est donc important que ces mod√®les fonctionnent bien pour fournir des r√©sultats fiables. Tout comme toute application logicielle, les syst√®mes d'IA peuvent ne pas r√©pondre aux attentes ou avoir des r√©sultats ind√©sirables. C'est pourquoi il est essentiel de comprendre et d'expliquer le comportement d'un mod√®le d'IA.

Imaginez ce qui peut se passer lorsque les donn√©es que vous utilisez pour construire ces mod√®les manquent de certaines d√©mographies, comme la race, le sexe, les opinions politiques, la religion, ou repr√©sentent de mani√®re disproportionn√©e ces d√©mographies. Que se passe-t-il lorsque la sortie du mod√®le est interpr√©t√©e comme favorisant une certaine d√©mographie ? Quelle est la cons√©quence pour l'application ? De plus, que se passe-t-il lorsque le mod√®le a un r√©sultat n√©gatif et nuit aux personnes ? Qui est responsable du comportement des syst√®mes d'IA ? Ce sont quelques-unes des questions que nous explorerons dans ce programme.

Dans cette le√ßon, vous allez :

- Prendre conscience de l'importance de l'√©quit√© dans l'apprentissage automatique et des pr√©judices li√©s √† l'√©quit√©.
- Vous familiariser avec la pratique d'explorer des valeurs aberrantes et des sc√©narios inhabituels pour garantir la fiabilit√© et la s√©curit√©.
- Comprendre la n√©cessit√© de responsabiliser tout le monde en concevant des syst√®mes inclusifs.
- Explorer √† quel point il est vital de prot√©ger la vie priv√©e et la s√©curit√© des donn√©es et des personnes.
- Voir l'importance d'adopter une approche en "bo√Æte de verre" pour expliquer le comportement des mod√®les d'IA.
- √ätre conscient de l'importance de la responsabilit√© pour instaurer la confiance dans les syst√®mes d'IA.

## Pr√©requis

Comme pr√©requis, veuillez suivre le parcours d'apprentissage "Principes de l'IA responsable" et regarder la vid√©o ci-dessous sur le sujet :

En savoir plus sur l'IA responsable en suivant ce [parcours d'apprentissage](https://docs.microsoft.com/learn/modules/responsible-ai-principles/?WT.mc_id=academic-77952-leestott)

[![L'approche de Microsoft en mati√®re d'IA responsable](https://img.youtube.com/vi/dnC8-uUZXSc/0.jpg)](https://youtu.be/dnC8-uUZXSc "L'approche de Microsoft en mati√®re d'IA responsable")

> üé• Cliquez sur l'image ci-dessus pour une vid√©o : L'approche de Microsoft en mati√®re d'IA responsable

## √âquit√©

Les syst√®mes d'IA doivent traiter tout le monde de mani√®re √©quitable et √©viter d'affecter des groupes de personnes similaires de mani√®res diff√©rentes. Par exemple, lorsque les syst√®mes d'IA fournissent des recommandations sur des traitements m√©dicaux, des demandes de pr√™t ou des emplois, ils doivent faire les m√™mes recommandations √† tous ceux qui ont des sympt√¥mes, des circonstances financi√®res ou des qualifications professionnelles similaires. Chacun de nous, en tant qu'humains, porte des biais h√©rit√©s qui influencent nos d√©cisions et actions. Ces biais peuvent √™tre √©vidents dans les donn√©es que nous utilisons pour entra√Æner les syst√®mes d'IA. Une telle manipulation peut parfois se produire de mani√®re involontaire. Il est souvent difficile de savoir consciemment quand vous introduisez un biais dans les donn√©es.

**‚ÄúL'in√©quit√©‚Äù** englobe les impacts n√©gatifs, ou ‚Äúpr√©judices‚Äù, pour un groupe de personnes, comme ceux d√©finis en termes de race, de sexe, d'√¢ge ou de statut de handicap. Les principaux pr√©judices li√©s √† l'√©quit√© peuvent √™tre class√©s comme suit :

- **Allocation**, si un sexe ou une ethnie, par exemple, est favoris√© par rapport √† un autre.
- **Qualit√© de service**. Si vous entra√Ænez les donn√©es pour un sc√©nario sp√©cifique mais que la r√©alit√© est beaucoup plus complexe, cela entra√Æne un service de mauvaise qualit√©. Par exemple, un distributeur de savon liquide qui ne semble pas √™tre capable de d√©tecter les personnes √† la peau fonc√©e. [R√©f√©rence](https://gizmodo.com/why-cant-this-soap-dispenser-identify-dark-skin-1797931773)
- **D√©nigrement**. Critiquer et √©tiqueter injustement quelque chose ou quelqu'un. Par exemple, une technologie de labellisation d'images a tristement √©tiquet√© des images de personnes √† la peau fonc√©e comme des gorilles.
- **Sur- ou sous-repr√©sentation**. L'id√©e est qu'un certain groupe n'est pas vu dans une certaine profession, et tout service ou fonction qui continue √† promouvoir cela contribue √† nuire.
- **St√©r√©otypage**. Associer un groupe donn√© √† des attributs pr√©assign√©s. Par exemple, un syst√®me de traduction entre l'anglais et le turc peut avoir des inexactitudes en raison de mots ayant des associations st√©r√©otyp√©es avec le sexe.

![traduction en turc](../../../../translated_images/gender-bias-translate-en-tr.f185fd8822c2d4372912f2b690f6aaddd306ffbb49d795ad8d12a4bf141e7af0.fr.png)
> traduction en turc

![traduction en anglais](../../../../translated_images/gender-bias-translate-tr-en.4eee7e3cecb8c70e13a8abbc379209bc8032714169e585bdeac75af09b1752aa.fr.png)
> traduction en anglais

Lors de la conception et des tests des syst√®mes d'IA, nous devons nous assurer que l'IA est √©quitable et qu'elle n'est pas programm√©e pour prendre des d√©cisions biais√©es ou discriminatoires, ce qui est √©galement interdit aux √™tres humains. Garantir l'√©quit√© dans l'IA et l'apprentissage automatique reste un d√©fi sociotechnique complexe.

### Fiabilit√© et s√©curit√©

Pour √©tablir la confiance, les syst√®mes d'IA doivent √™tre fiables, s√ªrs et coh√©rents dans des conditions normales et inattendues. Il est important de savoir comment les syst√®mes d'IA se comporteront dans une vari√©t√© de situations, surtout lorsqu'ils sont confront√©s √† des cas particuliers. Lors de la cr√©ation de solutions d'IA, il est n√©cessaire de se concentrer sur la mani√®re de g√©rer une grande vari√©t√© de circonstances que les solutions d'IA pourraient rencontrer. Par exemple, une voiture autonome doit placer la s√©curit√© des personnes comme une priorit√© absolue. En cons√©quence, l'IA qui alimente la voiture doit prendre en compte tous les sc√©narios possibles que la voiture pourrait rencontrer, tels que la nuit, les temp√™tes, les blizzards, les enfants traversant la rue, les animaux de compagnie, les travaux routiers, etc. La capacit√© d'un syst√®me d'IA √† g√©rer une large gamme de conditions de mani√®re fiable et s√©curis√©e refl√®te le niveau d'anticipation que le data scientist ou le d√©veloppeur d'IA a pris en compte lors de la conception ou des tests du syst√®me.

> [üé• Cliquez ici pour une vid√©o : ](https://www.microsoft.com/videoplayer/embed/RE4vvIl)

### Inclusivit√©

Les syst√®mes d'IA doivent √™tre con√ßus pour engager et responsabiliser tout le monde. Lors de la conception et de la mise en ≈ìuvre de syst√®mes d'IA, les data scientists et les d√©veloppeurs d'IA identifient et abordent les barri√®res potentielles dans le syst√®me qui pourraient involontairement exclure des personnes. Par exemple, il y a 1 milliard de personnes handicap√©es dans le monde. Avec l'avancement de l'IA, elles peuvent acc√©der plus facilement √† une large gamme d'informations et d'opportunit√©s dans leur vie quotidienne. En s'attaquant aux barri√®res, cela cr√©e des opportunit√©s d'innover et de d√©velopper des produits d'IA offrant de meilleures exp√©riences qui b√©n√©ficient √† tous.

> [üé• Cliquez ici pour une vid√©o : inclusivit√© dans l'IA](https://www.microsoft.com/videoplayer/embed/RE4vl9v)

### S√©curit√© et vie priv√©e

Les syst√®mes d'IA doivent √™tre s√ªrs et respecter la vie priv√©e des personnes. Les gens ont moins confiance dans les syst√®mes qui mettent en danger leur vie priv√©e, leurs informations ou leur vie. Lors de l'entra√Ænement de mod√®les d'apprentissage automatique, nous nous appuyons sur des donn√©es pour produire les meilleurs r√©sultats. Dans ce faisant, l'origine des donn√©es et leur int√©grit√© doivent √™tre prises en compte. Par exemple, les donn√©es ont-elles √©t√© soumises par l'utilisateur ou sont-elles disponibles publiquement ? Ensuite, lors du traitement des donn√©es, il est crucial de d√©velopper des syst√®mes d'IA capables de prot√©ger les informations confidentielles et de r√©sister aux attaques. √Ä mesure que l'IA devient plus r√©pandue, la protection de la vie priv√©e et la s√©curisation des informations personnelles et professionnelles importantes deviennent de plus en plus critiques et complexes. Les probl√®mes de confidentialit√© et de s√©curit√© des donn√©es n√©cessitent une attention particuli√®rement √©troite pour l'IA, car l'acc√®s aux donn√©es est essentiel pour que les syst√®mes d'IA puissent faire des pr√©dictions et des d√©cisions pr√©cises et √©clair√©es concernant les personnes.

> [üé• Cliquez ici pour une vid√©o : s√©curit√© dans l'IA](https://www.microsoft.com/videoplayer/embed/RE4voJF)

- En tant qu'industrie, nous avons fait des avanc√©es significatives en mati√®re de confidentialit√© et de s√©curit√©, aliment√©es en grande partie par des r√©glementations comme le RGPD (R√®glement g√©n√©ral sur la protection des donn√©es).
- Pourtant, avec les syst√®mes d'IA, nous devons reconna√Ætre la tension entre le besoin de plus de donn√©es personnelles pour rendre les syst√®mes plus personnels et efficaces ‚Äì et la vie priv√©e.
- Tout comme avec la naissance des ordinateurs connect√©s √† Internet, nous assistons √©galement √† une forte augmentation des probl√®mes de s√©curit√© li√©s √† l'IA.
- En m√™me temps, nous avons vu l'IA utilis√©e pour am√©liorer la s√©curit√©. Par exemple, la plupart des scanners antivirus modernes sont aujourd'hui aliment√©s par des heuristiques d'IA.
- Nous devons veiller √† ce que nos processus de science des donn√©es s'harmonisent avec les derni√®res pratiques en mati√®re de confidentialit√© et de s√©curit√©.

### Transparence

Les syst√®mes d'IA doivent √™tre compr√©hensibles. Une partie cruciale de la transparence consiste √† expliquer le comportement des syst√®mes d'IA et de leurs composants. Am√©liorer la compr√©hension des syst√®mes d'IA n√©cessite que les parties prenantes comprennent comment et pourquoi ils fonctionnent afin qu'elles puissent identifier les probl√®mes de performance potentiels, les pr√©occupations en mati√®re de s√©curit√© et de confidentialit√©, les biais, les pratiques d'exclusion ou les r√©sultats inattendus. Nous croyons √©galement que ceux qui utilisent des syst√®mes d'IA doivent √™tre honn√™tes et transparents sur quand, pourquoi et comment ils choisissent de les d√©ployer, ainsi que sur les limites des syst√®mes qu'ils utilisent. Par exemple, si une banque utilise un syst√®me d'IA pour soutenir ses d√©cisions de pr√™t √† la consommation, il est important d'examiner les r√©sultats et de comprendre quelles donn√©es influencent les recommandations du syst√®me. Les gouvernements commencent √† r√©glementer l'IA dans divers secteurs, donc les data scientists et les organisations doivent expliquer si un syst√®me d'IA respecte les exigences r√©glementaires, surtout lorsqu'il y a un r√©sultat ind√©sirable.

> [üé• Cliquez ici pour une vid√©o : transparence dans l'IA](https://www.microsoft.com/videoplayer/embed/RE4voJF)

- En raison de la complexit√© des syst√®mes d'IA, il est difficile de comprendre comment ils fonctionnent et d'interpr√©ter les r√©sultats.
- Ce manque de compr√©hension affecte la mani√®re dont ces syst√®mes sont g√©r√©s, op√©rationnalis√©s et document√©s.
- Ce manque de compr√©hension affecte plus important encore les d√©cisions prises en utilisant les r√©sultats produits par ces syst√®mes.

### Responsabilit√©

Les personnes qui con√ßoivent et d√©ploient des syst√®mes d'IA doivent √™tre responsables du fonctionnement de leurs syst√®mes. Le besoin de responsabilit√© est particuli√®rement crucial avec des technologies d'utilisation sensible comme la reconnaissance faciale. R√©cemment, il y a eu une demande croissante pour la technologie de reconnaissance faciale, en particulier de la part des organisations d'application de la loi qui voient le potentiel de la technologie dans des utilisations comme la recherche d'enfants disparus. Cependant, ces technologies pourraient potentiellement √™tre utilis√©es par un gouvernement pour mettre en danger les libert√©s fondamentales de ses citoyens en permettant, par exemple, une surveillance continue de personnes sp√©cifiques. Par cons√©quent, les data scientists et les organisations doivent √™tre responsables de l'impact de leur syst√®me d'IA sur les individus ou la soci√©t√©.

[![Un chercheur en IA avertit de la surveillance de masse par la reconnaissance faciale](../../../../translated_images/accountability.41d8c0f4b85b6231301d97f17a450a805b7a07aaeb56b34015d71c757cad142e.fr.png)](https://www.youtube.com/watch?v=Wldt8P5V6D0 "L'approche de Microsoft en mati√®re d'IA responsable")

> üé• Cliquez sur l'image ci-dessus pour une vid√©o : Avertissements de surveillance de masse par la reconnaissance faciale

En fin de compte, l'une des plus grandes questions pour notre g√©n√©ration, en tant que premi√®re g√©n√©ration qui int√®gre l'IA dans la soci√©t√©, est comment s'assurer que les ordinateurs restent responsables envers les personnes et comment s'assurer que les personnes qui con√ßoivent des ordinateurs restent responsables envers tout le monde.

## √âvaluation d'impact

Avant d'entra√Æner un mod√®le d'apprentissage automatique, il est important de r√©aliser une √©valuation d'impact pour comprendre le but du syst√®me d'IA ; quelle est son utilisation pr√©vue ; o√π il sera d√©ploy√© ; et qui interagira avec le syst√®me. Ces √©l√©ments sont utiles pour les examinateurs ou les testeurs √©valuant le syst√®me afin de savoir quels facteurs prendre en compte lors de l'identification des risques potentiels et des cons√©quences attendues.

Les domaines suivants sont des axes d'attention lors de la r√©alisation d'une √©valuation d'impact :

* **Impact n√©gatif sur les individus**. √ätre conscient de toute restriction ou exigence, d'une utilisation non prise en charge ou de toute limitation connue entravant les performances du syst√®me est vital pour s'assurer que le syst√®me n'est pas utilis√© d'une mani√®re qui pourrait nuire aux individus.
* **Exigences en mati√®re de donn√©es**. Comprendre comment et o√π le syst√®me utilisera des donn√©es permet aux examinateurs d'explorer les exigences en mati√®re de donn√©es dont vous devez tenir compte (par exemple, les r√©glementations sur les donn√©es RGPD ou HIPPA). De plus, examinez si la source ou la quantit√© de donn√©es est suffisante pour l'entra√Ænement.
* **R√©sum√© de l'impact**. Rassembler une liste de pr√©judices potentiels qui pourraient d√©couler de l'utilisation du syst√®me. Tout au long du cycle de vie de l'apprentissage automatique, v√©rifiez si les probl√®mes identifi√©s sont att√©nu√©s ou trait√©s.
* **Objectifs applicables** pour chacun des six principes fondamentaux. √âvaluer si les objectifs de chacun des principes sont atteints et s'il existe des lacunes.

## D√©bogage avec l'IA responsable

Tout comme le d√©bogage d'une application logicielle, le d√©bogage d'un syst√®me d'IA est un processus n√©cessaire d'identification et de r√©solution des probl√®mes dans le syst√®me. De nombreux facteurs peuvent affecter un mod√®le qui ne fonctionne pas comme pr√©vu ou de mani√®re responsable. La plupart des m√©triques de performance des mod√®les traditionnels sont des agr√©gats quantitatifs de la performance d'un mod√®le, qui ne suffisent pas √† analyser comment un mod√®le viole les principes de l'IA responsable. De plus, un mod√®le d'apprentissage automatique est une bo√Æte noire qui rend difficile la compr√©hension des √©l√©ments qui influencent son r√©sultat ou de fournir une explication lorsqu'il commet une erreur. Plus tard dans ce cours, nous apprendrons comment utiliser le tableau de bord de l'IA responsable pour aider √† d√©boguer les syst√®mes d'IA. Le tableau de bord fournit un outil holistique pour les data scientists et les d√©veloppeurs d'IA afin de r√©aliser :

* **Analyse des erreurs**. Identifier la distribution des erreurs du mod√®le qui peuvent affecter l'√©quit√© ou la fiabilit√© du syst√®me.
* **Aper√ßu du mod√®le**. D√©couvrir o√π se trouvent les disparit√©s dans la performance du mod√®le √† travers les cohortes de donn√©es.
* **Analyse des donn√©es**. Comprendre la distribution des donn√©es et identifier tout biais potentiel dans les donn√©es qui pourrait entra√Æner des probl√®mes d'√©quit√©, d'inclusivit√© et de fiabilit√©.
* **Interpr√©tabilit√© du mod√®le**. Comprendre ce qui affecte ou influence les pr√©dictions du mod√®le. Cela aide √† expliquer le comportement du mod√®le, ce qui est important pour la transparence et la responsabilit√©.

## üöÄ D√©fi

Pour √©viter que des pr√©judices ne soient introduits d√®s le d√©part, nous devrions :

- avoir une diversit√© de parcours et de perspectives parmi les personnes travaillant sur les syst√®mes
- investir dans des ensembles de donn√©es qui refl√®tent la diversit√© de notre soci√©t√©
- d√©velopper de meilleures m√©thodes tout au long du cycle de vie de l'apprentissage automatique pour d√©tecter et corriger l'IA responsable lorsqu'elle se produit

Pensez √† des sc√©narios r√©els o√π le manque de confiance dans un mod√®le est √©vident lors de la construction et de l'utilisation du mod√®le. Quoi d'autre devrions-nous consid√©rer ?

## [Quiz post-conf√©rence](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/6/)
## Revue & Auto-apprentissage

Dans cette le√ßon, vous avez appris quelques bases des concepts d'√©quit√© et d'in√©quit√© dans l'apprentissage automatique.

Regardez cet atelier pour approfondir les sujets :

- √Ä la recherche d'une IA responsable : Mettre les principes en pratique par Besmira Nushi, Mehrnoosh Sameki et Amit Sharma

[![Bo√Æte √† outils d'IA responsable : un cadre open-source pour construire une IA responsable](https://img.youtube.com/vi/tGgJCrA-MZU/0.jpg)](https://www.youtube.com/watch?v=tGgJCrA-MZU "Bo√Æte √† outils RAI : un cadre open-source pour construire une IA responsable")

> üé• Cliquez sur l'image ci-dessus pour une vid√©o : Bo√Æte √† outils RAI : un cadre open-source pour construire une IA responsable par Besmira Nushi, Mehrnoosh Sameki et Amit Sharma

Aussi, lisez :

- Centre de ressources RAI de Microsoft : [Ressources d'IA responsable ‚Äì Microsoft AI](https://www.microsoft.com/ai/responsible-ai-resources?activetab=pivot1%3aprimaryr4)

- Groupe de recherche FATE de Microsoft : [FATE : √âquit√©, Responsabilit√©, Transparence et √âthique dans l'IA - Microsoft Research](https://www.microsoft.com/research/theme/fate/)

Bo√Æte √† outils RAI :

- [D√©p√¥t GitHub de la bo√Æte √† outils d'IA responsable](https://github.com/microsoft/responsible-ai-toolbox)

Lisez √† propos des outils d'Azure Machine Learning pour garantir l'√©quit√© :

- [Azure Machine Learning](https://docs.microsoft.com/azure/machine-learning/concept-fairness-ml?WT.mc_id=academic-77952-leestott)

## Devoir

[Explorez la bo√Æte √† outils R

**Avertissement** :  
Ce document a √©t√© traduit √† l'aide de services de traduction automatique bas√©s sur l'IA. Bien que nous nous effor√ßons d'assurer l'exactitude, veuillez noter que les traductions automatiques peuvent contenir des erreurs ou des inexactitudes. Le document original dans sa langue natale doit √™tre consid√©r√© comme la source autoritaire. Pour des informations critiques, une traduction humaine professionnelle est recommand√©e. Nous ne sommes pas responsables des malentendus ou des interpr√©tations erron√©es r√©sultant de l'utilisation de cette traduction.