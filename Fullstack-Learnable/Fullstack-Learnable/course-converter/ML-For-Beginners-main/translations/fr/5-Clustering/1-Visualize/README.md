# Introduction au clustering

Le clustering est un type d'[Apprentissage Non Supervis√©](https://wikipedia.org/wiki/Unsupervised_learning) qui suppose qu'un ensemble de donn√©es n'est pas √©tiquet√© ou que ses entr√©es ne sont pas associ√©es √† des sorties pr√©d√©finies. Il utilise divers algorithmes pour trier des donn√©es non √©tiquet√©es et fournir des regroupements selon les motifs qu'il discernent dans les donn√©es.

[![No One Like You par PSquare](https://img.youtube.com/vi/ty2advRiWJM/0.jpg)](https://youtu.be/ty2advRiWJM "No One Like You par PSquare")

> üé• Cliquez sur l'image ci-dessus pour une vid√©o. Pendant que vous √©tudiez l'apprentissage machine avec le clustering, profitez de quelques morceaux de Dance Hall nig√©rian - c'est une chanson tr√®s bien not√©e de 2014 par PSquare.

## [Quiz avant le cours](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/27/)
### Introduction

Le [clustering](https://link.springer.com/referenceworkentry/10.1007%2F978-0-387-30164-8_124) est tr√®s utile pour l'exploration des donn√©es. Voyons s'il peut aider √† d√©couvrir des tendances et des motifs dans la mani√®re dont les auditeurs nig√©rians consomment la musique.

‚úÖ Prenez une minute pour r√©fl√©chir aux utilisations du clustering. Dans la vie r√©elle, le clustering se produit chaque fois que vous avez une pile de linge et que vous devez trier les v√™tements des membres de votre famille üß¶üëïüëñü©≤. En science des donn√©es, le clustering se produit lorsque l'on essaie d'analyser les pr√©f√©rences d'un utilisateur ou de d√©terminer les caract√©ristiques d'un ensemble de donn√©es non √©tiquet√©. Le clustering, d'une certaine mani√®re, aide √† donner un sens au chaos, comme un tiroir √† chaussettes.

[![Introduction au ML](https://img.youtube.com/vi/esmzYhuFnds/0.jpg)](https://youtu.be/esmzYhuFnds "Introduction au Clustering")

> üé• Cliquez sur l'image ci-dessus pour une vid√©o : John Guttag du MIT introduit le clustering.

Dans un cadre professionnel, le clustering peut √™tre utilis√© pour d√©terminer des choses comme la segmentation de march√©, par exemple, pour d√©terminer quels groupes d'√¢ge ach√®tent quels articles. Une autre utilisation serait la d√©tection d'anomalies, peut-√™tre pour d√©tecter une fraude √† partir d'un ensemble de donn√©es de transactions par carte de cr√©dit. Ou vous pourriez utiliser le clustering pour d√©terminer des tumeurs dans un lot de scans m√©dicaux.

‚úÖ R√©fl√©chissez une minute √† la fa√ßon dont vous pourriez avoir rencontr√© le clustering "dans la nature", dans un cadre bancaire, de commerce √©lectronique ou commercial.

> üéì Fait int√©ressant, l'analyse des clusters a vu le jour dans les domaines de l'anthropologie et de la psychologie dans les ann√©es 1930. Pouvez-vous imaginer comment cela aurait pu √™tre utilis√© ?

Alternativement, vous pourriez l'utiliser pour regrouper des r√©sultats de recherche - par liens d'achat, images ou avis, par exemple. Le clustering est utile lorsque vous avez un grand ensemble de donn√©es que vous souhaitez r√©duire et sur lequel vous souhaitez effectuer une analyse plus granulaire, de sorte que la technique puisse √™tre utilis√©e pour en apprendre davantage sur les donn√©es avant la construction d'autres mod√®les.

‚úÖ Une fois vos donn√©es organis√©es en clusters, vous leur assignez un identifiant de cluster, et cette technique peut √™tre utile pour pr√©server la confidentialit√© d'un ensemble de donn√©es ; vous pouvez plut√¥t faire r√©f√©rence √† un point de donn√©es par son identifiant de cluster, plut√¥t que par des donn√©es identifiables plus r√©v√©latrices. Pouvez-vous penser √† d'autres raisons pour lesquelles vous feriez r√©f√©rence √† un identifiant de cluster plut√¥t qu'√† d'autres √©l√©ments du cluster pour l'identifier ?

Approfondissez votre compr√©hension des techniques de clustering dans ce [module d'apprentissage](https://docs.microsoft.com/learn/modules/train-evaluate-cluster-models?WT.mc_id=academic-77952-leestott).

## Commencer avec le clustering

[Scikit-learn propose un large √©ventail](https://scikit-learn.org/stable/modules/clustering.html) de m√©thodes pour effectuer du clustering. Le type que vous choisissez d√©pendra de votre cas d'utilisation. Selon la documentation, chaque m√©thode a divers avantages. Voici un tableau simplifi√© des m√©thodes prises en charge par Scikit-learn et de leurs cas d'utilisation appropri√©s :

| Nom de la m√©thode                  | Cas d'utilisation                                                        |
| :---------------------------------- | :---------------------------------------------------------------------- |
| K-Means                             | usage g√©n√©ral, inductif                                                |
| Propagation d'affinit√©             | nombreux, clusters in√©gaux, inductif                                   |
| Mean-shift                          | nombreux, clusters in√©gaux, inductif                                   |
| Clustering spectral                 | peu, clusters √©gaux, transductif                                       |
| Clustering hi√©rarchique de Ward    | nombreux, clusters contraints, transductif                             |
| Clustering agglom√©ratif            | nombreux, distances non euclidiennes, transductif                      |
| DBSCAN                              | g√©om√©trie non plate, clusters in√©gaux, transductif                    |
| OPTICS                              | g√©om√©trie non plate, clusters in√©gaux avec densit√© variable, transductif |
| M√©langes gaussiens                  | g√©om√©trie plate, inductif                                             |
| BIRCH                               | grand ensemble de donn√©es avec des valeurs aberrantes, inductif        |

> üéì La fa√ßon dont nous cr√©ons des clusters a beaucoup √† voir avec la mani√®re dont nous regroupons les points de donn√©es. D√©composons un peu le vocabulaire :
>
> üéì ['Transductif' vs. 'inductif'](https://wikipedia.org/wiki/Transduction_(machine_learning))
> 
> L'inf√©rence transductive est d√©riv√©e de cas d'entra√Ænement observ√©s qui se rapportent √† des cas de test sp√©cifiques. L'inf√©rence inductive est d√©riv√©e de cas d'entra√Ænement qui se rapportent √† des r√®gles g√©n√©rales qui ne sont ensuite appliqu√©es qu'aux cas de test.
> 
> Un exemple : Imaginez que vous ayez un ensemble de donn√©es qui est seulement partiellement √©tiquet√©. Certaines choses sont des 'disques', certaines des 'cd', et certaines sont vides. Votre travail est de fournir des √©tiquettes pour les vides. Si vous choisissez une approche inductive, vous entra√Æneriez un mod√®le √† la recherche de 'disques' et de 'cd', et appliqueriez ces √©tiquettes √† vos donn√©es non √©tiquet√©es. Cette approche aura du mal √† classifier des choses qui sont en r√©alit√© des 'cassettes'. Une approche transductive, en revanche, g√®re ces donn√©es inconnues de mani√®re plus efficace car elle s'efforce de regrouper des √©l√©ments similaires ensemble puis applique une √©tiquette √† un groupe. Dans ce cas, les clusters pourraient refl√©ter des 'objets musicaux ronds' et des 'objets musicaux carr√©s'.
> 
> üéì ['G√©om√©trie non plate' vs. 'plate'](https://datascience.stackexchange.com/questions/52260/terminology-flat-geometry-in-the-context-of-clustering)
> 
> D√©riv√©e de la terminologie math√©matique, la g√©om√©trie non plate vs. plate fait r√©f√©rence √† la mesure des distances entre les points par des m√©thodes g√©om√©triques soit 'plates' ([Euclidienne](https://wikipedia.org/wiki/Euclidean_geometry)) soit 'non plates' (non euclidiennes).
>
> 'Plate' dans ce contexte fait r√©f√©rence √† la g√©om√©trie euclidienne (dont certaines parties sont enseign√©es comme 'g√©om√©trie plane'), et non plate fait r√©f√©rence √† la g√©om√©trie non euclidienne. Quel rapport la g√©om√©trie a-t-elle avec l'apprentissage machine ? Eh bien, en tant que deux domaines ancr√©s dans les math√©matiques, il doit y avoir un moyen commun de mesurer les distances entre les points dans les clusters, et cela peut √™tre fait de mani√®re 'plate' ou 'non plate', selon la nature des donn√©es. Les [distances euclidiennes](https://wikipedia.org/wiki/Euclidean_distance) sont mesur√©es comme la longueur d'un segment de ligne entre deux points. Les [distances non euclidiennes](https://wikipedia.org/wiki/Non-Euclidean_geometry) sont mesur√©es le long d'une courbe. Si vos donn√©es, visualis√©es, semblent ne pas exister sur un plan, vous pourriez avoir besoin d'utiliser un algorithme sp√©cialis√© pour les g√©rer.
>
![Infographie sur la g√©om√©trie plate vs non plate](../../../../translated_images/flat-nonflat.d1c8c6e2a96110c1d57fa0b72913f6aab3c245478524d25baf7f4a18efcde224.fr.png)
> Infographie par [Dasani Madipalli](https://twitter.com/dasani_decoded)
> 
> üéì ['Distances'](https://web.stanford.edu/class/cs345a/slides/12-clustering.pdf)
> 
> Les clusters sont d√©finis par leur matrice de distance, c'est-√†-dire les distances entre les points. Cette distance peut √™tre mesur√©e de plusieurs mani√®res. Les clusters euclidiens sont d√©finis par la moyenne des valeurs des points et contiennent un 'centro√Øde' ou point central. Les distances sont donc mesur√©es par rapport √† ce centro√Øde. Les distances non euclidiennes se r√©f√®rent aux 'clustroids', le point le plus proche des autres points. Les clustroids, √† leur tour, peuvent √™tre d√©finis de diff√©rentes mani√®res.
> 
> üéì ['Contraint'](https://wikipedia.org/wiki/Constrained_clustering)
> 
> Le [Clustering Contraint](https://web.cs.ucdavis.edu/~davidson/Publications/ICDMTutorial.pdf) introduit l'apprentissage 'semi-supervis√©' dans cette m√©thode non supervis√©e. Les relations entre les points sont signal√©es comme 'ne peuvent pas √™tre li√©es' ou 'doivent √™tre li√©es', de sorte que certaines r√®gles sont impos√©es √† l'ensemble de donn√©es.
>
> Un exemple : Si un algorithme est l√¢ch√© sur un lot de donn√©es non √©tiquet√©es ou semi-√©tiquet√©es, les clusters qu'il produit peuvent √™tre de mauvaise qualit√©. Dans l'exemple ci-dessus, les clusters pourraient regrouper des 'objets musicaux ronds', des 'objets musicaux carr√©s' et des 'objets triangulaires' et des 'biscuits'. Si des contraintes ou des r√®gles √† suivre sont donn√©es ("l'objet doit √™tre en plastique", "l'objet doit pouvoir produire de la musique"), cela peut aider √† 'contraindre' l'algorithme √† faire de meilleurs choix.
> 
> üéì 'Densit√©'
> 
> Les donn√©es qui sont 'bruyantes' sont consid√©r√©es comme 'denses'. Les distances entre les points dans chacun de ses clusters peuvent se r√©v√©ler, apr√®s examen, plus ou moins denses, ou 'encombr√©es', et donc ces donn√©es doivent √™tre analys√©es avec la m√©thode de clustering appropri√©e. [Cet article](https://www.kdnuggets.com/2020/02/understanding-density-based-clustering.html) d√©montre la diff√©rence entre l'utilisation des algorithmes de clustering K-Means et HDBSCAN pour explorer un ensemble de donn√©es bruyantes avec une densit√© de cluster in√©gale.

## Algorithmes de clustering

Il existe plus de 100 algorithmes de clustering, et leur utilisation d√©pend de la nature des donn√©es √† disposition. Discutons de certains des principaux :

- **Clustering hi√©rarchique**. Si un objet est class√© par sa proximit√© √† un objet voisin, plut√¥t qu'√† un plus √©loign√©, des clusters sont form√©s en fonction de la distance de leurs membres √† d'autres objets. Le clustering agglom√©ratif de Scikit-learn est hi√©rarchique.

   ![Infographie sur le clustering hi√©rarchique](../../../../translated_images/hierarchical.bf59403aa43c8c47493bfdf1cc25230f26e45f4e38a3d62e8769cd324129ac15.fr.png)
   > Infographie par [Dasani Madipalli](https://twitter.com/dasani_decoded)

- **Clustering par centro√Øde**. Cet algorithme populaire n√©cessite le choix de 'k', ou le nombre de clusters √† former, apr√®s quoi l'algorithme d√©termine le point central d'un cluster et regroupe les donn√©es autour de ce point. Le [clustering K-means](https://wikipedia.org/wiki/K-means_clustering) est une version populaire du clustering par centro√Øde. Le centre est d√©termin√© par la moyenne la plus proche, d'o√π le nom. La distance au cluster est minimis√©e.

   ![Infographie sur le clustering par centro√Øde](../../../../translated_images/centroid.097fde836cf6c9187d0b2033e9f94441829f9d86f4f0b1604dd4b3d1931aee34.fr.png)
   > Infographie par [Dasani Madipalli](https://twitter.com/dasani_decoded)

- **Clustering bas√© sur la distribution**. Bas√© sur la mod√©lisation statistique, le clustering bas√© sur la distribution se concentre sur la d√©termination de la probabilit√© qu'un point de donn√©es appartienne √† un cluster, et l'assigne en cons√©quence. Les m√©thodes de m√©lange gaussien appartiennent √† ce type.

- **Clustering bas√© sur la densit√©**. Les points de donn√©es sont assign√©s √† des clusters en fonction de leur densit√©, ou de leur regroupement autour les uns des autres. Les points de donn√©es √©loign√©s du groupe sont consid√©r√©s comme des valeurs aberrantes ou du bruit. DBSCAN, Mean-shift et OPTICS appartiennent √† ce type de clustering.

- **Clustering bas√© sur une grille**. Pour des ensembles de donn√©es multidimensionnels, une grille est cr√©√©e et les donn√©es sont divis√©es parmi les cellules de la grille, cr√©ant ainsi des clusters.

## Exercice - cluster vos donn√©es

Le clustering en tant que technique est grandement aid√© par une visualisation appropri√©e, alors commen√ßons par visualiser nos donn√©es musicales. Cet exercice nous aidera √† d√©cider quelle m√©thode de clustering nous devrions utiliser le plus efficacement pour la nature de ces donn√©es.

1. Ouvrez le fichier [_notebook.ipynb_](https://github.com/microsoft/ML-For-Beginners/blob/main/5-Clustering/1-Visualize/notebook.ipynb) dans ce dossier.

1. Importez le package `Seaborn` pour une bonne visualisation des donn√©es.

    ```python
    !pip install seaborn
    ```

1. Ajoutez les donn√©es des chansons depuis [_nigerian-songs.csv_](https://github.com/microsoft/ML-For-Beginners/blob/main/5-Clustering/data/nigerian-songs.csv). Chargez un dataframe avec des donn√©es sur les chansons. Pr√©parez-vous √† explorer ces donn√©es en important les biblioth√®ques et en affichant les donn√©es :

    ```python
    import matplotlib.pyplot as plt
    import pandas as pd
    
    df = pd.read_csv("../data/nigerian-songs.csv")
    df.head()
    ```

    V√©rifiez les premi√®res lignes de donn√©es :

    |     | name                     | album                        | artist              | artist_top_genre | release_date | length | popularity | danceability | acousticness | energy | instrumentalness | liveness | loudness | speechiness | tempo   | time_signature |
    | --- | ------------------------ | ---------------------------- | ------------------- | ---------------- | ------------ | ------ | ---------- | ------------ | ------------ | ------ | ---------------- | -------- | -------- | ----------- | ------- | -------------- |
    | 0   | Sparky                   | Mandy & The Jungle           | Cruel Santino       | alternative r&b  | 2019         | 144000 | 48         | 0.666        | 0.851        | 0.42   | 0.534            | 0.11     | -6.699   | 0.0829      | 133.015 | 5              |
    | 1   | shuga rush               | EVERYTHING YOU HEARD IS TRUE | Odunsi (The Engine) | afropop          | 2020         | 89488  | 30         | 0.71         | 0.0822       | 0.683  | 0.000169         | 0.101    | -5.64    | 0.36        | 129.993 | 3              |
    | 2   | LITT!                    | LITT!                        | AYL√ò                | indie r&b        | 2018         | 207758 | 40         | 0.836        | 0.272        | 0.564  | 0.000537         | 0.11     | -7.127   | 0.0424      | 130.005 | 4              |
    | 3   | Confident / Feeling Cool | Enjoy Your Life              | Lady Donli          | nigerian pop     | 2019         | 175135 | 14         | 0.894        | 0.798        | 0.611  | 0.000187         | 0.0964   | -4.961   | 0.113       | 111.087 | 4              |
    | 4   | wanted you               | rare.                        | Odunsi (The Engine) | afropop          | 2018         | 152049 | 25         | 0.702        | 0.116        | 0.833  | 0.91             | 0.348    | -6.044   | 0.0447      | 105.115 | 4              |

1. Obtenez des informations sur le dataframe en appelant `info()` :

    ```python
    df.info()
    ```

   La sortie devrait ressembler √† ceci :

    ```output
    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 530 entries, 0 to 529
    Data columns (total 16 columns):
     #   Column            Non-Null Count  Dtype  
    ---  ------            --------------  -----  
     0   name              530 non-null    object 
     1   album             530 non-null    object 
     2   artist            530 non-null    object 
     3   artist_top_genre  530 non-null    object 
     4   release_date      530 non-null    int64  
     5   length            530 non-null    int64  
     6   popularity        530 non-null    int64  
     7   danceability      530 non-null    float64
     8   acousticness      530 non-null    float64
     9   energy            530 non-null    float64
     10  instrumentalness  530 non-null    float64
     11  liveness          530 non-null    float64
     12  loudness          530 non-null    float64
     13  speechiness       530 non-null    float64
     14  tempo             530 non-null    float64
     15  time_signature    530 non-null    int64  
    dtypes: float64(8), int64(4), object(4)
    memory usage: 66.4+ KB
    ```

1. V√©rifiez les valeurs nulles, en appelant `isnull()` et en v√©rifiant que la somme est 0 :

    ```python
    df.isnull().sum()
    ```

    Tout semble bon :

    ```output
    name                0
    album               0
    artist              0
    artist_top_genre    0
    release_date        0
    length              0
    popularity          0
    danceability        0
    acousticness        0
    energy              0
    instrumentalness    0
    liveness            0
    loudness            0
    speechiness         0
    tempo               0
    time_signature      0
    dtype: int64
    ```

1. D√©crivez les donn√©es :

    ```python
    df.describe()
    ```

    |       | release_date | length      | popularity | danceability | acousticness | energy   | instrumentalness | liveness | loudness  | speechiness | tempo      | time_signature |
    | ----- | ------------ | ----------- | ---------- | ------------ | ------------ | -------- | ---------------- | -------- | --------- | ----------- | ---------- | -------------- |
    | count | 530          | 530         | 530        | 530          | 530          | 530      | 530              | 530      | 530       | 530         | 530        | 530            |
    | mean  | 2015.390566  | 222298.1698 | 17.507547  | 0.741619     | 0.265412     | 0.760623 | 0.016305         | 0.147308 | -4.953011 | 0.130748    | 116.487864 | 3.986792       |
    | std   | 3.131688     | 39696.82226 | 18.992212  | 0.117522     | 0.208342     | 0.148533 | 0.090321         | 0.123588 | 2.464186  | 0.092939    | 23.518601  | 0.333701       |
    | min   | 1998        
## [Quiz post-conf√©rence](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/28/)

## Revue et auto-apprentissage

Avant d'appliquer des algorithmes de clustering, comme nous l'avons appris, il est judicieux de comprendre la nature de votre ensemble de donn√©es. Lisez-en plus sur ce sujet [ici](https://www.kdnuggets.com/2019/10/right-clustering-algorithm.html)

[Cet article utile](https://www.freecodecamp.org/news/8-clustering-algorithms-in-machine-learning-that-all-data-scientists-should-know/) vous guide √† travers les diff√©rentes mani√®res dont divers algorithmes de clustering se comportent, selon les formes de donn√©es.

## Devoir

[Recherchez d'autres visualisations pour le clustering](assignment.md)

**Avertissement** :  
Ce document a √©t√© traduit √† l'aide de services de traduction automatis√©s bas√©s sur l'IA. Bien que nous nous effor√ßons d'assurer l'exactitude, veuillez noter que les traductions automatiques peuvent contenir des erreurs ou des inexactitudes. Le document original dans sa langue natale doit √™tre consid√©r√© comme la source autoritaire. Pour des informations critiques, une traduction humaine professionnelle est recommand√©e. Nous ne sommes pas responsables des malentendus ou des erreurs d'interpr√©tation r√©sultant de l'utilisation de cette traduction.