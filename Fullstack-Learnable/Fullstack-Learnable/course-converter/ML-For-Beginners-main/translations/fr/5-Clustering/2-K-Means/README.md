# K-Means clustering

## [Quiz avant le cours](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/29/)

Dans cette le√ßon, vous apprendrez comment cr√©er des clusters en utilisant Scikit-learn et le jeu de donn√©es de musique nig√©riane que vous avez import√© plus t√¥t. Nous couvrirons les bases de K-Means pour le clustering. Gardez √† l'esprit que, comme vous l'avez appris dans la le√ßon pr√©c√©dente, il existe de nombreuses fa√ßons de travailler avec des clusters et la m√©thode que vous utilisez d√©pend de vos donn√©es. Nous allons essayer K-Means car c'est la technique de clustering la plus courante. Commen√ßons !

Termes que vous apprendrez :

- Score de silhouette
- M√©thode du coude
- Inertie
- Variance

## Introduction

[K-Means Clustering](https://wikipedia.org/wiki/K-means_clustering) est une m√©thode d√©riv√©e du domaine du traitement du signal. Elle est utilis√©e pour diviser et partitionner des groupes de donn√©es en 'k' clusters en utilisant une s√©rie d'observations. Chaque observation vise √† regrouper un point de donn√©es donn√© le plus pr√®s de sa 'moyenne' la plus proche, ou le point central d'un cluster.

Les clusters peuvent √™tre visualis√©s sous forme de [diagrammes de Voronoi](https://wikipedia.org/wiki/Voronoi_diagram), qui incluent un point (ou 'graine') et sa r√©gion correspondante. 

![diagramme de voronoi](../../../../translated_images/voronoi.1dc1613fb0439b9564615eca8df47a4bcd1ce06217e7e72325d2406ef2180795.fr.png)

> infographie par [Jen Looper](https://twitter.com/jenlooper)

Le processus de clustering K-Means [s'ex√©cute en trois √©tapes](https://scikit-learn.org/stable/modules/clustering.html#k-means) :

1. L'algorithme s√©lectionne un nombre k de points centraux en √©chantillonnant √† partir du jeu de donn√©es. Apr√®s cela, il boucle :
    1. Il assigne chaque √©chantillon au centro√Øde le plus proche.
    2. Il cr√©e de nouveaux centro√Ødes en prenant la valeur moyenne de tous les √©chantillons assign√©s aux centro√Ødes pr√©c√©dents.
    3. Ensuite, il calcule la diff√©rence entre les nouveaux et anciens centro√Ødes et r√©p√®te jusqu'√† ce que les centro√Ødes soient stabilis√©s.

Un inconv√©nient de l'utilisation de K-Means est le fait que vous devrez √©tablir 'k', c'est-√†-dire le nombre de centro√Ødes. Heureusement, la 'm√©thode du coude' aide √† estimer une bonne valeur de d√©part pour 'k'. Vous allez l'essayer dans un instant.

## Pr√©requis

Vous travaillerez dans le fichier [_notebook.ipynb_](https://github.com/microsoft/ML-For-Beginners/blob/main/5-Clustering/2-K-Means/notebook.ipynb) de cette le√ßon qui inclut l'importation des donn√©es et le nettoyage pr√©liminaire que vous avez effectu√© dans la le√ßon pr√©c√©dente.

## Exercice - pr√©paration

Commencez par jeter un autre coup d'≈ìil aux donn√©es des chansons.

1. Cr√©ez un boxplot, en appelant `boxplot()` pour chaque colonne :

    ```python
    plt.figure(figsize=(20,20), dpi=200)
    
    plt.subplot(4,3,1)
    sns.boxplot(x = 'popularity', data = df)
    
    plt.subplot(4,3,2)
    sns.boxplot(x = 'acousticness', data = df)
    
    plt.subplot(4,3,3)
    sns.boxplot(x = 'energy', data = df)
    
    plt.subplot(4,3,4)
    sns.boxplot(x = 'instrumentalness', data = df)
    
    plt.subplot(4,3,5)
    sns.boxplot(x = 'liveness', data = df)
    
    plt.subplot(4,3,6)
    sns.boxplot(x = 'loudness', data = df)
    
    plt.subplot(4,3,7)
    sns.boxplot(x = 'speechiness', data = df)
    
    plt.subplot(4,3,8)
    sns.boxplot(x = 'tempo', data = df)
    
    plt.subplot(4,3,9)
    sns.boxplot(x = 'time_signature', data = df)
    
    plt.subplot(4,3,10)
    sns.boxplot(x = 'danceability', data = df)
    
    plt.subplot(4,3,11)
    sns.boxplot(x = 'length', data = df)
    
    plt.subplot(4,3,12)
    sns.boxplot(x = 'release_date', data = df)
    ```

    Ces donn√©es sont un peu bruyantes : en observant chaque colonne sous forme de boxplot, vous pouvez voir des valeurs aberrantes.

    ![valeurs aberrantes](../../../../translated_images/boxplots.8228c29dabd0f29227dd38624231a175f411f1d8d4d7c012cb770e00e4fdf8b6.fr.png)

Vous pourriez parcourir le jeu de donn√©es et supprimer ces valeurs aberrantes, mais cela rendrait les donn√©es plut√¥t minimales.

1. Pour l'instant, choisissez les colonnes que vous utiliserez pour votre exercice de clustering. Choisissez celles avec des plages similaires et encodez la colonne `artist_top_genre` en tant que donn√©es num√©riques :

    ```python
    from sklearn.preprocessing import LabelEncoder
    le = LabelEncoder()
    
    X = df.loc[:, ('artist_top_genre','popularity','danceability','acousticness','loudness','energy')]
    
    y = df['artist_top_genre']
    
    X['artist_top_genre'] = le.fit_transform(X['artist_top_genre'])
    
    y = le.transform(y)
    ```

1. Maintenant, vous devez choisir combien de clusters cibler. Vous savez qu'il y a 3 genres musicaux que nous avons extraits du jeu de donn√©es, alors essayons 3 :

    ```python
    from sklearn.cluster import KMeans
    
    nclusters = 3 
    seed = 0
    
    km = KMeans(n_clusters=nclusters, random_state=seed)
    km.fit(X)
    
    # Predict the cluster for each data point
    
    y_cluster_kmeans = km.predict(X)
    y_cluster_kmeans
    ```

Vous voyez un tableau imprim√© avec des clusters pr√©vus (0, 1 ou 2) pour chaque ligne du dataframe.

1. Utilisez ce tableau pour calculer un 'score de silhouette' :

    ```python
    from sklearn import metrics
    score = metrics.silhouette_score(X, y_cluster_kmeans)
    score
    ```

## Score de silhouette

Recherchez un score de silhouette plus proche de 1. Ce score varie de -1 √† 1, et si le score est 1, le cluster est dense et bien s√©par√© des autres clusters. Une valeur proche de 0 repr√©sente des clusters qui se chevauchent avec des √©chantillons tr√®s proches de la fronti√®re de d√©cision des clusters voisins. [(Source)](https://dzone.com/articles/kmeans-silhouette-score-explained-with-python-exam)

Notre score est **.53**, donc juste au milieu. Cela indique que nos donn√©es ne sont pas particuli√®rement bien adapt√©es √† ce type de clustering, mais continuons.

### Exercice - construire un mod√®le

1. Importez `KMeans` et commencez le processus de clustering.

    ```python
    from sklearn.cluster import KMeans
    wcss = []
    
    for i in range(1, 11):
        kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42)
        kmeans.fit(X)
        wcss.append(kmeans.inertia_)
    
    ```

    Il y a quelques parties ici qui m√©ritent d'√™tre expliqu√©es.

    > üéì range : Ce sont les it√©rations du processus de clustering

    > üéì random_state : "D√©termine la g√©n√©ration de nombres al√©atoires pour l'initialisation des centro√Ødes." [Source](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans)

    > üéì WCSS : "somme des carr√©s √† l'int√©rieur des clusters" mesure la distance moyenne au carr√© de tous les points au sein d'un cluster par rapport au centro√Øde du cluster. [Source](https://medium.com/@ODSC/unsupervised-learning-evaluating-clusters-bd47eed175ce). 

    > üéì Inertie : Les algorithmes K-Means tentent de choisir des centro√Ødes pour minimiser 'l'inertie', "une mesure de la coh√©rence interne des clusters." [Source](https://scikit-learn.org/stable/modules/clustering.html). La valeur est ajout√©e √† la variable wcss √† chaque it√©ration.

    > üéì k-means++ : Dans [Scikit-learn](https://scikit-learn.org/stable/modules/clustering.html#k-means), vous pouvez utiliser l'optimisation 'k-means++', qui "initialise les centro√Ødes pour √™tre (g√©n√©ralement) √©loign√©s les uns des autres, ce qui conduit probablement √† de meilleurs r√©sultats qu'une initialisation al√©atoire."

### M√©thode du coude

Auparavant, vous avez suppos√© que, parce que vous avez cibl√© 3 genres musicaux, vous devriez choisir 3 clusters. Mais est-ce vraiment le cas ?

1. Utilisez la 'm√©thode du coude' pour vous en assurer.

    ```python
    plt.figure(figsize=(10,5))
    sns.lineplot(x=range(1, 11), y=wcss, marker='o', color='red')
    plt.title('Elbow')
    plt.xlabel('Number of clusters')
    plt.ylabel('WCSS')
    plt.show()
    ```

    Utilisez la variable `wcss` que vous avez construite √† l'√©tape pr√©c√©dente pour cr√©er un graphique montrant o√π se trouve la 'flexion' dans le coude, ce qui indique le nombre optimal de clusters. Peut-√™tre que c'est **3** !

    ![m√©thode du coude](../../../../translated_images/elbow.72676169eed744ff03677e71334a16c6b8f751e9e716e3d7f40dd7cdef674cca.fr.png)

## Exercice - afficher les clusters

1. Essayez √† nouveau le processus, cette fois en d√©finissant trois clusters, et affichez les clusters sous forme de nuage de points :

    ```python
    from sklearn.cluster import KMeans
    kmeans = KMeans(n_clusters = 3)
    kmeans.fit(X)
    labels = kmeans.predict(X)
    plt.scatter(df['popularity'],df['danceability'],c = labels)
    plt.xlabel('popularity')
    plt.ylabel('danceability')
    plt.show()
    ```

1. V√©rifiez l'exactitude du mod√®le :

    ```python
    labels = kmeans.labels_
    
    correct_labels = sum(y == labels)
    
    print("Result: %d out of %d samples were correctly labeled." % (correct_labels, y.size))
    
    print('Accuracy score: {0:0.2f}'. format(correct_labels/float(y.size)))
    ```

    L'exactitude de ce mod√®le n'est pas tr√®s bonne, et la forme des clusters vous donne un indice sur la raison. 

    ![clusters](../../../../translated_images/clusters.b635354640d8e4fd4a49ef545495518e7be76172c97c13bd748f5b79f171f69a.fr.png)

    Ces donn√©es sont trop d√©s√©quilibr√©es, trop peu corr√©l√©es et il y a trop de variance entre les valeurs des colonnes pour bien se regrouper. En fait, les clusters qui se forment sont probablement fortement influenc√©s ou biais√©s par les trois cat√©gories de genre que nous avons d√©finies ci-dessus. Cela a √©t√© un processus d'apprentissage !

    Dans la documentation de Scikit-learn, vous pouvez voir qu'un mod√®le comme celui-ci, avec des clusters pas tr√®s bien d√©marqu√©s, a un probl√®me de 'variance' :

    ![mod√®les probl√©matiques](../../../../translated_images/problems.f7fb539ccd80608e1f35c319cf5e3ad1809faa3c08537aead8018c6b5ba2e33a.fr.png)
    > Infographie de Scikit-learn

## Variance

La variance est d√©finie comme "la moyenne des diff√©rences au carr√© par rapport √† la moyenne" [(Source)](https://www.mathsisfun.com/data/standard-deviation.html). Dans le contexte de ce probl√®me de clustering, cela fait r√©f√©rence aux donn√©es dont les nombres de notre jeu de donn√©es tendent √† diverger un peu trop de la moyenne. 

‚úÖ C'est un excellent moment pour r√©fl√©chir √† toutes les mani√®res dont vous pourriez corriger ce probl√®me. Ajuster un peu plus les donn√©es ? Utiliser d'autres colonnes ? Utiliser un algorithme diff√©rent ? Indice : Essayez [de normaliser vos donn√©es](https://www.mygreatlearning.com/blog/learning-data-science-with-k-means-clustering/) et testez d'autres colonnes.

> Essayez ce '[calculateur de variance](https://www.calculatorsoup.com/calculators/statistics/variance-calculator.php)' pour mieux comprendre le concept.

---

## üöÄD√©fi

Passez du temps avec ce notebook, en ajustant les param√®tres. Pouvez-vous am√©liorer l'exactitude du mod√®le en nettoyant davantage les donn√©es (en supprimant les valeurs aberrantes, par exemple) ? Vous pouvez utiliser des poids pour donner plus de poids √† certains √©chantillons de donn√©es. Que pouvez-vous faire d'autre pour cr√©er de meilleurs clusters ?

Indice : Essayez de normaliser vos donn√©es. Il y a du code comment√© dans le notebook qui ajoute une normalisation standard pour que les colonnes de donn√©es se ressemblent davantage en termes de plage. Vous constaterez que, bien que le score de silhouette diminue, la 'flexion' dans le graphique du coude s'adoucit. Cela est d√ª au fait que laisser les donn√©es non normalis√©es permet aux donn√©es avec moins de variance de porter plus de poids. Lisez un peu plus sur ce probl√®me [ici](https://stats.stackexchange.com/questions/21222/are-mean-normalization-and-feature-scaling-needed-for-k-means-clustering/21226#21226).

## [Quiz apr√®s le cours](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/30/)

## R√©vision & Auto-apprentissage

Jetez un ≈ìil √† un simulateur K-Means [comme celui-ci](https://user.ceng.metu.edu.tr/~akifakkus/courses/ceng574/k-means/). Vous pouvez utiliser cet outil pour visualiser des points de donn√©es d'exemple et d√©terminer ses centro√Ødes. Vous pouvez modifier l'al√©atoire des donn√©es, le nombre de clusters et le nombre de centro√Ødes. Cela vous aide-t-il √† comprendre comment les donn√©es peuvent √™tre regroup√©es ?

De plus, jetez un ≈ìil √† [ce document sur K-Means](https://stanford.edu/~cpiech/cs221/handouts/kmeans.html) de Stanford.

## Devoir

[Essayez diff√©rentes m√©thodes de clustering](assignment.md)

**Avertissement** :  
Ce document a √©t√© traduit √† l'aide de services de traduction automatique bas√©s sur l'IA. Bien que nous nous effor√ßons d'assurer l'exactitude, veuillez noter que les traductions automatiques peuvent contenir des erreurs ou des inexactitudes. Le document original dans sa langue native doit √™tre consid√©r√© comme la source autoritaire. Pour des informations critiques, une traduction humaine professionnelle est recommand√©e. Nous ne sommes pas responsables des malentendus ou des interpr√©tations erron√©es d√©coulant de l'utilisation de cette traduction.