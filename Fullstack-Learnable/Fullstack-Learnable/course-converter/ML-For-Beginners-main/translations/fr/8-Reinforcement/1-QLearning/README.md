## V√©rification de la politique

Puisque la Q-Table r√©pertorie l'¬´ attractivit√© ¬ª de chaque action √† chaque √©tat, il est assez facile de l'utiliser pour d√©finir la navigation efficace dans notre monde. Dans le cas le plus simple, nous pouvons s√©lectionner l'action correspondant √† la valeur la plus √©lev√©e de la Q-Table : (code block 9)

```python
def qpolicy_strict(m):
        x,y = m.human
        v = probs(Q[x,y])
        a = list(actions)[np.argmax(v)]
        return a

walk(m,qpolicy_strict)
```

> Si vous essayez le code ci-dessus plusieurs fois, vous remarquerez peut-√™tre qu'il "se bloque" parfois, et que vous devez appuyer sur le bouton STOP dans le notebook pour l'interrompre. Cela se produit car il peut y avoir des situations o√π deux √©tats "pointent" l'un vers l'autre en termes de valeur Q optimale, auquel cas les agents finissent par se d√©placer ind√©finiment entre ces √©tats.

## üöÄD√©fi

> **T√¢che 1 :** Modifiez le `walk` function to limit the maximum length of path by a certain number of steps (say, 100), and watch the code above return this value from time to time.

> **Task 2:** Modify the `walk` function so that it does not go back to the places where it has already been previously. This will prevent `walk` from looping, however, the agent can still end up being "trapped" in a location from which it is unable to escape.

## Navigation

A better navigation policy would be the one that we used during training, which combines exploitation and exploration. In this policy, we will select each action with a certain probability, proportional to the values in the Q-Table. This strategy may still result in the agent returning back to a position it has already explored, but, as you can see from the code below, it results in a very short average path to the desired location (remember that `print_statistics` pour ex√©cuter la simulation 100 fois : (code block 10)

```python
def qpolicy(m):
        x,y = m.human
        v = probs(Q[x,y])
        a = random.choices(list(actions),weights=v)[0]
        return a

print_statistics(qpolicy)
```

Apr√®s avoir ex√©cut√© ce code, vous devriez obtenir une longueur de chemin moyenne beaucoup plus petite qu'auparavant, dans la plage de 3 √† 6.

## Enqu√™te sur le processus d'apprentissage

Comme nous l'avons mentionn√©, le processus d'apprentissage est un √©quilibre entre exploration et exploitation des connaissances acquises sur la structure de l'espace probl√®me. Nous avons vu que les r√©sultats de l'apprentissage (la capacit√© √† aider un agent √† trouver un chemin court vers l'objectif) se sont am√©lior√©s, mais il est √©galement int√©ressant d'observer comment la longueur moyenne du chemin se comporte pendant le processus d'apprentissage :

Les apprentissages peuvent √™tre r√©sum√©s comme suit :

- **La longueur moyenne du chemin augmente**. Ce que nous voyons ici, c'est qu'au d√©but, la longueur moyenne du chemin augmente. Cela est probablement d√ª au fait que lorsque nous ne savons rien sur l'environnement, nous avons tendance √† nous retrouver coinc√©s dans de mauvais √©tats, comme l'eau ou le loup. √Ä mesure que nous en apprenons davantage et commen√ßons √† utiliser ces connaissances, nous pouvons explorer l'environnement plus longtemps, mais nous ne savons toujours pas tr√®s bien o√π se trouvent les pommes.

- **La longueur du chemin diminue, √† mesure que nous apprenons davantage**. Une fois que nous avons suffisamment appris, il devient plus facile pour l'agent d'atteindre l'objectif, et la longueur du chemin commence √† diminuer. Cependant, nous restons ouverts √† l'exploration, donc nous nous √©cartons souvent du meilleur chemin et explorons de nouvelles options, rendant le chemin plus long que l'optimal.

- **Augmentation brutale de la longueur**. Ce que nous observons √©galement sur ce graphique, c'est qu'√† un certain moment, la longueur a augment√© de mani√®re brutale. Cela indique la nature stochastique du processus, et que nous pouvons √† un moment "g√¢cher" les coefficients de la Q-Table en les √©crasant avec de nouvelles valeurs. Cela devrait id√©alement √™tre minimis√© en diminuant le taux d'apprentissage (par exemple, vers la fin de l'entra√Ænement, nous n'ajustons les valeurs de la Q-Table que d'une petite valeur).

Dans l'ensemble, il est important de se rappeler que le succ√®s et la qualit√© du processus d'apprentissage d√©pendent fortement des param√®tres, tels que le taux d'apprentissage, la d√©cote du taux d'apprentissage et le facteur d'actualisation. Ceux-ci sont souvent appel√©s **hyperparam√®tres**, pour les distinguer des **param√®tres**, que nous optimisons pendant l'entra√Ænement (par exemple, les coefficients de la Q-Table). Le processus de recherche des meilleures valeurs d'hyperparam√®tres est appel√© **optimisation des hyperparam√®tres**, et cela m√©rite un sujet √† part enti√®re.

## [Quiz post-lecture](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/46/)

## Devoir 
[Un monde plus r√©aliste](assignment.md)

**Avertissement** :  
Ce document a √©t√© traduit √† l'aide de services de traduction automatique bas√©s sur l'IA. Bien que nous nous effor√ßons d'assurer l'exactitude, veuillez noter que les traductions automatiques peuvent contenir des erreurs ou des inexactitudes. Le document original dans sa langue natale doit √™tre consid√©r√© comme la source autoris√©e. Pour des informations critiques, une traduction professionnelle par un humain est recommand√©e. Nous ne sommes pas responsables des malentendus ou des interpr√©tations erron√©es r√©sultant de l'utilisation de cette traduction.