# Introduction √† l'apprentissage par renforcement

L'apprentissage par renforcement, RL, est consid√©r√© comme l'un des paradigmes fondamentaux de l'apprentissage automatique, aux c√¥t√©s de l'apprentissage supervis√© et de l'apprentissage non supervis√©. Le RL est enti√®rement ax√© sur les d√©cisions : prendre les bonnes d√©cisions ou, du moins, apprendre de celles-ci.

Imaginez que vous avez un environnement simul√© comme le march√© boursier. Que se passe-t-il si vous imposez une r√©glementation donn√©e ? A-t-elle un effet positif ou n√©gatif ? Si quelque chose de n√©gatif se produit, vous devez prendre ce _renforcement n√©gatif_, en tirer des le√ßons et changer de cap. Si c'est un r√©sultat positif, vous devez capitaliser sur ce _renforcement positif_.

![peter and the wolf](../../../translated_images/peter.779730f9ba3a8a8d9290600dcf55f2e491c0640c785af7ac0d64f583c49b8864.fr.png)

> Peter et ses amis doivent √©chapper au loup affam√© ! Image par [Jen Looper](https://twitter.com/jenlooper)

## Sujet r√©gional : Pierre et le Loup (Russie)

[Pierre et le Loup](https://en.wikipedia.org/wiki/Peter_and_the_Wolf) est un conte musical √©crit par un compositeur russe [Sergei Prokofiev](https://en.wikipedia.org/wiki/Sergei_Prokofiev). C'est l'histoire du jeune pionnier Pierre, qui s'aventure courageusement hors de sa maison vers la clairi√®re pour chasser le loup. Dans cette section, nous allons entra√Æner des algorithmes d'apprentissage automatique qui aideront Pierre :

- **Explorer** les environs et construire une carte de navigation optimale
- **Apprendre** √† utiliser un skateboard et √† s'y √©quilibrer, afin de se d√©placer plus rapidement.

[![Pierre et le Loup](https://img.youtube.com/vi/Fmi5zHg4QSM/0.jpg)](https://www.youtube.com/watch?v=Fmi5zHg4QSM)

> üé• Cliquez sur l'image ci-dessus pour √©couter Pierre et le Loup de Prokofiev

## Apprentissage par renforcement

Dans les sections pr√©c√©dentes, vous avez vu deux exemples de probl√®mes d'apprentissage automatique :

- **Supervis√©**, o√π nous avons des ensembles de donn√©es qui sugg√®rent des solutions types au probl√®me que nous voulons r√©soudre. [Classification](../4-Classification/README.md) et [r√©gression](../2-Regression/README.md) sont des t√¢ches d'apprentissage supervis√©.
- **Non supervis√©**, o√π nous n'avons pas de donn√©es d'entra√Ænement √©tiquet√©es. L'exemple principal de l'apprentissage non supervis√© est [Clustering](../5-Clustering/README.md).

Dans cette section, nous allons vous introduire √† un nouveau type de probl√®me d'apprentissage qui ne n√©cessite pas de donn√©es d'entra√Ænement √©tiquet√©es. Il existe plusieurs types de tels probl√®mes :

- **[Apprentissage semi-supervis√©](https://wikipedia.org/wiki/Semi-supervised_learning)**, o√π nous avons beaucoup de donn√©es non √©tiquet√©es qui peuvent √™tre utilis√©es pour pr√©former le mod√®le.
- **[Apprentissage par renforcement](https://wikipedia.org/wiki/Reinforcement_learning)**, dans lequel un agent apprend comment se comporter en r√©alisant des exp√©riences dans un environnement simul√©.

### Exemple - jeu vid√©o

Supposons que vous souhaitiez apprendre √† un ordinateur √† jouer √† un jeu, comme les √©checs ou [Super Mario](https://wikipedia.org/wiki/Super_Mario). Pour que l'ordinateur puisse jouer √† un jeu, nous devons lui faire pr√©dire quel mouvement effectuer dans chacun des √©tats du jeu. Bien que cela puisse sembler √™tre un probl√®me de classification, ce n'est pas le cas - car nous n'avons pas d'ensemble de donn√©es avec des √©tats et des actions correspondantes. Bien que nous puissions avoir des donn√©es comme des parties d'√©checs existantes ou des enregistrements de joueurs jouant √† Super Mario, il est probable que ces donn√©es ne couvrent pas suffisamment un nombre assez large d'√©tats possibles.

Au lieu de chercher des donn√©es de jeu existantes, **l'apprentissage par renforcement** (RL) repose sur l'id√©e de *faire jouer l'ordinateur* de nombreuses fois et d'observer le r√©sultat. Ainsi, pour appliquer l'apprentissage par renforcement, nous avons besoin de deux choses :

- **Un environnement** et **un simulateur** qui nous permettent de jouer √† un jeu plusieurs fois. Ce simulateur d√©finirait toutes les r√®gles du jeu ainsi que les √©tats et actions possibles.

- **Une fonction de r√©compense**, qui nous indiquerait √† quel point nous avons bien jou√© √† chaque mouvement ou partie.

La principale diff√©rence entre les autres types d'apprentissage automatique et le RL est qu'en RL, nous ne savons g√©n√©ralement pas si nous gagnons ou perdons jusqu'√† ce que nous terminions le jeu. Ainsi, nous ne pouvons pas dire si un certain mouvement √† lui seul est bon ou non - nous ne recevons une r√©compense qu'√† la fin du jeu. Et notre objectif est de concevoir des algorithmes qui nous permettront de former un mod√®le dans des conditions d'incertitude. Nous allons apprendre un algorithme de RL appel√© **Q-learning**.

## Le√ßons

1. [Introduction √† l'apprentissage par renforcement et Q-Learning](1-QLearning/README.md)
2. [Utiliser un environnement de simulation gym](2-Gym/README.md)

## Cr√©dits

"Introduction √† l'apprentissage par renforcement" a √©t√© √©crit avec ‚ô•Ô∏è par [Dmitry Soshnikov](http://soshnikov.com)

**Avertissement** :  
Ce document a √©t√© traduit √† l'aide de services de traduction automatique bas√©s sur l'IA. Bien que nous nous effor√ßons d'assurer l'exactitude, veuillez noter que les traductions automatiques peuvent contenir des erreurs ou des inexactitudes. Le document original dans sa langue native doit √™tre consid√©r√© comme la source autoritaire. Pour des informations critiques, une traduction humaine professionnelle est recommand√©e. Nous ne sommes pas responsables des malentendus ou des erreurs d'interpr√©tation r√©sultant de l'utilisation de cette traduction.