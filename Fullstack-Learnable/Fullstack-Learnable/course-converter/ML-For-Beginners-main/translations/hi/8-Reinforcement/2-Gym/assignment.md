# ट्रेन माउंटेन कार

[OpenAI Gym](http://gym.openai.com) को इस तरह से डिज़ाइन किया गया है कि सभी परिवेश एक ही API प्रदान करते हैं - अर्थात् एक ही तरीके `reset`, `step` और `render`, और **क्रिया स्थान** और **अवलोकन स्थान** के समान अमूर्तता। इस प्रकार, न्यूनतम कोड परिवर्तनों के साथ विभिन्न परिवेशों के लिए समान सुदृढीकरण शिक्षण एल्गोरिदम को अनुकूलित करना संभव होना चाहिए।

## एक माउंटेन कार पर्यावरण

[Mountain Car environment](https://gym.openai.com/envs/MountainCar-v0/) में एक कार एक घाटी में फंसी होती है:
लक्ष्य है घाटी से बाहर निकलना और झंडा पकड़ना, प्रत्येक कदम पर निम्नलिखित क्रियाओं में से एक करके:

| मूल्य | अर्थ |
|---|---|
| 0 | बाईं ओर तेजी लाएं |
| 1 | तेजी न लाएं |
| 2 | दाईं ओर तेजी लाएं |

हालांकि, इस समस्या की मुख्य चाल यह है कि कार का इंजन एक ही पास में पहाड़ पर चढ़ने के लिए पर्याप्त मजबूत नहीं है। इसलिए, सफल होने का एकमात्र तरीका है गति बढ़ाने के लिए आगे-पीछे चलाना।

अवलोकन स्थान में केवल दो मान होते हैं:

| संख्या | अवलोकन  | न्यूनतम | अधिकतम |
|-----|--------------|-----|-----|
|  0  | कार की स्थिति | -1.2| 0.6 |
|  1  | कार की वेग | -0.07 | 0.07 |

माउंटेन कार के लिए इनाम प्रणाली काफी पेचीदा है:

 * यदि एजेंट ने पहाड़ के ऊपर झंडे तक पहुंच (स्थिति = 0.5) प्राप्त कर लिया है तो 0 का इनाम दिया जाता है।
 * यदि एजेंट की स्थिति 0.5 से कम है तो -1 का इनाम दिया जाता है।

एपिसोड समाप्त हो जाता है यदि कार की स्थिति 0.5 से अधिक है, या एपिसोड की लंबाई 200 से अधिक है।
## निर्देश

हमारे सुदृढीकरण शिक्षण एल्गोरिदम को माउंटेन कार समस्या को हल करने के लिए अनुकूलित करें। मौजूदा [notebook.ipynb](../../../../8-Reinforcement/2-Gym/notebook.ipynb) कोड से शुरू करें, नए पर्यावरण को प्रतिस्थापित करें, राज्य विवर्तनिकीकरण कार्यों को बदलें, और मौजूदा एल्गोरिदम को न्यूनतम कोड संशोधनों के साथ प्रशिक्षित करने का प्रयास करें। हाइपरपैरामीटर समायोजित करके परिणाम का अनुकूलन करें।

> **Note**: एल्गोरिदम को अभिसरण करने के लिए हाइपरपैरामीटर समायोजन की आवश्यकता हो सकती है। 
## रूब्रिक

| मानदंड | उत्कृष्ट | पर्याप्त | सुधार की आवश्यकता |
| -------- | --------- | -------- | ----------------- |
|          | Q-Learning एल्गोरिदम को सफलतापूर्वक CartPole उदाहरण से अनुकूलित किया गया है, न्यूनतम कोड संशोधनों के साथ, जो 200 कदमों के भीतर झंडा पकड़ने की समस्या को हल करने में सक्षम है। | इंटरनेट से एक नया Q-Learning एल्गोरिदम अपनाया गया है, लेकिन अच्छी तरह से प्रलेखित है; या मौजूदा एल्गोरिदम अपनाया गया है, लेकिन वांछित परिणाम नहीं प्राप्त करता है | छात्र किसी भी एल्गोरिदम को सफलतापूर्वक अपनाने में सक्षम नहीं था, लेकिन समाधान की ओर महत्वपूर्ण कदम उठाए हैं (राज्य विवर्तनिकीकरण, Q-Table डेटा संरचना, आदि को लागू किया है) |

**अस्वीकरण**:
यह दस्तावेज़ मशीन-आधारित एआई अनुवाद सेवाओं का उपयोग करके अनुवादित किया गया है। जबकि हम सटीकता के लिए प्रयासरत हैं, कृपया ध्यान दें कि स्वचालित अनुवादों में त्रुटियाँ या अशुद्धियाँ हो सकती हैं। मूल भाषा में दस्तावेज़ को आधिकारिक स्रोत माना जाना चाहिए। महत्वपूर्ण जानकारी के लिए, पेशेवर मानव अनुवाद की सिफारिश की जाती है। इस अनुवाद के उपयोग से उत्पन्न किसी भी गलतफहमी या गलत व्याख्या के लिए हम जिम्मेदार नहीं हैं।