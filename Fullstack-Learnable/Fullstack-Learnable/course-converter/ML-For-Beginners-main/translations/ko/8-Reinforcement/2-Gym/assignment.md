# 산악 자동차 훈련

[OpenAI Gym](http://gym.openai.com)은 모든 환경이 동일한 API, 즉 동일한 메소드 `reset`, `step` 및 `render`를 제공하도록 설계되었습니다. 또한 **행동 공간**과 **관찰 공간**의 동일한 추상화를 제공합니다. 따라서 동일한 강화 학습 알고리즘을 최소한의 코드 변경으로 다양한 환경에 적용할 수 있어야 합니다.

## 산악 자동차 환경

[산악 자동차 환경](https://gym.openai.com/envs/MountainCar-v0/)에는 계곡에 갇힌 자동차가 있습니다:
목표는 계곡을 빠져나가 깃발을 잡는 것입니다. 각 단계에서 다음 중 하나의 행동을 수행합니다:

| 값 | 의미 |
|---|---|
| 0 | 왼쪽으로 가속 |
| 1 | 가속하지 않음 |
| 2 | 오른쪽으로 가속 |

이 문제의 주요 트릭은 자동차 엔진이 한 번에 산을 오를 만큼 강력하지 않다는 것입니다. 따라서 성공하려면 앞뒤로 운전하여 모멘텀을 쌓는 것이 유일한 방법입니다.

관찰 공간은 단 두 가지 값으로 구성됩니다:

| 번호 | 관찰 항목  | 최소값 | 최대값 |
|-----|--------------|-----|-----|
|  0  | 자동차 위치 | -1.2| 0.6 |
|  1  | 자동차 속도 | -0.07 | 0.07 |

산악 자동차의 보상 시스템은 다소 까다롭습니다:

 * 에이전트가 산 정상에 있는 깃발(위치 = 0.5)에 도달하면 보상 0이 주어집니다.
 * 에이전트의 위치가 0.5 미만이면 보상 -1이 주어집니다.

에피소드는 자동차 위치가 0.5를 초과하거나 에피소드 길이가 200을 초과하면 종료됩니다.
## 지침

우리의 강화 학습 알고리즘을 산악 자동차 문제를 해결하도록 조정하십시오. 기존 [notebook.ipynb](../../../../8-Reinforcement/2-Gym/notebook.ipynb) 코드로 시작하여 새 환경을 대체하고 상태 이산화 함수를 변경하고 최소한의 코드 수정으로 기존 알고리즘을 훈련시키십시오. 하이퍼파라미터를 조정하여 결과를 최적화하십시오.

> **참고**: 알고리즘이 수렴하려면 하이퍼파라미터 조정이 필요할 수 있습니다.
## 평가 기준

| 기준 | 모범적 | 적절한 | 개선 필요 |
| -------- | --------- | -------- | ----------------- |
|          | Q-러닝 알고리즘이 최소한의 코드 수정으로 CartPole 예제에서 성공적으로 조정되어 200단계 이내에 깃발을 잡는 문제를 해결할 수 있습니다. | 새로운 Q-러닝 알고리즘이 인터넷에서 채택되었지만 잘 문서화되어 있거나 기존 알고리즘이 채택되었지만 원하는 결과에 도달하지 못함 | 학생이 성공적으로 알고리즘을 채택하지 못했지만 솔루션을 향해 상당한 단계를 밟았음(상태 이산화, Q-테이블 데이터 구조 등 구현) |

**면책 조항**:
이 문서는 기계 기반 AI 번역 서비스를 사용하여 번역되었습니다. 정확성을 위해 노력하고 있지만, 자동 번역에는 오류나 부정확한 내용이 포함될 수 있습니다. 원본 문서의 원어가 권위 있는 출처로 간주되어야 합니다. 중요한 정보에 대해서는 전문 인간 번역을 권장합니다. 이 번역의 사용으로 인해 발생하는 오해나 오역에 대해 당사는 책임을 지지 않습니다.