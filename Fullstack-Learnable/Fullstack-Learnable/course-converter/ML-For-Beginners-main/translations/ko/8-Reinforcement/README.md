# 강화 학습 소개

강화 학습(RL)은 지도 학습과 비지도 학습과 함께 기본적인 기계 학습 패러다임 중 하나로 여겨집니다. RL은 올바른 결정을 내리거나 최소한 그 결정에서 배우는 것과 관련이 있습니다.

예를 들어 주식 시장과 같은 시뮬레이션 환경이 있다고 상상해보세요. 특정 규제를 도입하면 어떤 일이 발생할까요? 긍정적인 효과가 있을까요, 부정적인 효과가 있을까요? 부정적인 일이 발생하면, 이 _부정적 강화_에서 배워서 방향을 바꿔야 합니다. 긍정적인 결과라면, 그 _긍정적 강화_를 바탕으로 더 나아가야 합니다.

![피터와 늑대](../../../translated_images/peter.779730f9ba3a8a8d9290600dcf55f2e491c0640c785af7ac0d64f583c49b8864.ko.png)

> 피터와 그의 친구들이 배고픈 늑대에게서 도망쳐야 해요! 이미지 제공: [Jen Looper](https://twitter.com/jenlooper)

## 지역 주제: 피터와 늑대 (러시아)

[피터와 늑대](https://en.wikipedia.org/wiki/Peter_and_the_Wolf)는 러시아 작곡가 [세르게이 프로코피예프](https://en.wikipedia.org/wiki/Sergei_Prokofiev)가 쓴 음악 동화입니다. 이 이야기는 용감한 소년 피터가 집을 나와 숲 속 공터에서 늑대를 쫓는 이야기입니다. 이 섹션에서는 피터를 도울 기계 학습 알고리즘을 훈련할 것입니다:

- 주변 지역을 **탐색**하고 최적의 내비게이션 지도를 작성합니다.
- 더 빠르게 이동하기 위해 스케이트보드를 타고 균형을 잡는 법을 **배웁니다**.

[![피터와 늑대](https://img.youtube.com/vi/Fmi5zHg4QSM/0.jpg)](https://www.youtube.com/watch?v=Fmi5zHg4QSM)

> 🎥 위 이미지를 클릭하여 프로코피예프의 피터와 늑대를 들어보세요

## 강화 학습

이전 섹션에서는 두 가지 기계 학습 문제의 예를 보았습니다:

- **지도 학습**은 우리가 해결하고자 하는 문제에 대한 샘플 솔루션을 제안하는 데이터셋을 가지고 있는 경우입니다. [분류](../4-Classification/README.md)와 [회귀](../2-Regression/README.md)는 지도 학습 과제입니다.
- **비지도 학습**은 라벨이 지정된 학습 데이터가 없는 경우입니다. 비지도 학습의 주요 예는 [클러스터링](../5-Clustering/README.md)입니다.

이 섹션에서는 라벨이 지정된 학습 데이터가 필요하지 않은 새로운 유형의 학습 문제를 소개할 것입니다. 이러한 문제에는 여러 유형이 있습니다:

- **[반지도 학습](https://wikipedia.org/wiki/Semi-supervised_learning)**은 라벨이 지정되지 않은 많은 데이터를 사용하여 모델을 사전 훈련할 수 있는 경우입니다.
- **[강화 학습](https://wikipedia.org/wiki/Reinforcement_learning)**은 에이전트가 시뮬레이션된 환경에서 실험을 수행하면서 행동하는 방법을 배우는 경우입니다.

### 예제 - 컴퓨터 게임

컴퓨터에게 체스나 [슈퍼 마리오](https://wikipedia.org/wiki/Super_Mario)와 같은 게임을 가르치고 싶다고 가정해보세요. 컴퓨터가 게임을 하려면 각 게임 상태에서 어떤 움직임을 취할지 예측해야 합니다. 이것은 분류 문제처럼 보일 수 있지만, 그렇지 않습니다. 왜냐하면 상태와 해당 행동을 포함하는 데이터셋이 없기 때문입니다. 기존의 체스 경기나 슈퍼 마리오를 플레이하는 플레이어의 기록과 같은 데이터가 있을 수 있지만, 그 데이터가 가능한 상태의 충분한 수를 충분히 포괄하지 못할 가능성이 큽니다.

기존의 게임 데이터를 찾는 대신, **강화 학습**(RL)은 *컴퓨터가 여러 번 게임을 하게 하고* 결과를 관찰하는 아이디어에 기반합니다. 따라서 강화 학습을 적용하려면 두 가지가 필요합니다:

- **환경**과 **시뮬레이터**는 여러 번 게임을 할 수 있게 해줍니다. 이 시뮬레이터는 모든 게임 규칙뿐만 아니라 가능한 상태와 행동을 정의합니다.

- **보상 함수**는 각 움직임이나 게임 동안 얼마나 잘했는지 알려줍니다.

다른 유형의 기계 학습과 RL의 주요 차이점은 RL에서는 일반적으로 게임이 끝날 때까지 우리가 이겼는지 졌는지 알 수 없다는 것입니다. 따라서 특정 움직임이 좋거나 나쁜지 단독으로 판단할 수 없으며, 게임이 끝날 때 보상을 받습니다. 우리의 목표는 불확실한 조건에서 모델을 훈련할 수 있는 알고리즘을 설계하는 것입니다. 우리는 **Q-learning**이라는 RL 알고리즘에 대해 배울 것입니다.

## 레슨

1. [강화 학습과 Q-Learning 소개](1-QLearning/README.md)
2. [Gym 시뮬레이션 환경 사용하기](2-Gym/README.md)

## 크레딧

"강화 학습 소개"는 [Dmitry Soshnikov](http://soshnikov.com) 가 ♥️를 담아 작성했습니다.

**면책 조항**:
이 문서는 기계 기반 AI 번역 서비스를 사용하여 번역되었습니다. 정확성을 위해 노력하지만 자동 번역에는 오류나 부정확성이 있을 수 있습니다. 원어로 작성된 원본 문서를 권위 있는 출처로 간주해야 합니다. 중요한 정보의 경우, 전문적인 인간 번역을 권장합니다. 이 번역 사용으로 인해 발생하는 오해나 오역에 대해 당사는 책임을 지지 않습니다.