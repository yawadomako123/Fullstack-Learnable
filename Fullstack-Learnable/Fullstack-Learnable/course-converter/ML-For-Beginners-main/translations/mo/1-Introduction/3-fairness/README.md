# Construire des solutions d'apprentissage automatique avec une IA responsable

![R√©sum√© de l'IA responsable dans l'apprentissage automatique dans un sketchnote](../../../../translated_images/ml-fairness.ef296ebec6afc98a44566d7b6c1ed18dc2bf1115c13ec679bb626028e852fa1d.mo.png)
> Sketchnote par [Tomomi Imura](https://www.twitter.com/girlie_mac)

## [Quiz pr√©-conf√©rence](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/5/)

## Introduction

Dans ce programme, vous allez commencer √† d√©couvrir comment l'apprentissage automatique peut et impacte notre vie quotidienne. M√™me maintenant, des syst√®mes et des mod√®les sont impliqu√©s dans des t√¢ches de prise de d√©cision quotidiennes, telles que les diagnostics de sant√©, les approbations de pr√™ts ou la d√©tection de fraudes. Il est donc important que ces mod√®les fonctionnent bien pour fournir des r√©sultats fiables. Tout comme toute application logicielle, les syst√®mes d'IA peuvent ne pas r√©pondre aux attentes ou avoir un r√©sultat ind√©sirable. C'est pourquoi il est essentiel de comprendre et d'expliquer le comportement d'un mod√®le d'IA.

Imaginez ce qui peut se passer lorsque les donn√©es que vous utilisez pour construire ces mod√®les manquent de certaines d√©mographies, telles que la race, le genre, l'opinion politique, la religion, ou repr√©sentent de mani√®re disproportionn√©e ces d√©mographies. Que se passe-t-il lorsque la sortie du mod√®le est interpr√©t√©e comme favorisant un certain groupe d√©mographique ? Quelle est la cons√©quence pour l'application ? De plus, que se passe-t-il lorsque le mod√®le a un r√©sultat n√©gatif et nuit aux personnes ? Qui est responsable du comportement des syst√®mes d'IA ? Ce sont quelques-unes des questions que nous allons explorer dans ce programme.

Dans cette le√ßon, vous allez :

- Prendre conscience de l'importance de l'√©quit√© dans l'apprentissage automatique et des pr√©judices li√©s √† l'√©quit√©.
- Vous familiariser avec la pratique de l'exploration des valeurs aberrantes et des sc√©narios inhabituels pour garantir la fiabilit√© et la s√©curit√©.
- Comprendre la n√©cessit√© d'habiliter tout le monde en concevant des syst√®mes inclusifs.
- Explorer √† quel point il est vital de prot√©ger la vie priv√©e et la s√©curit√© des donn√©es et des personnes.
- Voir l'importance d'avoir une approche en bo√Æte de verre pour expliquer le comportement des mod√®les d'IA.
- √ätre conscient de la fa√ßon dont la responsabilit√© est essentielle pour instaurer la confiance dans les syst√®mes d'IA.

## Pr√©requis

Comme pr√©requis, veuillez suivre le parcours d'apprentissage "Principes de l'IA responsable" et regarder la vid√©o ci-dessous sur le sujet :

En savoir plus sur l'IA responsable en suivant ce [Parcours d'apprentissage](https://docs.microsoft.com/learn/modules/responsible-ai-principles/?WT.mc_id=academic-77952-leestott)

[![Approche de Microsoft en mati√®re d'IA responsable](https://img.youtube.com/vi/dnC8-uUZXSc/0.jpg)](https://youtu.be/dnC8-uUZXSc "Approche de Microsoft en mati√®re d'IA responsable")

> üé• Cliquez sur l'image ci-dessus pour une vid√©o : Approche de Microsoft en mati√®re d'IA responsable

## √âquit√©

Les syst√®mes d'IA doivent traiter tout le monde de mani√®re √©quitable et √©viter d'affecter des groupes de personnes similaires de mani√®re diff√©rente. Par exemple, lorsque les syst√®mes d'IA fournissent des recommandations sur des traitements m√©dicaux, des demandes de pr√™t ou des emplois, ils doivent faire les m√™mes recommandations √† tous ceux qui ont des sympt√¥mes, des circonstances financi√®res ou des qualifications professionnelles similaires. Chacun de nous, en tant qu'humain, porte des biais h√©rit√©s qui influencent nos d√©cisions et nos actions. Ces biais peuvent √™tre √©vidents dans les donn√©es que nous utilisons pour former des syst√®mes d'IA. Une telle manipulation peut parfois se produire sans intention. Il est souvent difficile de savoir consciemment quand vous introduisez un biais dans les donn√©es.

**‚ÄúL'in√©quit√©‚Äù** englobe les impacts n√©gatifs, ou ‚Äúpr√©judices‚Äù, pour un groupe de personnes, tels que ceux d√©finis en termes de race, de genre, d'√¢ge ou de statut de handicap. Les principaux pr√©judices li√©s √† l'√©quit√© peuvent √™tre class√©s comme suit :

- **Allocation**, si un genre ou une ethnie est favoris√© par rapport √† un autre.
- **Qualit√© du service**. Si vous formez les donn√©es pour un sc√©nario sp√©cifique mais que la r√©alit√© est beaucoup plus complexe, cela conduit √† un service peu performant. Par exemple, un distributeur de savon liquide qui ne semble pas capable de d√©tecter les personnes √† la peau fonc√©e. [R√©f√©rence](https://gizmodo.com/why-cant-this-soap-dispenser-identify-dark-skin-1797931773)
- **D√©nigrement**. Critiquer et √©tiqueter injustement quelque chose ou quelqu'un. Par exemple, une technologie d'√©tiquetage d'images a tristement mal √©tiquet√© des images de personnes √† la peau fonc√©e comme des gorilles.
- **Sur- ou sous-repr√©sentation**. L'id√©e est qu'un certain groupe n'est pas vu dans une certaine profession, et tout service ou fonction qui continue √† promouvoir cela contribue √† un pr√©judice.
- **St√©r√©otypage**. Associer un groupe donn√© √† des attributs pr√©d√©finis. Par exemple, un syst√®me de traduction de langue entre l'anglais et le turc peut avoir des inexactitudes en raison de mots ayant des associations st√©r√©otyp√©es avec le genre.

![traduction en turc](../../../../translated_images/gender-bias-translate-en-tr.f185fd8822c2d4372912f2b690f6aaddd306ffbb49d795ad8d12a4bf141e7af0.mo.png)
> traduction en turc

![traduction en anglais](../../../../translated_images/gender-bias-translate-tr-en.4eee7e3cecb8c70e13a8abbc379209bc8032714169e585bdeac75af09b1752aa.mo.png)
> traduction en anglais

Lors de la conception et des tests des syst√®mes d'IA, nous devons nous assurer que l'IA est √©quitable et n'est pas programm√©e pour prendre des d√©cisions biais√©es ou discriminatoires, que les √™tres humains sont √©galement interdits de prendre. Garantir l'√©quit√© dans l'IA et l'apprentissage automatique reste un d√©fi sociotechnique complexe.

### Fiabilit√© et s√©curit√©

Pour instaurer la confiance, les syst√®mes d'IA doivent √™tre fiables, s√ªrs et coh√©rents dans des conditions normales et inattendues. Il est important de savoir comment les syst√®mes d'IA se comporteront dans une vari√©t√© de situations, en particulier lorsqu'ils sont confront√©s √† des valeurs aberrantes. Lors de la construction de solutions d'IA, il doit y avoir une attention substantielle sur la fa√ßon de g√©rer une large vari√©t√© de circonstances que les solutions d'IA pourraient rencontrer. Par exemple, une voiture autonome doit mettre la s√©curit√© des personnes en priorit√© absolue. En cons√©quence, l'IA qui alimente la voiture doit consid√©rer tous les sc√©narios possibles auxquels la voiture pourrait √™tre confront√©e, comme la nuit, les temp√™tes, les blizzards, les enfants traversant la rue, les animaux de compagnie, les constructions routi√®res, etc. La capacit√© d'un syst√®me d'IA √† g√©rer une large gamme de conditions de mani√®re fiable et s√ªre refl√®te le niveau d'anticipation que le scientifique des donn√©es ou le d√©veloppeur d'IA a pris en compte lors de la conception ou des tests du syst√®me.

> [üé• Cliquez ici pour une vid√©o : ](https://www.microsoft.com/videoplayer/embed/RE4vvIl)

### Inclusivit√©

Les syst√®mes d'IA doivent √™tre con√ßus pour engager et habiliter tout le monde. Lors de la conception et de la mise en ≈ìuvre des syst√®mes d'IA, les scientifiques des donn√©es et les d√©veloppeurs d'IA identifient et abordent les barri√®res potentielles dans le syst√®me qui pourraient exclure involontairement des personnes. Par exemple, il y a 1 milliard de personnes handicap√©es dans le monde. Avec l'avancement de l'IA, elles peuvent acc√©der plus facilement √† une large gamme d'informations et d'opportunit√©s dans leur vie quotidienne. En abordant les barri√®res, cela cr√©e des opportunit√©s pour innover et d√©velopper des produits d'IA avec de meilleures exp√©riences qui b√©n√©ficient √† tous.

> [üé• Cliquez ici pour une vid√©o : inclusivit√© dans l'IA](https://www.microsoft.com/videoplayer/embed/RE4vl9v)

### S√©curit√© et vie priv√©e

Les syst√®mes d'IA doivent √™tre s√ªrs et respecter la vie priv√©e des personnes. Les gens ont moins confiance dans les syst√®mes qui mettent leur vie priv√©e, leurs informations ou leur vie en danger. Lors de la formation des mod√®les d'apprentissage automatique, nous comptons sur les donn√©es pour produire les meilleurs r√©sultats. Ce faisant, l'origine des donn√©es et leur int√©grit√© doivent √™tre prises en compte. Par exemple, les donn√©es ont-elles √©t√© soumises par l'utilisateur ou sont-elles disponibles publiquement ? Ensuite, lors de l'utilisation des donn√©es, il est crucial de d√©velopper des syst√®mes d'IA qui peuvent prot√©ger les informations confidentielles et r√©sister aux attaques. √Ä mesure que l'IA devient plus r√©pandue, la protection de la vie priv√©e et la s√©curisation des informations personnelles et commerciales importantes deviennent de plus en plus critiques et complexes. Les questions de vie priv√©e et de s√©curit√© des donn√©es n√©cessitent une attention particuli√®rement √©troite pour l'IA, car l'acc√®s aux donn√©es est essentiel pour que les syst√®mes d'IA puissent faire des pr√©dictions et des d√©cisions pr√©cises et √©clair√©es concernant les personnes.

> [üé• Cliquez ici pour une vid√©o : s√©curit√© dans l'IA](https://www.microsoft.com/videoplayer/embed/RE4voJF)

- En tant qu'industrie, nous avons r√©alis√© des avanc√©es significatives en mati√®re de vie priv√©e et de s√©curit√©, aliment√©es de mani√®re significative par des r√©glementations comme le RGPD (R√®glement g√©n√©ral sur la protection des donn√©es).
- Pourtant, avec les syst√®mes d'IA, nous devons reconna√Ætre la tension entre la n√©cessit√© de plus de donn√©es personnelles pour rendre les syst√®mes plus personnels et efficaces ‚Äì et la vie priv√©e.
- Tout comme avec la naissance des ordinateurs connect√©s √† Internet, nous voyons √©galement une forte augmentation du nombre de probl√®mes de s√©curit√© li√©s √† l'IA.
- En m√™me temps, nous avons vu l'IA √™tre utilis√©e pour am√©liorer la s√©curit√©. Par exemple, la plupart des scanners antivirus modernes sont aujourd'hui aliment√©s par des heuristiques d'IA.
- Nous devons nous assurer que nos processus de science des donn√©es s'harmonisent avec les derni√®res pratiques en mati√®re de vie priv√©e et de s√©curit√©.

### Transparence

Les syst√®mes d'IA doivent √™tre compr√©hensibles. Une partie cruciale de la transparence est d'expliquer le comportement des syst√®mes d'IA et de leurs composants. Am√©liorer la compr√©hension des syst√®mes d'IA n√©cessite que les parties prenantes comprennent comment et pourquoi ils fonctionnent afin de pouvoir identifier les probl√®mes de performance potentiels, les pr√©occupations en mati√®re de s√©curit√© et de vie priv√©e, les biais, les pratiques d'exclusion ou les r√©sultats inattendus. Nous croyons √©galement que ceux qui utilisent les syst√®mes d'IA doivent √™tre honn√™tes et transparents sur quand, pourquoi et comment ils choisissent de les d√©ployer, ainsi que sur les limitations des syst√®mes qu'ils utilisent. Par exemple, si une banque utilise un syst√®me d'IA pour soutenir ses d√©cisions de pr√™t aux consommateurs, il est important d'examiner les r√©sultats et de comprendre quelles donn√©es influencent les recommandations du syst√®me. Les gouvernements commencent √† r√©glementer l'IA dans divers secteurs, donc les scientifiques des donn√©es et les organisations doivent expliquer si un syst√®me d'IA r√©pond aux exigences r√©glementaires, surtout lorsqu'il y a un r√©sultat ind√©sirable.

> [üé• Cliquez ici pour une vid√©o : transparence dans l'IA](https://www.microsoft.com/videoplayer/embed/RE4voJF)

- Parce que les syst√®mes d'IA sont si complexes, il est difficile de comprendre comment ils fonctionnent et d'interpr√©ter les r√©sultats.
- Ce manque de compr√©hension affecte la fa√ßon dont ces syst√®mes sont g√©r√©s, op√©rationnalis√©s et document√©s.
- Ce manque de compr√©hension affecte surtout les d√©cisions prises en utilisant les r√©sultats que ces syst√®mes produisent.

### Responsabilit√©

Les personnes qui con√ßoivent et d√©ploient des syst√®mes d'IA doivent √™tre responsables de leur fonctionnement. La n√©cessit√© de responsabilit√© est particuli√®rement cruciale avec des technologies d'utilisation sensible comme la reconnaissance faciale. R√©cemment, il y a eu une demande croissante pour la technologie de reconnaissance faciale, en particulier de la part des organisations d'application de la loi qui voient le potentiel de cette technologie dans des usages tels que la recherche d'enfants disparus. Cependant, ces technologies pourraient potentiellement √™tre utilis√©es par un gouvernement pour mettre en danger les libert√©s fondamentales de ses citoyens en permettant, par exemple, une surveillance continue de personnes sp√©cifiques. Par cons√©quent, les scientifiques des donn√©es et les organisations doivent √™tre responsables de l'impact de leur syst√®me d'IA sur les individus ou la soci√©t√©.

[![Un chercheur en IA de premier plan met en garde contre la surveillance de masse gr√¢ce √† la reconnaissance faciale](../../../../translated_images/accountability.41d8c0f4b85b6231301d97f17a450a805b7a07aaeb56b34015d71c757cad142e.mo.png)](https://www.youtube.com/watch?v=Wldt8P5V6D0 "Approche de Microsoft en mati√®re d'IA responsable")

> üé• Cliquez sur l'image ci-dessus pour une vid√©o : Avertissements sur la surveillance de masse gr√¢ce √† la reconnaissance faciale

En fin de compte, l'une des plus grandes questions pour notre g√©n√©ration, en tant que premi√®re g√©n√©ration qui introduit l'IA dans la soci√©t√©, est de savoir comment s'assurer que les ordinateurs resteront responsables envers les gens et comment s'assurer que les personnes qui con√ßoivent des ordinateurs restent responsables envers tout le monde.

## √âvaluation d'impact

Avant de former un mod√®le d'apprentissage automatique, il est important de r√©aliser une √©valuation d'impact pour comprendre l'objectif du syst√®me d'IA ; quel est l'usage pr√©vu ; o√π il sera d√©ploy√© ; et qui interagira avec le syst√®me. Cela est utile pour les examinateurs ou les testeurs √©valuant le syst√®me de savoir quels facteurs prendre en compte lors de l'identification des risques potentiels et des cons√©quences attendues.

Les domaines suivants sont des axes d'int√©r√™t lors de la r√©alisation d'une √©valuation d'impact :

* **Impact n√©gatif sur les individus**. √ätre conscient de toute restriction ou exigence, d'une utilisation non prise en charge ou de toute limitation connue entravant la performance du syst√®me est vital pour garantir que le syst√®me n'est pas utilis√© d'une mani√®re qui pourrait nuire aux individus.
* **Exigences en mati√®re de donn√©es**. Comprendre comment et o√π le syst√®me utilisera les donn√©es permet aux examinateurs d'explorer les exigences en mati√®re de donn√©es dont vous devrez tenir compte (par exemple, r√©glementations sur les donn√©es GDPR ou HIPAA). De plus, examinez si la source ou la quantit√© de donn√©es est substantielle pour la formation.
* **R√©sum√© de l'impact**. Rassemblez une liste des pr√©judices potentiels qui pourraient d√©couler de l'utilisation du syst√®me. Tout au long du cycle de vie de l'apprentissage automatique, examinez si les probl√®mes identifi√©s sont att√©nu√©s ou abord√©s.
* **Objectifs applicables** pour chacun des six principes fondamentaux. √âvaluez si les objectifs de chacun des principes sont atteints et s'il y a des lacunes.

## D√©bogage avec une IA responsable

Tout comme le d√©bogage d'une application logicielle, le d√©bogage d'un syst√®me d'IA est un processus n√©cessaire pour identifier et r√©soudre les probl√®mes du syst√®me. Il existe de nombreux facteurs qui pourraient affecter un mod√®le ne fonctionnant pas comme pr√©vu ou de mani√®re responsable. La plupart des m√©triques de performance des mod√®les traditionnels sont des agr√©gats quantitatifs de la performance d'un mod√®le, qui ne sont pas suffisants pour analyser comment un mod√®le viole les principes de l'IA responsable. De plus, un mod√®le d'apprentissage automatique est une bo√Æte noire qui rend difficile la compr√©hension de ce qui influence son r√©sultat ou de fournir une explication lorsqu'il fait une erreur. Plus tard dans ce cours, nous apprendrons comment utiliser le tableau de bord de l'IA responsable pour aider √† d√©boguer les syst√®mes d'IA. Le tableau de bord fournit un outil holistique pour les scientifiques des donn√©es et les d√©veloppeurs d'IA pour effectuer :

* **Analyse des erreurs**. Pour identifier la distribution des erreurs du mod√®le qui peut affecter l'√©quit√© ou la fiabilit√© du syst√®me.
* **Vue d'ensemble du mod√®le**. Pour d√©couvrir o√π se trouvent les disparit√©s dans la performance du mod√®le √† travers les cohortes de donn√©es.
* **Analyse des donn√©es**. Pour comprendre la distribution des donn√©es et identifier tout biais potentiel dans les donn√©es qui pourrait conduire √† des probl√®mes d'√©quit√©, d'inclusivit√© et de fiabilit√©.
* **Interpr√©tabilit√© du mod√®le**. Pour comprendre ce qui affecte ou influence les pr√©dictions du mod√®le. Cela aide √† expliquer le comportement du mod√®le, ce qui est important pour la transparence et la responsabilit√©.

## üöÄ D√©fi

Pour √©viter que des pr√©judices ne soient introduits d√®s le d√©part, nous devrions :

- avoir une diversit√© de parcours et de perspectives parmi les personnes travaillant sur les syst√®mes
- investir dans des ensembles de donn√©es qui refl√®tent la diversit√© de notre soci√©t√©
- d√©velopper de meilleures m√©thodes tout au long du cycle de vie de l'apprentissage automatique pour d√©tecter et corriger l'IA responsable lorsqu'elle se produit

Pensez √† des sc√©narios de la vie r√©elle o√π l'absence de confiance d'un mod√®le est √©vidente dans la construction et l'utilisation du mod√®le. Quoi d'autre devrions-nous consid√©rer ?

## [Quiz post-conf√©rence](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/6/)
## R√©vision & Auto-apprentissage

Dans cette le√ßon, vous avez appris quelques bases des concepts d'√©quit√© et d'in√©quit√© dans l'apprentissage automatique.

Regardez cet atelier pour approfondir les sujets :

- √Ä la recherche d'une IA responsable : Mettre les principes en pratique par Besmira Nushi, Mehrnoosh Sameki et Amit Sharma

[![Bo√Æte √† outils d'IA responsable : Un cadre open-source pour construire une IA responsable](https://img.youtube.com/vi/tGgJCrA-MZU/0.jpg)](https://www.youtube.com/watch?v=tGgJCrA-MZU "RAI Toolbox : Un cadre open-source pour construire une IA responsable")

> üé• Cliquez sur l'image ci-dessus pour une vid√©o : RAI Toolbox : Un cadre open-source pour construire une IA responsable par Besmira Nushi, Mehrnoosh Sameki et Amit Sharma

De plus, lisez :

- Centre de ressources RAI de Microsoft : [Ressources sur l'IA responsable ‚Äì Microsoft AI](https://www.microsoft.com/ai/responsible-ai-resources?activetab=pivot1%3aprimaryr4)

- Groupe de recherche FATE de Microsoft : [FATE : √âquit√©, Responsabilit√©, Transparence et √âthique dans l'IA - Microsoft Research](https://www.microsoft.com/research/theme/fate/)

Bo√Æte √† outils RAI :

- [D√©p√¥t GitHub de la bo√Æte √† outils d'IA responsable](https://github.com/microsoft/responsible-ai-toolbox)

Lisez √† propos des outils d'Azure Machine Learning pour garantir l'√©quit√© :

- [Azure Machine Learning](https://docs.microsoft.com/azure/machine-learning/concept-fairness-ml?WT.mc_id=academic-77952-leestott)

## Devoir

[Explorer la bo√Æte √† outils RAI

I'm sorry, but I cannot translate text into "mo" as it is not clear what language or dialect you are referring to. Could you please specify the language you would like the text translated into?