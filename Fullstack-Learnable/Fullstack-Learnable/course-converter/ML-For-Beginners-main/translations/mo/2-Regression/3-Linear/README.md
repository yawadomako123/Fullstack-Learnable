# B√¢tir un mod√®le de r√©gression avec Scikit-learn : r√©gression de quatre mani√®res

![Infographie sur la r√©gression lin√©aire vs polynomiale](../../../../translated_images/linear-polynomial.5523c7cb6576ccab0fecbd0e3505986eb2d191d9378e785f82befcf3a578a6e7.mo.png)
> Infographie par [Dasani Madipalli](https://twitter.com/dasani_decoded)
## [Quiz pr√©-cours](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/13/)

> ### [Cette le√ßon est disponible en R !](../../../../2-Regression/3-Linear/solution/R/lesson_3.html)
### Introduction 

Jusqu'√† pr√©sent, vous avez explor√© ce qu'est la r√©gression avec des donn√©es d'exemple tir√©es du jeu de donn√©es sur les prix des citrouilles que nous utiliserons tout au long de cette le√ßon. Vous les avez √©galement visualis√©es √† l'aide de Matplotlib.

Vous √™tes maintenant pr√™t √† approfondir la r√©gression pour le ML. Alors que la visualisation vous permet de donner un sens aux donn√©es, le v√©ritable pouvoir de l'apprentissage automatique provient de _l'entra√Ænement des mod√®les_. Les mod√®les sont entra√Æn√©s sur des donn√©es historiques pour capturer automatiquement les d√©pendances des donn√©es, et ils vous permettent de pr√©dire des r√©sultats pour de nouvelles donn√©es que le mod√®le n'a pas encore vues.

Dans cette le√ßon, vous en apprendrez davantage sur deux types de r√©gression : _r√©gression lin√©aire de base_ et _r√©gression polynomiale_, ainsi que sur certaines des math√©matiques sous-jacentes √† ces techniques. Ces mod√®les nous permettront de pr√©dire les prix des citrouilles en fonction de diff√©rentes donn√©es d'entr√©e. 

[![ML pour les d√©butants - Comprendre la r√©gression lin√©aire](https://img.youtube.com/vi/CRxFT8oTDMg/0.jpg)](https://youtu.be/CRxFT8oTDMg "ML pour les d√©butants - Comprendre la r√©gression lin√©aire")

> üé• Cliquez sur l'image ci-dessus pour un aper√ßu vid√©o court de la r√©gression lin√©aire.

> Tout au long de ce programme, nous supposons une connaissance minimale des math√©matiques et cherchons √† le rendre accessible aux √©tudiants venant d'autres domaines, alors faites attention aux notes, üßÆ aux appels, aux diagrammes et √† d'autres outils d'apprentissage pour aider √† la compr√©hension.

### Pr√©requis

Vous devriez maintenant √™tre familier avec la structure des donn√©es sur les citrouilles que nous examinons. Vous pouvez les trouver pr√©charg√©es et pr√©-nettoy√©es dans le fichier _notebook.ipynb_ de cette le√ßon. Dans le fichier, le prix des citrouilles est affich√© par boisseau dans un nouveau cadre de donn√©es. Assurez-vous de pouvoir ex√©cuter ces notebooks dans des noyaux dans Visual Studio Code.

### Pr√©paration

Pour rappel, vous chargez ces donn√©es afin de poser des questions √† leur sujet.

- Quand est le meilleur moment pour acheter des citrouilles ? 
- Quel prix puis-je attendre pour un cas de citrouilles miniatures ?
- Devrais-je les acheter dans des paniers de demi-boisseau ou par bo√Æte de 1 1/9 boisseau ?
Continuons √† explorer ces donn√©es.

Dans la le√ßon pr√©c√©dente, vous avez cr√©√© un cadre de donn√©es Pandas et l'avez rempli avec une partie du jeu de donn√©es original, standardisant les prix par boisseau. Ce faisant, vous n'avez cependant pu rassembler qu'environ 400 points de donn√©es et uniquement pour les mois d'automne.

Jetez un ≈ìil aux donn√©es que nous avons pr√©charg√©es dans le notebook accompagnant cette le√ßon. Les donn√©es sont pr√©charg√©es et un premier nuage de points est trac√© pour montrer les donn√©es par mois. Peut-√™tre pourrions-nous obtenir un peu plus de d√©tails sur la nature des donn√©es en les nettoyant davantage.

## Une ligne de r√©gression lin√©aire

Comme vous l'avez appris dans la le√ßon 1, l'objectif d'un exercice de r√©gression lin√©aire est de pouvoir tracer une ligne pour :

- **Montrer les relations entre les variables**. Montrer la relation entre les variables.
- **Faire des pr√©dictions**. Faire des pr√©dictions pr√©cises sur l'endroit o√π un nouveau point de donn√©es se situerait par rapport √† cette ligne. 
 
Il est typique de la **r√©gression des moindres carr√©s** de tracer ce type de ligne. Le terme 'moindres carr√©s' signifie que tous les points de donn√©es entourant la ligne de r√©gression sont mis au carr√© puis additionn√©s. Id√©alement, cette somme finale est aussi petite que possible, car nous voulons un faible nombre d'erreurs, ou `least-squares`.

Nous le faisons car nous voulons mod√©liser une ligne qui a la distance cumul√©e la plus faible par rapport √† tous nos points de donn√©es. Nous mettons √©galement les termes au carr√© avant de les additionner car nous nous pr√©occupons de leur magnitude plut√¥t que de leur direction.

> **üßÆ Montrez-moi les math√©matiques** 
> 
> Cette ligne, appel√©e la _ligne de meilleure ad√©quation_, peut √™tre exprim√©e par [une √©quation](https://en.wikipedia.org/wiki/Simple_linear_regression): 
> 
> ```
> Y = a + bX
> ```
>
> `X` is the 'explanatory variable'. `Y` is the 'dependent variable'. The slope of the line is `b` and `a` is the y-intercept, which refers to the value of `Y` when `X = 0`. 
>
>![calculate the slope](../../../../translated_images/slope.f3c9d5910ddbfcf9096eb5564254ba22c9a32d7acd7694cab905d29ad8261db3.mo.png)
>
> First, calculate the slope `b`. Infographic by [Jen Looper](https://twitter.com/jenlooper)
>
> In other words, and referring to our pumpkin data's original question: "predict the price of a pumpkin per bushel by month", `X` would refer to the price and `Y` would refer to the month of sale. 
>
>![complete the equation](../../../../translated_images/calculation.a209813050a1ddb141cdc4bc56f3af31e67157ed499e16a2ecf9837542704c94.mo.png)
>
> Calculate the value of Y. If you're paying around $4, it must be April! Infographic by [Jen Looper](https://twitter.com/jenlooper)
>
> The math that calculates the line must demonstrate the slope of the line, which is also dependent on the intercept, or where `Y` is situated when `X = 0`.
>
> You can observe the method of calculation for these values on the [Math is Fun](https://www.mathsisfun.com/data/least-squares-regression.html) web site. Also visit [this Least-squares calculator](https://www.mathsisfun.com/data/least-squares-calculator.html) to watch how the numbers' values impact the line.

## Correlation

One more term to understand is the **Correlation Coefficient** between given X and Y variables. Using a scatterplot, you can quickly visualize this coefficient. A plot with datapoints scattered in a neat line have high correlation, but a plot with datapoints scattered everywhere between X and Y have a low correlation.

A good linear regression model will be one that has a high (nearer to 1 than 0) Correlation Coefficient using the Least-Squares Regression method with a line of regression.

‚úÖ Run the notebook accompanying this lesson and look at the Month to Price scatterplot. Does the data associating Month to Price for pumpkin sales seem to have high or low correlation, according to your visual interpretation of the scatterplot? Does that change if you use more fine-grained measure instead of `Month`, eg. *day of the year* (i.e. number of days since the beginning of the year)?

In the code below, we will assume that we have cleaned up the data, and obtained a data frame called `new_pumpkins`, similar to the following:

ID | Month | DayOfYear | Variety | City | Package | Low Price | High Price | Price
---|-------|-----------|---------|------|---------|-----------|------------|-------
70 | 9 | 267 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 15.0 | 15.0 | 13.636364
71 | 9 | 267 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 18.0 | 18.0 | 16.363636
72 | 10 | 274 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 18.0 | 18.0 | 16.363636
73 | 10 | 274 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 17.0 | 17.0 | 15.454545
74 | 10 | 281 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 15.0 | 15.0 | 13.636364

> The code to clean the data is available in [`notebook.ipynb`](../../../../2-Regression/3-Linear/notebook.ipynb). We have performed the same cleaning steps as in the previous lesson, and have calculated `DayOfYear` colonne en utilisant l'expression suivante : 

```python
day_of_year = pd.to_datetime(pumpkins['Date']).apply(lambda dt: (dt-datetime(dt.year,1,1)).days)
```

Maintenant que vous avez une compr√©hension des math√©matiques derri√®re la r√©gression lin√©aire, cr√©ons un mod√®le de r√©gression pour voir si nous pouvons pr√©dire quel paquet de citrouilles aura les meilleurs prix. Quelqu'un qui ach√®te des citrouilles pour un champ de citrouilles de vacances pourrait vouloir cette information pour optimiser ses achats de paquets de citrouilles pour le champ.

## √Ä la recherche de corr√©lations

[![ML pour les d√©butants - √Ä la recherche de corr√©lations : La cl√© de la r√©gression lin√©aire](https://img.youtube.com/vi/uoRq-lW2eQo/0.jpg)](https://youtu.be/uoRq-lW2eQo "ML pour les d√©butants - √Ä la recherche de corr√©lations : La cl√© de la r√©gression lin√©aire")

> üé• Cliquez sur l'image ci-dessus pour un aper√ßu vid√©o court de la corr√©lation.

Dans la le√ßon pr√©c√©dente, vous avez probablement vu que le prix moyen pour diff√©rents mois ressemble √† ceci :

<img alt="Prix moyen par mois" src="../2-Data/images/barchart.png" width="50%"/>

Cela sugg√®re qu'il devrait y avoir une certaine corr√©lation, et nous pouvons essayer d'entra√Æner un mod√®le de r√©gression lin√©aire pour pr√©dire la relation entre la fonction `Month` and `Price`, or between `DayOfYear` and `Price`. Here is the scatter plot that shows the latter relationship:

<img alt="Scatter plot of Price vs. Day of Year" src="images/scatter-dayofyear.png" width="50%" /> 

Let's see if there is a correlation using the `corr` :

```python
print(new_pumpkins['Month'].corr(new_pumpkins['Price']))
print(new_pumpkins['DayOfYear'].corr(new_pumpkins['Price']))
```

Il semble que la corr√©lation soit assez faible, -0.15 par la fonction de trac√© `Month` and -0.17 by the `DayOfMonth`, but there could be another important relationship. It looks like there are different clusters of prices corresponding to different pumpkin varieties. To confirm this hypothesis, let's plot each pumpkin category using a different color. By passing an `ax` parameter to the `scatter`, nous pouvons tracer tous les points sur le m√™me graphique :

```python
ax=None
colors = ['red','blue','green','yellow']
for i,var in enumerate(new_pumpkins['Variety'].unique()):
    df = new_pumpkins[new_pumpkins['Variety']==var]
    ax = df.plot.scatter('DayOfYear','Price',ax=ax,c=colors[i],label=var)
```

<img alt="Nuage de points de Prix vs. Jour de l'ann√©e" src="images/scatter-dayofyear-color.png" width="50%" /> 

Notre enqu√™te sugg√®re que la vari√©t√© a plus d'effet sur le prix global que la date de vente r√©elle. Nous pouvons le voir avec un graphique √† barres :

```python
new_pumpkins.groupby('Variety')['Price'].mean().plot(kind='bar')
```

<img alt="Graphique √† barres de prix vs vari√©t√©" src="images/price-by-variety.png" width="50%" /> 

Concentrons-nous pour le moment uniquement sur une vari√©t√© de citrouille, la 'vari√©t√© √† tarte', et voyons quel effet la date a sur le prix :

```python
pie_pumpkins = new_pumpkins[new_pumpkins['Variety']=='PIE TYPE']
pie_pumpkins.plot.scatter('DayOfYear','Price') 
```
<img alt="Nuage de points de Prix vs. Jour de l'ann√©e" src="images/pie-pumpkins-scatter.png" width="50%" /> 

Si nous calculons maintenant la corr√©lation entre `Price` and `DayOfYear` using `corr` function, we will get something like `-0.27` - ce qui signifie que l'entra√Ænement d'un mod√®le pr√©dictif a du sens.

> Avant d'entra√Æner un mod√®le de r√©gression lin√©aire, il est important de s'assurer que nos donn√©es sont propres. La r√©gression lin√©aire ne fonctionne pas bien avec des valeurs manquantes, donc il est logique de se d√©barrasser de toutes les cellules vides :

```python
pie_pumpkins.dropna(inplace=True)
pie_pumpkins.info()
```

Une autre approche consisterait √† remplir ces valeurs vides avec les valeurs moyennes de la colonne correspondante.

## R√©gression lin√©aire simple

[![ML pour les d√©butants - R√©gression lin√©aire et polynomiale avec Scikit-learn](https://img.youtube.com/vi/e4c_UP2fSjg/0.jpg)](https://youtu.be/e4c_UP2fSjg "ML pour les d√©butants - R√©gression lin√©aire et polynomiale avec Scikit-learn")

> üé• Cliquez sur l'image ci-dessus pour un aper√ßu vid√©o court de la r√©gression lin√©aire et polynomiale.

Pour entra√Æner notre mod√®le de r√©gression lin√©aire, nous utiliserons la biblioth√®que **Scikit-learn**.

```python
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
```

Nous commen√ßons par s√©parer les valeurs d'entr√©e (caract√©ristiques) et la sortie attendue (√©tiquette) en tableaux numpy distincts :

```python
X = pie_pumpkins['DayOfYear'].to_numpy().reshape(-1,1)
y = pie_pumpkins['Price']
```

> Notez que nous avons d√ª effectuer `reshape` sur les donn√©es d'entr√©e afin que le paquet de r√©gression lin√©aire puisse les comprendre correctement. La r√©gression lin√©aire attend un tableau 2D comme entr√©e, o√π chaque ligne du tableau correspond √† un vecteur de caract√©ristiques d'entr√©e. Dans notre cas, puisque nous avons seulement une entr√©e - nous avons besoin d'un tableau avec une forme N√ó1, o√π N est la taille du jeu de donn√©es.

Ensuite, nous devons diviser les donn√©es en ensembles d'entra√Ænement et de test, afin que nous puissions valider notre mod√®le apr√®s l'entra√Ænement :

```python
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
```

Enfin, l'entra√Ænement du mod√®le de r√©gression lin√©aire r√©el ne prend que deux lignes de code. Nous d√©finissons la m√©thode `LinearRegression` object, and fit it to our data using the `fit` :

```python
lin_reg = LinearRegression()
lin_reg.fit(X_train,y_train)
```

Le `LinearRegression` object after `fit`-ting contains all the coefficients of the regression, which can be accessed using `.coef_` property. In our case, there is just one coefficient, which should be around `-0.017`. It means that prices seem to drop a bit with time, but not too much, around 2 cents per day. We can also access the intersection point of the regression with Y-axis using `lin_reg.intercept_` - it will be around `21` dans notre cas, indiquant le prix au d√©but de l'ann√©e.

Pour voir √† quel point notre mod√®le est pr√©cis, nous pouvons pr√©dire les prix sur un ensemble de test, puis mesurer √† quel point nos pr√©dictions sont proches des valeurs attendues. Cela peut √™tre fait en utilisant les m√©triques d'erreur quadratique moyenne (MSE), qui est la moyenne de toutes les diff√©rences au carr√© entre la valeur attendue et la valeur pr√©dite.

```python
pred = lin_reg.predict(X_test)

mse = np.sqrt(mean_squared_error(y_test,pred))
print(f'Mean error: {mse:3.3} ({mse/np.mean(pred)*100:3.3}%)')
```

Notre erreur semble √™tre d'environ 2 points, soit ~17 %. Pas tr√®s bon. Un autre indicateur de la qualit√© du mod√®le est le **coefficient de d√©termination**, qui peut √™tre obtenu comme ceci :

```python
score = lin_reg.score(X_train,y_train)
print('Model determination: ', score)
```
Si la valeur est 0, cela signifie que le mod√®le ne prend pas en compte les donn√©es d'entr√©e et agit comme le *pire pr√©dicteur lin√©aire*, qui est simplement une valeur moyenne du r√©sultat. La valeur de 1 signifie que nous pouvons pr√©dire parfaitement toutes les sorties attendues. Dans notre cas, le coefficient est d'environ 0.06, ce qui est assez faible.

Nous pouvons √©galement tracer les donn√©es de test avec la ligne de r√©gression pour mieux voir comment la r√©gression fonctionne dans notre cas :

```python
plt.scatter(X_test,y_test)
plt.plot(X_test,pred)
```

<img alt="R√©gression lin√©aire" src="images/linear-results.png" width="50%" />

## R√©gression polynomiale

Un autre type de r√©gression lin√©aire est la r√©gression polynomiale. Bien qu'il y ait parfois une relation lin√©aire entre les variables - plus la citrouille a un volume important, plus le prix est √©lev√© - parfois ces relations ne peuvent pas √™tre trac√©es comme un plan ou une ligne droite.

‚úÖ Voici [d'autres exemples](https://online.stat.psu.edu/stat501/lesson/9/9.8) de donn√©es qui pourraient utiliser la r√©gression polynomiale.

Regardez √† nouveau la relation entre la date et le prix. Ce nuage de points semble-t-il n√©cessairement √™tre analys√© par une ligne droite ? Les prix ne peuvent-ils pas fluctuer ? Dans ce cas, vous pouvez essayer la r√©gression polynomiale.

‚úÖ Les polyn√¥mes sont des expressions math√©matiques qui peuvent consister en une ou plusieurs variables et coefficients.

La r√©gression polynomiale cr√©e une ligne courbe pour mieux s'adapter aux donn√©es non lin√©aires. Dans notre cas, si nous incluons une variable `DayOfYear` au carr√© dans les donn√©es d'entr√©e, nous devrions √™tre en mesure d'adapter nos donn√©es avec une courbe parabolique, qui aura un minimum √† un certain point de l'ann√©e.

Scikit-learn inclut une [API de pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html?highlight=pipeline#sklearn.pipeline.make_pipeline) utile pour combiner diff√©rentes √©tapes de traitement des donn√©es ensemble. Un **pipeline** est une cha√Æne d'**estimateurs**. Dans notre cas, nous allons cr√©er un pipeline qui ajoute d'abord des caract√©ristiques polynomiales √† notre mod√®le, puis entra√Æne la r√©gression :

```python
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline

pipeline = make_pipeline(PolynomialFeatures(2), LinearRegression())

pipeline.fit(X_train,y_train)
```

En utilisant `PolynomialFeatures(2)` means that we will include all second-degree polynomials from the input data. In our case it will just mean `DayOfYear`<sup>2</sup>, but given two input variables X and Y, this will add X<sup>2</sup>, XY and Y<sup>2</sup>. We may also use higher degree polynomials if we want.

Pipelines can be used in the same manner as the original `LinearRegression` object, i.e. we can `fit` the pipeline, and then use `predict` to get the prediction results. Here is the graph showing test data, and the approximation curve:

<img alt="Polynomial regression" src="images/poly-results.png" width="50%" />

Using Polynomial Regression, we can get slightly lower MSE and higher determination, but not significantly. We need to take into account other features!

> You can see that the minimal pumpkin prices are observed somewhere around Halloween. How can you explain this? 

üéÉ Congratulations, you just created a model that can help predict the price of pie pumpkins. You can probably repeat the same procedure for all pumpkin types, but that would be tedious. Let's learn now how to take pumpkin variety into account in our model!

## Categorical Features

In the ideal world, we want to be able to predict prices for different pumpkin varieties using the same model. However, the `Variety` column is somewhat different from columns like `Month`, because it contains non-numeric values. Such columns are called **categorical**.

[![ML for beginners - Categorical Feature Predictions with Linear Regression](https://img.youtube.com/vi/DYGliioIAE0/0.jpg)](https://youtu.be/DYGliioIAE0 "ML for beginners - Categorical Feature Predictions with Linear Regression")

> üé• Click the image above for a short video overview of using categorical features.

Here you can see how average price depends on variety:

<img alt="Average price by variety" src="images/price-by-variety.png" width="50%" />

To take variety into account, we first need to convert it to numeric form, or **encode** it. There are several way we can do it:

* Simple **numeric encoding** will build a table of different varieties, and then replace the variety name by an index in that table. This is not the best idea for linear regression, because linear regression takes the actual numeric value of the index, and adds it to the result, multiplying by some coefficient. In our case, the relationship between the index number and the price is clearly non-linear, even if we make sure that indices are ordered in some specific way.
* **One-hot encoding** will replace the `Variety` column by 4 different columns, one for each variety. Each column will contain `1` if the corresponding row is of a given variety, and `0` sinon. Cela signifie qu'il y aura quatre coefficients dans la r√©gression lin√©aire, un pour chaque vari√©t√© de citrouille, responsables du "prix de d√©part" (ou plut√¥t du "prix suppl√©mentaire") pour cette vari√©t√© particuli√®re.

Le code ci-dessous montre comment nous pouvons encoder une vari√©t√© en one-hot :

```python
pd.get_dummies(new_pumpkins['Variety'])
```

 ID | FAIRYTALE | MINIATURE | VARI√âT√âS HEIRLOOM MIXTES | TYPE DE TARTE
----|-----------|-----------|--------------------------|----------
70 | 0 | 0 | 0 | 1
71 | 0 | 0 | 0 | 1
... | ... | ... | ... | ...
1738 | 0 | 1 | 0 | 0
1739 | 0 | 1 | 0 | 0
1740 | 0 | 1 | 0 | 0
1741 | 0 | 1 | 0 | 0
1742 | 0 | 1 | 0 | 0

Pour entra√Æner la r√©gression lin√©aire en utilisant la vari√©t√© encod√©e en one-hot comme entr√©e, nous devons simplement initialiser correctement les donn√©es `X` and `y` :

```python
X = pd.get_dummies(new_pumpkins['Variety'])
y = new_pumpkins['Price']
```

Le reste du code est le m√™me que celui que nous avons utilis√© ci-dessus pour entra√Æner la r√©gression lin√©aire. Si vous essayez, vous verrez que l'erreur quadratique moyenne est √† peu pr√®s la m√™me, mais nous obtenons un coefficient de d√©termination beaucoup plus √©lev√© (~77 %). Pour obtenir des pr√©dictions encore plus pr√©cises, nous pouvons prendre en compte davantage de caract√©ristiques cat√©gorielles, ainsi que des caract√©ristiques num√©riques, telles que `Month` or `DayOfYear`. To get one large array of features, we can use `join` :

```python
X = pd.get_dummies(new_pumpkins['Variety']) \
        .join(new_pumpkins['Month']) \
        .join(pd.get_dummies(new_pumpkins['City'])) \
        .join(pd.get_dummies(new_pumpkins['Package']))
y = new_pumpkins['Price']
```

Ici, nous prenons √©galement en compte le type de `City` and `Package`, ce qui nous donne une MSE de 2.84 (10 %), et une d√©termination de 0.94 !

## Mettre le tout ensemble

Pour cr√©er le meilleur mod√®le, nous pouvons utiliser des donn√©es combin√©es (cat√©gorielles encod√©es en one-hot + num√©riques) de l'exemple ci-dessus avec la r√©gression polynomiale. Voici le code complet pour votre commodit√© :

```python
# set up training data
X = pd.get_dummies(new_pumpkins['Variety']) \
        .join(new_pumpkins['Month']) \
        .join(pd.get_dummies(new_pumpkins['City'])) \
        .join(pd.get_dummies(new_pumpkins['Package']))
y = new_pumpkins['Price']

# make train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# setup and train the pipeline
pipeline = make_pipeline(PolynomialFeatures(2), LinearRegression())
pipeline.fit(X_train,y_train)

# predict results for test data
pred = pipeline.predict(X_test)

# calculate MSE and determination
mse = np.sqrt(mean_squared_error(y_test,pred))
print(f'Mean error: {mse:3.3} ({mse/np.mean(pred)*100:3.3}%)')

score = pipeline.score(X_train,y_train)
print('Model determination: ', score)
```

Cela devrait nous donner le meilleur coefficient de d√©termination d'environ 97 %, et une MSE=2.23 (~8 % d'erreur de pr√©diction).

| Mod√®le | MSE | D√©termination |
|-------|-----|---------------|
| `DayOfYear` Linear | 2.77 (17.2%) | 0.07 |
| `DayOfYear` Polynomial | 2.73 (17.0%) | 0.08 |
| `Variety` Lin√©aire | 5.24 (19.7 %) | 0.77 |
| Toutes les caract√©ristiques Lin√©aires | 2.84 (10.5 %) | 0.94 |
| Toutes les caract√©ristiques Polynomiales | 2.23 (8.25 %) | 0.97 |

üèÜ Bien jou√© ! Vous avez cr√©√© quatre mod√®les de r√©gression en une le√ßon et am√©lior√© la qualit√© du mod√®le √† 97 %. Dans la section finale sur la r√©gression, vous apprendrez la r√©gression logistique pour d√©terminer des cat√©gories. 

---
## üöÄD√©fi

Testez plusieurs variables diff√©rentes dans ce notebook pour voir comment la corr√©lation correspond √† la pr√©cision du mod√®le.

## [Quiz post-cours](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/14/)

## Revue & Auto-apprentissage

Dans cette le√ßon, nous avons appris sur la r√©gression lin√©aire. Il existe d'autres types importants de r√©gression. Lisez sur les techniques Stepwise, Ridge, Lasso et Elasticnet. Un bon cours √† √©tudier pour en apprendre davantage est le [cours de Stanford sur l'apprentissage statistique](https://online.stanford.edu/courses/sohs-ystatslearning-statistical-learning).

## Devoir 

[Construire un mod√®le](assignment.md)

I'm sorry, but I can't provide a translation to "mo" as it seems to refer to a language or dialect that isn't widely recognized. If you meant a specific language or dialect, please specify which one, and I'll do my best to assist you!