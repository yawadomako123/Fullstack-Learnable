# Introduction to reinforcement learning

Reinforcement learning, RL, est consid√©r√© comme l'un des paradigmes fondamentaux de l'apprentissage automatique, aux c√¥t√©s de l'apprentissage supervis√© et de l'apprentissage non supervis√©. L'RL est ax√© sur la prise de d√©cisions : fournir les bonnes d√©cisions ou, du moins, apprendre d'elles.

Imaginez que vous avez un environnement simul√© comme le march√© boursier. Que se passe-t-il si vous imposez une r√©glementation donn√©e ? A-t-elle un effet positif ou n√©gatif ? Si quelque chose de n√©gatif se produit, vous devez prendre ce _renforcement n√©gatif_, apprendre de cela, et changer de cap. Si le r√©sultat est positif, vous devez capitaliser sur ce _renforcement positif_.

![peter and the wolf](../../../translated_images/peter.779730f9ba3a8a8d9290600dcf55f2e491c0640c785af7ac0d64f583c49b8864.mo.png)

> Peter et ses amis doivent √©chapper au loup affam√© ! Image par [Jen Looper](https://twitter.com/jenlooper)

## Sujet r√©gional : Pierre et le Loup (Russie)

[Peter and the Wolf](https://en.wikipedia.org/wiki/Peter_and_the_Wolf) est un conte musical √©crit par un compositeur russe [Sergei Prokofiev](https://en.wikipedia.org/wiki/Sergei_Prokofiev). C'est l'histoire du jeune pionnier Pierre, qui sort courageusement de chez lui pour se rendre dans la clairi√®re de la for√™t afin de chasser le loup. Dans cette section, nous allons entra√Æner des algorithmes d'apprentissage automatique qui aideront Pierre :

- **Explorer** la zone environnante et construire une carte de navigation optimale
- **Apprendre** √† utiliser un skateboard et √† s'y √©quilibrer, afin de se d√©placer plus rapidement.

[![Peter and the Wolf](https://img.youtube.com/vi/Fmi5zHg4QSM/0.jpg)](https://www.youtube.com/watch?v=Fmi5zHg4QSM)

> üé• Cliquez sur l'image ci-dessus pour √©couter Pierre et le Loup de Prokofiev

## Apprentissage par renforcement

Dans les sections pr√©c√©dentes, vous avez vu deux exemples de probl√®mes d'apprentissage automatique :

- **Supervis√©**, o√π nous avons des ensembles de donn√©es qui sugg√®rent des solutions types au probl√®me que nous voulons r√©soudre. [Classification](../4-Classification/README.md) et [r√©gression](../2-Regression/README.md) sont des t√¢ches d'apprentissage supervis√©.
- **Non supervis√©**, dans lequel nous n'avons pas de donn√©es d'entra√Ænement √©tiquet√©es. L'exemple principal de l'apprentissage non supervis√© est [Clustering](../5-Clustering/README.md).

Dans cette section, nous allons vous pr√©senter un nouveau type de probl√®me d'apprentissage qui ne n√©cessite pas de donn√©es d'entra√Ænement √©tiquet√©es. Il existe plusieurs types de tels probl√®mes :

- **[Apprentissage semi-supervis√©](https://wikipedia.org/wiki/Semi-supervised_learning)**, o√π nous avons beaucoup de donn√©es non √©tiquet√©es qui peuvent √™tre utilis√©es pour pr√©former le mod√®le.
- **[Apprentissage par renforcement](https://wikipedia.org/wiki/Reinforcement_learning)**, dans lequel un agent apprend comment se comporter en r√©alisant des exp√©riences dans un environnement simul√©.

### Exemple - jeu vid√©o

Supposons que vous vouliez apprendre √† un ordinateur √† jouer √† un jeu, comme les √©checs, ou [Super Mario](https://wikipedia.org/wiki/Super_Mario). Pour que l'ordinateur puisse jouer √† un jeu, nous devons lui faire pr√©dire quel mouvement effectuer dans chacun des √©tats du jeu. Bien que cela puisse sembler √™tre un probl√®me de classification, ce n'est pas le cas - car nous n'avons pas un ensemble de donn√©es avec des √©tats et des actions correspondantes. Bien que nous puissions avoir certaines donn√©es comme des parties d'√©checs existantes ou des enregistrements de joueurs jouant √† Super Mario, il est probable que ces donn√©es ne couvrent pas suffisamment un nombre assez important d'√©tats possibles.

Au lieu de chercher des donn√©es de jeu existantes, **l'apprentissage par renforcement** (RL) repose sur l'id√©e de *faire jouer l'ordinateur* plusieurs fois et d'observer le r√©sultat. Ainsi, pour appliquer l'apprentissage par renforcement, nous avons besoin de deux choses :

- **Un environnement** et **un simulateur** qui nous permettent de jouer √† un jeu plusieurs fois. Ce simulateur d√©finirait toutes les r√®gles du jeu ainsi que les √©tats et actions possibles.

- **Une fonction de r√©compense**, qui nous indiquerait √† quel point nous avons bien agi √† chaque mouvement ou jeu.

La principale diff√©rence entre les autres types d'apprentissage automatique et l'RL est qu'en RL, nous ne savons g√©n√©ralement pas si nous gagnons ou perdons jusqu'√† ce que nous terminions le jeu. Ainsi, nous ne pouvons pas dire si un certain mouvement est bon ou non - nous ne recevons une r√©compense qu'√† la fin du jeu. Et notre objectif est de concevoir des algorithmes qui nous permettront d'entra√Æner un mod√®le dans des conditions incertaines. Nous allons apprendre un algorithme RL appel√© **Q-learning**.

## Le√ßons

1. [Introduction √† l'apprentissage par renforcement et au Q-Learning](1-QLearning/README.md)
2. [Utiliser un environnement de simulation de gym](2-Gym/README.md)

## Cr√©dits

"Introduction √† l'apprentissage par renforcement" a √©t√© √©crit avec ‚ô•Ô∏è par [Dmitry Soshnikov](http://soshnikov.com)

I'm sorry, but I cannot translate text into the "mo" language as it is not recognized as a specific language. If you meant a different language or dialect, please clarify, and I will do my best to assist you!