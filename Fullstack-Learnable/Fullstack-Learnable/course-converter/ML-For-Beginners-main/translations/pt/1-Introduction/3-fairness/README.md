# Construindo solu√ß√µes de Machine Learning com IA respons√°vel

![Resumo da IA respons√°vel em Machine Learning em um sketchnote](../../../../translated_images/ml-fairness.ef296ebec6afc98a44566d7b6c1ed18dc2bf1115c13ec679bb626028e852fa1d.pt.png)
> Sketchnote por [Tomomi Imura](https://www.twitter.com/girlie_mac)

## [Quiz pr√©-aula](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/5/)

## Introdu√ß√£o

Neste curr√≠culo, voc√™ come√ßar√° a descobrir como o machine learning pode e est√° impactando nossas vidas cotidianas. Mesmo agora, sistemas e modelos est√£o envolvidos em tarefas di√°rias de tomada de decis√£o, como diagn√≥sticos de sa√∫de, aprova√ß√µes de empr√©stimos ou detec√ß√£o de fraudes. Portanto, √© importante que esses modelos funcionem bem para fornecer resultados confi√°veis. Assim como qualquer aplica√ß√£o de software, os sistemas de IA podem n√£o atender √†s expectativas ou ter um resultado indesejado. Por isso, √© essencial entender e explicar o comportamento de um modelo de IA.

Imagine o que pode acontecer quando os dados que voc√™ est√° usando para construir esses modelos carecem de certas demografias, como ra√ßa, g√™nero, vis√£o pol√≠tica, religi√£o ou representam desproporcionalmente tais demografias. E quando a sa√≠da do modelo √© interpretada de forma a favorecer alguma demografia? Qual √© a consequ√™ncia para a aplica√ß√£o? Al√©m disso, o que acontece quando o modelo tem um resultado adverso e prejudica as pessoas? Quem √© respons√°vel pelo comportamento dos sistemas de IA? Essas s√£o algumas perguntas que exploraremos neste curr√≠culo.

Nesta li√ß√£o, voc√™ ir√°:

- Aumentar sua conscientiza√ß√£o sobre a import√¢ncia da equidade em machine learning e os danos relacionados √† equidade.
- Familiarizar-se com a pr√°tica de explorar outliers e cen√°rios incomuns para garantir confiabilidade e seguran√ßa.
- Compreender a necessidade de capacitar todos ao projetar sistemas inclusivos.
- Explorar como √© vital proteger a privacidade e a seguran√ßa de dados e pessoas.
- Ver a import√¢ncia de ter uma abordagem de caixa de vidro para explicar o comportamento dos modelos de IA.
- Estar ciente de como a responsabilidade √© essencial para construir confian√ßa em sistemas de IA.

## Pr√©-requisitos

Como pr√©-requisito, por favor, fa√ßa o "Caminho de Aprendizagem sobre Princ√≠pios de IA Respons√°vel" e assista ao v√≠deo abaixo sobre o tema:

Saiba mais sobre IA Respons√°vel seguindo este [Caminho de Aprendizagem](https://docs.microsoft.com/learn/modules/responsible-ai-principles/?WT.mc_id=academic-77952-leestott)

[![Abordagem da Microsoft para IA Respons√°vel](https://img.youtube.com/vi/dnC8-uUZXSc/0.jpg)](https://youtu.be/dnC8-uUZXSc "Abordagem da Microsoft para IA Respons√°vel")

> üé• Clique na imagem acima para assistir a um v√≠deo: Abordagem da Microsoft para IA Respons√°vel

## Equidade

Os sistemas de IA devem tratar todos de forma justa e evitar afetar grupos semelhantes de maneiras diferentes. Por exemplo, quando os sistemas de IA fornecem orienta√ß√µes sobre tratamentos m√©dicos, aplica√ß√µes de empr√©stimos ou emprego, eles devem fazer as mesmas recomenda√ß√µes a todos com sintomas, circunst√¢ncias financeiras ou qualifica√ß√µes profissionais semelhantes. Cada um de n√≥s, como seres humanos, carrega preconceitos herdados que afetam nossas decis√µes e a√ß√µes. Esses preconceitos podem ser evidentes nos dados que usamos para treinar sistemas de IA. Essa manipula√ß√£o pode, √†s vezes, ocorrer de forma n√£o intencional. Muitas vezes, √© dif√≠cil saber conscientemente quando voc√™ est√° introduzindo preconceito nos dados.

**‚ÄúInjusti√ßa‚Äù** abrange impactos negativos, ou ‚Äúdanos‚Äù, para um grupo de pessoas, como aqueles definidos em termos de ra√ßa, g√™nero, idade ou status de defici√™ncia. Os principais danos relacionados √† equidade podem ser classificados como:

- **Aloca√ß√£o**, se um g√™nero ou etnia, por exemplo, for favorecido em rela√ß√£o a outro.
- **Qualidade do servi√ßo**. Se voc√™ treinar os dados para um cen√°rio espec√≠fico, mas a realidade for muito mais complexa, isso leva a um servi√ßo de baixo desempenho. Por exemplo, um dispensador de sab√£o l√≠quido que n√£o parecia conseguir detectar pessoas com pele escura. [Refer√™ncia](https://gizmodo.com/why-cant-this-soap-dispenser-identify-dark-skin-1797931773)
- **Denigra√ß√£o**. Criticar e rotular injustamente algo ou algu√©m. Por exemplo, uma tecnologia de rotulagem de imagens infamemente rotulou erroneamente imagens de pessoas de pele escura como gorilas.
- **Super- ou sub-representa√ß√£o**. A ideia √© que um determinado grupo n√£o √© visto em uma determinada profiss√£o, e qualquer servi√ßo ou fun√ß√£o que continue promovendo isso est√° contribuindo para o dano.
- **Estereotipagem**. Associar um determinado grupo a atributos pr√©-designados. Por exemplo, um sistema de tradu√ß√£o de linguagem entre ingl√™s e turco pode ter imprecis√µes devido a palavras com associa√ß√µes estereotipadas de g√™nero.

![tradu√ß√£o para o turco](../../../../translated_images/gender-bias-translate-en-tr.f185fd8822c2d4372912f2b690f6aaddd306ffbb49d795ad8d12a4bf141e7af0.pt.png)
> tradu√ß√£o para o turco

![tradu√ß√£o de volta para o ingl√™s](../../../../translated_images/gender-bias-translate-tr-en.4eee7e3cecb8c70e13a8abbc379209bc8032714169e585bdeac75af09b1752aa.pt.png)
> tradu√ß√£o de volta para o ingl√™s

Ao projetar e testar sistemas de IA, precisamos garantir que a IA seja justa e n√£o programada para tomar decis√µes tendenciosas ou discriminat√≥rias, que os seres humanos tamb√©m est√£o proibidos de fazer. Garantir a equidade em IA e machine learning continua sendo um desafio sociot√©cnico complexo.

### Confiabilidade e seguran√ßa

Para construir confian√ßa, os sistemas de IA precisam ser confi√°veis, seguros e consistentes em condi√ß√µes normais e inesperadas. √â importante saber como os sistemas de IA se comportar√£o em uma variedade de situa√ß√µes, especialmente quando s√£o outliers. Ao construir solu√ß√µes de IA, deve haver uma quantidade substancial de foco em como lidar com uma ampla variedade de circunst√¢ncias que as solu√ß√µes de IA encontrariam. Por exemplo, um carro aut√¥nomo precisa colocar a seguran√ßa das pessoas como prioridade m√°xima. Como resultado, a IA que alimenta o carro precisa considerar todos os poss√≠veis cen√°rios que o carro poderia encontrar, como noite, tempestades ou nevascas, crian√ßas correndo pela rua, animais de estima√ß√£o, constru√ß√µes de estrada, etc. Qu√£o bem um sistema de IA pode lidar com uma ampla gama de condi√ß√µes de forma confi√°vel e segura reflete o n√≠vel de antecipa√ß√£o que o cientista de dados ou desenvolvedor de IA considerou durante o design ou teste do sistema.

> [üé• Clique aqui para um v√≠deo: ](https://www.microsoft.com/videoplayer/embed/RE4vvIl)

### Inclusividade

Os sistemas de IA devem ser projetados para envolver e capacitar todos. Ao projetar e implementar sistemas de IA, os cientistas de dados e desenvolvedores de IA identificam e abordam barreiras potenciais no sistema que poderiam excluir pessoas de forma n√£o intencional. Por exemplo, existem 1 bilh√£o de pessoas com defici√™ncia em todo o mundo. Com o avan√ßo da IA, elas podem acessar uma ampla gama de informa√ß√µes e oportunidades mais facilmente em suas vidas di√°rias. Ao abordar as barreiras, cria-se oportunidades para inovar e desenvolver produtos de IA com experi√™ncias melhores que beneficiem a todos.

> [üé• Clique aqui para um v√≠deo: inclus√£o em IA](https://www.microsoft.com/videoplayer/embed/RE4vl9v)

### Seguran√ßa e privacidade

Os sistemas de IA devem ser seguros e respeitar a privacidade das pessoas. As pessoas t√™m menos confian√ßa em sistemas que colocam sua privacidade, informa√ß√µes ou vidas em risco. Ao treinar modelos de machine learning, dependemos de dados para produzir os melhores resultados. Ao fazer isso, a origem dos dados e a integridade devem ser consideradas. Por exemplo, os dados foram enviados pelo usu√°rio ou estavam dispon√≠veis publicamente? Em seguida, ao trabalhar com os dados, √© crucial desenvolver sistemas de IA que possam proteger informa√ß√µes confidenciais e resistir a ataques. √Ä medida que a IA se torna mais prevalente, proteger a privacidade e garantir informa√ß√µes pessoais e empresariais importantes est√° se tornando cada vez mais cr√≠tico e complexo. Quest√µes de privacidade e seguran√ßa de dados exigem aten√ß√£o especial para IA, pois o acesso a dados √© essencial para que os sistemas de IA fa√ßam previs√µes e decis√µes precisas e informadas sobre as pessoas.

> [üé• Clique aqui para um v√≠deo: seguran√ßa em IA](https://www.microsoft.com/videoplayer/embed/RE4voJF)

- Como ind√∫stria, fizemos avan√ßos significativos em Privacidade e seguran√ßa, impulsionados significativamente por regulamenta√ß√µes como o GDPR (Regulamento Geral sobre a Prote√ß√£o de Dados).
- No entanto, com sistemas de IA, devemos reconhecer a tens√£o entre a necessidade de mais dados pessoais para tornar os sistemas mais pessoais e eficazes ‚Äì e a privacidade.
- Assim como com o surgimento de computadores conectados √† internet, tamb√©m estamos vendo um grande aumento no n√∫mero de problemas de seguran√ßa relacionados √† IA.
- Ao mesmo tempo, temos visto a IA sendo usada para melhorar a seguran√ßa. Como exemplo, a maioria dos scanners antiv√≠rus modernos √© impulsionada por heur√≠sticas de IA hoje.
- Precisamos garantir que nossos processos de Ci√™ncia de Dados se misturem harmoniosamente com as pr√°ticas mais recentes de privacidade e seguran√ßa.

### Transpar√™ncia

Os sistemas de IA devem ser compreens√≠veis. Uma parte crucial da transpar√™ncia √© explicar o comportamento dos sistemas de IA e seus componentes. Melhorar a compreens√£o dos sistemas de IA requer que as partes interessadas compreendam como e por que eles funcionam, para que possam identificar poss√≠veis problemas de desempenho, preocupa√ß√µes de seguran√ßa e privacidade, preconceitos, pr√°ticas excludentes ou resultados indesejados. Tamb√©m acreditamos que aqueles que usam sistemas de IA devem ser honestos e transparentes sobre quando, por que e como escolhem implant√°-los, bem como sobre as limita√ß√µes dos sistemas que usam. Por exemplo, se um banco usa um sistema de IA para apoiar suas decis√µes de empr√©stimos ao consumidor, √© importante examinar os resultados e entender quais dados influenciam as recomenda√ß√µes do sistema. Os governos est√£o come√ßando a regulamentar a IA em v√°rias ind√∫strias, ent√£o cientistas de dados e organiza√ß√µes devem explicar se um sistema de IA atende aos requisitos regulat√≥rios, especialmente quando h√° um resultado indesejado.

> [üé• Clique aqui para um v√≠deo: transpar√™ncia em IA](https://www.microsoft.com/videoplayer/embed/RE4voJF)

- Como os sistemas de IA s√£o t√£o complexos, √© dif√≠cil entender como eles funcionam e interpretar os resultados.
- Essa falta de compreens√£o afeta a forma como esses sistemas s√£o gerenciados, operacionalizados e documentados.
- Essa falta de compreens√£o, mais importante ainda, afeta as decis√µes tomadas com base nos resultados que esses sistemas produzem.

### Responsabilidade

As pessoas que projetam e implantam sistemas de IA devem ser respons√°veis por como seus sistemas operam. A necessidade de responsabilidade √© particularmente crucial com tecnologias de uso sens√≠vel, como o reconhecimento facial. Recentemente, houve uma demanda crescente por tecnologia de reconhecimento facial, especialmente de organiza√ß√µes de aplica√ß√£o da lei que veem o potencial da tecnologia em usos como encontrar crian√ßas desaparecidas. No entanto, essas tecnologias poderiam potencialmente ser usadas por um governo para colocar em risco as liberdades fundamentais de seus cidad√£os, por exemplo, permitindo a vigil√¢ncia cont√≠nua de indiv√≠duos espec√≠ficos. Portanto, cientistas de dados e organiza√ß√µes precisam ser respons√°veis por como seu sistema de IA impacta indiv√≠duos ou a sociedade.

[![Pesquisador l√≠der em IA alerta sobre vigil√¢ncia em massa atrav√©s do reconhecimento facial](../../../../translated_images/accountability.41d8c0f4b85b6231301d97f17a450a805b7a07aaeb56b34015d71c757cad142e.pt.png)](https://www.youtube.com/watch?v=Wldt8P5V6D0 "Abordagem da Microsoft para IA Respons√°vel")

> üé• Clique na imagem acima para assistir a um v√≠deo: Alertas sobre Vigil√¢ncia em Massa atrav√©s do Reconhecimento Facial

No final, uma das maiores perguntas para nossa gera√ß√£o, como a primeira gera√ß√£o que est√° trazendo a IA para a sociedade, √© como garantir que os computadores permane√ßam respons√°veis perante as pessoas e como garantir que as pessoas que projetam computadores permane√ßam respons√°veis perante todos os outros.

## Avalia√ß√£o de impacto

Antes de treinar um modelo de machine learning, √© importante realizar uma avalia√ß√£o de impacto para entender o prop√≥sito do sistema de IA; qual √© o uso pretendido; onde ser√° implantado; e quem estar√° interagindo com o sistema. Esses fatores s√£o √∫teis para o(s) revisor(es) ou testadores que avaliam o sistema saberem quais fatores considerar ao identificar riscos potenciais e consequ√™ncias esperadas.

As seguintes √°reas s√£o foco ao realizar uma avalia√ß√£o de impacto:

* **Impacto adverso sobre indiv√≠duos**. Estar ciente de qualquer restri√ß√£o ou requisito, uso n√£o suportado ou quaisquer limita√ß√µes conhecidas que impe√ßam o desempenho do sistema √© vital para garantir que o sistema n√£o seja usado de maneira que possa causar danos a indiv√≠duos.
* **Requisitos de dados**. Compreender como e onde o sistema usar√° dados permite que os revisores explorem quaisquer requisitos de dados dos quais voc√™ deve estar ciente (por exemplo, regulamenta√ß√µes de dados GDPR ou HIPPA). Al√©m disso, examine se a fonte ou a quantidade de dados √© substancial para o treinamento.
* **Resumo do impacto**. Re√∫na uma lista de danos potenciais que poderiam surgir do uso do sistema. Ao longo do ciclo de vida do ML, revise se os problemas identificados foram mitigados ou abordados.
* **Metas aplic√°veis** para cada um dos seis princ√≠pios fundamentais. Avalie se as metas de cada um dos princ√≠pios est√£o sendo atendidas e se h√° alguma lacuna.

## Depura√ß√£o com IA respons√°vel

Semelhante √† depura√ß√£o de uma aplica√ß√£o de software, depurar um sistema de IA √© um processo necess√°rio de identifica√ß√£o e resolu√ß√£o de problemas no sistema. Existem muitos fatores que podem afetar um modelo que n√£o est√° apresentando o desempenho esperado ou respons√°vel. A maioria das m√©tricas tradicionais de desempenho de modelos s√£o agregados quantitativos do desempenho de um modelo, que n√£o s√£o suficientes para analisar como um modelo viola os princ√≠pios de IA respons√°vel. Al√©m disso, um modelo de machine learning √© uma caixa preta que torna dif√≠cil entender o que impulsiona seu resultado ou fornecer explica√ß√µes quando comete um erro. Mais adiante neste curso, aprenderemos como usar o painel de IA Respons√°vel para ajudar a depurar sistemas de IA. O painel fornece uma ferramenta hol√≠stica para cientistas de dados e desenvolvedores de IA realizarem:

* **An√°lise de erros**. Para identificar a distribui√ß√£o de erros do modelo que pode afetar a equidade ou confiabilidade do sistema.
* **Vis√£o geral do modelo**. Para descobrir onde existem disparidades no desempenho do modelo entre coortes de dados.
* **An√°lise de dados**. Para entender a distribui√ß√£o dos dados e identificar qualquer potencial vi√©s nos dados que poderia levar a problemas de equidade, inclusividade e confiabilidade.
* **Interpretabilidade do modelo**. Para entender o que afeta ou influencia as previs√µes do modelo. Isso ajuda a explicar o comportamento do modelo, o que √© importante para a transpar√™ncia e responsabilidade.

## üöÄ Desafio

Para evitar que danos sejam introduzidos desde o in√≠cio, devemos:

- ter uma diversidade de origens e perspectivas entre as pessoas que trabalham em sistemas
- investir em conjuntos de dados que reflitam a diversidade de nossa sociedade
- desenvolver melhores m√©todos ao longo do ciclo de vida do machine learning para detectar e corrigir a IA respons√°vel quando ocorrer

Pense em cen√°rios da vida real onde a falta de confian√ßa em um modelo √© evidente na constru√ß√£o e uso do modelo. O que mais devemos considerar?

## [Quiz p√≥s-aula](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/6/)
## Revis√£o e Autoestudo

Nesta li√ß√£o, voc√™ aprendeu alguns conceitos b√°sicos sobre equidade e injusti√ßa em machine learning.

Assista a este workshop para se aprofundar nos t√≥picos:

- Em busca da IA respons√°vel: Colocando princ√≠pios em pr√°tica por Besmira Nushi, Mehrnoosh Sameki e Amit Sharma

[![IA Respons√°vel Toolbox: Uma estrutura de c√≥digo aberto para construir IA respons√°vel](https://img.youtube.com/vi/tGgJCrA-MZU/0.jpg)](https://www.youtube.com/watch?v=tGgJCrA-MZU "RAI Toolbox: Uma estrutura de c√≥digo aberto para construir IA respons√°vel")

> üé• Clique na imagem acima para assistir a um v√≠deo: RAI Toolbox: Uma estrutura de c√≥digo aberto para construir IA respons√°vel por Besmira Nushi, Mehrnoosh Sameki e Amit Sharma

Al√©m disso, leia:

- Centro de recursos RAI da Microsoft: [Recursos de IA Respons√°vel ‚Äì Microsoft AI](https://www.microsoft.com/ai/responsible-ai-resources?activetab=pivot1%3aprimaryr4)

- Grupo de pesquisa FATE da Microsoft: [FATE: Equidade, Responsabilidade, Transpar√™ncia e √âtica em IA - Microsoft Research](https://www.microsoft.com/research/theme/fate/)

RAI Toolbox:

- [Reposit√≥rio do GitHub da IA Respons√°vel Toolbox](https://github.com/microsoft/responsible-ai-toolbox)

Leia sobre as ferramentas do Azure Machine Learning para garantir equidade:

- [Azure Machine Learning](https://docs.microsoft.com/azure/machine-learning/concept-fairness-ml?WT.mc_id=academic-77952-leestott)

## Tarefa

[Explore a RAI Toolbox](assignment.md)

**Aviso**:  
Este documento foi traduzido utilizando servi√ßos de tradu√ß√£o autom√°tica baseados em IA. Embora nos esforcemos pela precis√£o, esteja ciente de que tradu√ß√µes autom√°ticas podem conter erros ou imprecis√µes. O documento original em seu idioma nativo deve ser considerado a fonte autorit√°ria. Para informa√ß√µes cr√≠ticas, recomenda-se a tradu√ß√£o profissional feita por humanos. N√£o nos responsabilizamos por quaisquer mal-entendidos ou interpreta√ß√µes err√¥neas decorrentes do uso desta tradu√ß√£o.