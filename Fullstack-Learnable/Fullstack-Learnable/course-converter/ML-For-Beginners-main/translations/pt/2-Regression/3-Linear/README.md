# Construa um modelo de regress√£o usando Scikit-learn: regress√£o de quatro maneiras

![Infogr√°fico sobre regress√£o linear vs polinomial](../../../../translated_images/linear-polynomial.5523c7cb6576ccab0fecbd0e3505986eb2d191d9378e785f82befcf3a578a6e7.pt.png)
> Infogr√°fico por [Dasani Madipalli](https://twitter.com/dasani_decoded)
## [Question√°rio pr√©-aula](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/13/)

> ### [Esta li√ß√£o est√° dispon√≠vel em R!](../../../../2-Regression/3-Linear/solution/R/lesson_3.html)
### Introdu√ß√£o 

At√© agora, voc√™ explorou o que √© regress√£o com dados de amostra coletados do conjunto de dados de pre√ßos de ab√≥bora que usaremos ao longo desta li√ß√£o. Voc√™ tamb√©m visualizou isso usando Matplotlib.

Agora voc√™ est√° pronto para mergulhar mais fundo na regress√£o para ML. Enquanto a visualiza√ß√£o permite que voc√™ compreenda os dados, o verdadeiro poder do Aprendizado de M√°quina vem do _treinamento de modelos_. Modelos s√£o treinados com dados hist√≥ricos para capturar automaticamente as depend√™ncias dos dados, e eles permitem que voc√™ preveja resultados para novos dados, que o modelo n√£o viu antes.

Nesta li√ß√£o, voc√™ aprender√° mais sobre dois tipos de regress√£o: _regress√£o linear b√°sica_ e _regress√£o polinomial_, junto com um pouco da matem√°tica subjacente a essas t√©cnicas. Esses modelos nos permitir√£o prever os pre√ßos das ab√≥boras dependendo de diferentes dados de entrada. 

[![ML para iniciantes - Compreendendo Regress√£o Linear](https://img.youtube.com/vi/CRxFT8oTDMg/0.jpg)](https://youtu.be/CRxFT8oTDMg "ML para iniciantes - Compreendendo Regress√£o Linear")

> üé• Clique na imagem acima para um breve v√≠deo sobre regress√£o linear.

> Ao longo deste curr√≠culo, assumimos conhecimento m√≠nimo de matem√°tica e buscamos torn√°-la acess√≠vel para estudantes de outras √°reas, ent√£o fique atento a notas, üßÆ destaques, diagramas e outras ferramentas de aprendizado para ajudar na compreens√£o.

### Pr√©-requisitos

Voc√™ deve estar familiarizado agora com a estrutura dos dados de ab√≥bora que estamos examinando. Voc√™ pode encontr√°-los pr√©-carregados e pr√©-limpos no arquivo _notebook.ipynb_ desta li√ß√£o. No arquivo, o pre√ßo da ab√≥bora √© exibido por alqueire em um novo DataFrame. Certifique-se de que voc√™ pode executar esses notebooks em kernels no Visual Studio Code.

### Prepara√ß√£o

Como lembrete, voc√™ est√° carregando esses dados para poder fazer perguntas sobre eles. 

- Quando √© o melhor momento para comprar ab√≥boras? 
- Que pre√ßo posso esperar de uma caixa de ab√≥boras em miniatura?
- Devo compr√°-las em cestos de meia alqueire ou pela caixa de 1 1/9 alqueire?
Vamos continuar explorando esses dados.

Na li√ß√£o anterior, voc√™ criou um DataFrame do Pandas e o preencheu com parte do conjunto de dados original, padronizando os pre√ßos por alqueire. No entanto, ao fazer isso, voc√™ conseguiu reunir apenas cerca de 400 pontos de dados e apenas para os meses de outono. 

D√™ uma olhada nos dados que pr√©-carregamos no notebook que acompanha esta li√ß√£o. Os dados est√£o pr√©-carregados e um gr√°fico de dispers√£o inicial √© tra√ßado para mostrar os dados mensais. Talvez possamos obter um pouco mais de detalhe sobre a natureza dos dados limpando-os mais.

## Uma linha de regress√£o linear

Como voc√™ aprendeu na Li√ß√£o 1, o objetivo de um exerc√≠cio de regress√£o linear √© ser capaz de tra√ßar uma linha para:

- **Mostrar rela√ß√µes vari√°veis**. Mostrar a rela√ß√£o entre vari√°veis
- **Fazer previs√µes**. Fazer previs√µes precisas sobre onde um novo ponto de dados se encaixaria em rela√ß√£o a essa linha. 
 
√â t√≠pico da **Regress√£o de M√≠nimos Quadrados** desenhar esse tipo de linha. O termo 'm√≠nimos quadrados' significa que todos os pontos de dados em torno da linha de regress√£o s√£o elevados ao quadrado e, em seguida, somados. Idealmente, essa soma final √© a menor poss√≠vel, porque queremos um baixo n√∫mero de erros, ou `least-squares`. 

Fazemos isso porque queremos modelar uma linha que tenha a menor dist√¢ncia cumulativa de todos os nossos pontos de dados. Tamb√©m elevamos os termos ao quadrado antes de som√°-los, pois estamos preocupados com sua magnitude em vez de sua dire√ß√£o.

> **üßÆ Mostre-me a matem√°tica** 
> 
> Esta linha, chamada de _linha de melhor ajuste_, pode ser expressa por [uma equa√ß√£o](https://en.wikipedia.org/wiki/Simple_linear_regression): 
> 
> ```
> Y = a + bX
> ```
>
> `X` is the 'explanatory variable'. `Y` is the 'dependent variable'. The slope of the line is `b` and `a` is the y-intercept, which refers to the value of `Y` when `X = 0`. 
>
>![calculate the slope](../../../../translated_images/slope.f3c9d5910ddbfcf9096eb5564254ba22c9a32d7acd7694cab905d29ad8261db3.pt.png)
>
> First, calculate the slope `b`. Infographic by [Jen Looper](https://twitter.com/jenlooper)
>
> In other words, and referring to our pumpkin data's original question: "predict the price of a pumpkin per bushel by month", `X` would refer to the price and `Y` would refer to the month of sale. 
>
>![complete the equation](../../../../translated_images/calculation.a209813050a1ddb141cdc4bc56f3af31e67157ed499e16a2ecf9837542704c94.pt.png)
>
> Calculate the value of Y. If you're paying around $4, it must be April! Infographic by [Jen Looper](https://twitter.com/jenlooper)
>
> The math that calculates the line must demonstrate the slope of the line, which is also dependent on the intercept, or where `Y` is situated when `X = 0`.
>
> You can observe the method of calculation for these values on the [Math is Fun](https://www.mathsisfun.com/data/least-squares-regression.html) web site. Also visit [this Least-squares calculator](https://www.mathsisfun.com/data/least-squares-calculator.html) to watch how the numbers' values impact the line.

## Correlation

One more term to understand is the **Correlation Coefficient** between given X and Y variables. Using a scatterplot, you can quickly visualize this coefficient. A plot with datapoints scattered in a neat line have high correlation, but a plot with datapoints scattered everywhere between X and Y have a low correlation.

A good linear regression model will be one that has a high (nearer to 1 than 0) Correlation Coefficient using the Least-Squares Regression method with a line of regression.

‚úÖ Run the notebook accompanying this lesson and look at the Month to Price scatterplot. Does the data associating Month to Price for pumpkin sales seem to have high or low correlation, according to your visual interpretation of the scatterplot? Does that change if you use more fine-grained measure instead of `Month`, eg. *day of the year* (i.e. number of days since the beginning of the year)?

In the code below, we will assume that we have cleaned up the data, and obtained a data frame called `new_pumpkins`, similar to the following:

ID | Month | DayOfYear | Variety | City | Package | Low Price | High Price | Price
---|-------|-----------|---------|------|---------|-----------|------------|-------
70 | 9 | 267 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 15.0 | 15.0 | 13.636364
71 | 9 | 267 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 18.0 | 18.0 | 16.363636
72 | 10 | 274 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 18.0 | 18.0 | 16.363636
73 | 10 | 274 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 17.0 | 17.0 | 15.454545
74 | 10 | 281 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 15.0 | 15.0 | 13.636364

> The code to clean the data is available in [`notebook.ipynb`](../../../../2-Regression/3-Linear/notebook.ipynb). We have performed the same cleaning steps as in the previous lesson, and have calculated `DayOfYear` coluna usando a seguinte express√£o: 

```python
day_of_year = pd.to_datetime(pumpkins['Date']).apply(lambda dt: (dt-datetime(dt.year,1,1)).days)
```

Agora que voc√™ tem uma compreens√£o da matem√°tica por tr√°s da regress√£o linear, vamos criar um modelo de Regress√£o para ver se conseguimos prever qual pacote de ab√≥boras ter√° os melhores pre√ßos de ab√≥bora. Algu√©m comprando ab√≥boras para um patch de ab√≥boras de feriado pode querer essa informa√ß√£o para otimizar suas compras de pacotes de ab√≥bora para o patch.

## Procurando por Correla√ß√£o

[![ML para iniciantes - Procurando por Correla√ß√£o: A Chave para a Regress√£o Linear](https://img.youtube.com/vi/uoRq-lW2eQo/0.jpg)](https://youtu.be/uoRq-lW2eQo "ML para iniciantes - Procurando por Correla√ß√£o: A Chave para a Regress√£o Linear")

> üé• Clique na imagem acima para um breve v√≠deo sobre correla√ß√£o.

Na li√ß√£o anterior, voc√™ provavelmente viu que o pre√ßo m√©dio para diferentes meses parece assim:

<img alt="Pre√ßo m√©dio por m√™s" src="../2-Data/images/barchart.png" width="50%"/>

Isso sugere que deve haver alguma correla√ß√£o, e podemos tentar treinar um modelo de regress√£o linear para prever a rela√ß√£o entre `Month` and `Price`, or between `DayOfYear` and `Price`. Here is the scatter plot that shows the latter relationship:

<img alt="Scatter plot of Price vs. Day of Year" src="images/scatter-dayofyear.png" width="50%" /> 

Let's see if there is a correlation using the `corr` fun√ß√£o:

```python
print(new_pumpkins['Month'].corr(new_pumpkins['Price']))
print(new_pumpkins['DayOfYear'].corr(new_pumpkins['Price']))
```

Parece que a correla√ß√£o √© bastante pequena, -0.15 pela fun√ß√£o de plotagem `Month` and -0.17 by the `DayOfMonth`, but there could be another important relationship. It looks like there are different clusters of prices corresponding to different pumpkin varieties. To confirm this hypothesis, let's plot each pumpkin category using a different color. By passing an `ax` parameter to the `scatter`, podemos plotar todos os pontos no mesmo gr√°fico:

```python
ax=None
colors = ['red','blue','green','yellow']
for i,var in enumerate(new_pumpkins['Variety'].unique()):
    df = new_pumpkins[new_pumpkins['Variety']==var]
    ax = df.plot.scatter('DayOfYear','Price',ax=ax,c=colors[i],label=var)
```

<img alt="Gr√°fico de dispers√£o de Pre√ßo vs. Dia do Ano" src="images/scatter-dayofyear-color.png" width="50%" /> 

Nossa investiga√ß√£o sugere que a variedade tem mais efeito sobre o pre√ßo geral do que a data de venda real. Podemos ver isso com um gr√°fico de barras:

```python
new_pumpkins.groupby('Variety')['Price'].mean().plot(kind='bar')
```

<img alt="Gr√°fico de barras de pre√ßo vs variedade" src="images/price-by-variety.png" width="50%" /> 

Vamos nos concentrar por enquanto apenas em uma variedade de ab√≥bora, a 'tipo torta', e ver qual efeito a data tem sobre o pre√ßo:

```python
pie_pumpkins = new_pumpkins[new_pumpkins['Variety']=='PIE TYPE']
pie_pumpkins.plot.scatter('DayOfYear','Price') 
```
<img alt="Gr√°fico de dispers√£o de Pre√ßo vs. Dia do Ano" src="images/pie-pumpkins-scatter.png" width="50%" /> 

Se agora calcularmos a correla√ß√£o entre `Price` and `DayOfYear` using `corr` function, we will get something like `-0.27` - o que significa que treinar um modelo preditivo faz sentido.

> Antes de treinar um modelo de regress√£o linear, √© importante garantir que nossos dados estejam limpos. A regress√£o linear n√£o funciona bem com valores ausentes, portanto, faz sentido se livrar de todas as c√©lulas vazias:

```python
pie_pumpkins.dropna(inplace=True)
pie_pumpkins.info()
```

Outra abordagem seria preencher esses valores vazios com valores m√©dios da coluna correspondente.

## Regress√£o Linear Simples

[![ML para iniciantes - Regress√£o Linear e Polinomial usando Scikit-learn](https://img.youtube.com/vi/e4c_UP2fSjg/0.jpg)](https://youtu.be/e4c_UP2fSjg "ML para iniciantes - Regress√£o Linear e Polinomial usando Scikit-learn")

> üé• Clique na imagem acima para um breve v√≠deo sobre regress√£o linear e polinomial.

Para treinar nosso modelo de Regress√£o Linear, usaremos a biblioteca **Scikit-learn**.

```python
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
```

Come√ßamos separando os valores de entrada (caracter√≠sticas) e a sa√≠da esperada (r√≥tulo) em arrays numpy separados:

```python
X = pie_pumpkins['DayOfYear'].to_numpy().reshape(-1,1)
y = pie_pumpkins['Price']
```

> Note que tivemos que realizar `reshape` nos dados de entrada para que o pacote de Regress√£o Linear os entendesse corretamente. A Regress√£o Linear espera um array 2D como entrada, onde cada linha do array corresponde a um vetor de caracter√≠sticas de entrada. No nosso caso, como temos apenas uma entrada - precisamos de um array com formato N√ó1, onde N √© o tamanho do conjunto de dados.

Em seguida, precisamos dividir os dados em conjuntos de dados de treinamento e teste, para que possamos validar nosso modelo ap√≥s o treinamento:

```python
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
```

Finalmente, treinar o modelo de Regress√£o Linear real leva apenas duas linhas de c√≥digo. Definimos o m√©todo `LinearRegression` object, and fit it to our data using the `fit`:

```python
lin_reg = LinearRegression()
lin_reg.fit(X_train,y_train)
```

O `LinearRegression` object after `fit`-ting contains all the coefficients of the regression, which can be accessed using `.coef_` property. In our case, there is just one coefficient, which should be around `-0.017`. It means that prices seem to drop a bit with time, but not too much, around 2 cents per day. We can also access the intersection point of the regression with Y-axis using `lin_reg.intercept_` - it will be around `21` no nosso caso, indicando o pre√ßo no in√≠cio do ano.

Para ver qu√£o preciso √© nosso modelo, podemos prever pre√ßos em um conjunto de dados de teste e, em seguida, medir qu√£o pr√≥ximas nossas previs√µes est√£o dos valores esperados. Isso pode ser feito usando a m√©trica de erro quadr√°tico m√©dio (MSE), que √© a m√©dia de todas as diferen√ßas quadradas entre o valor esperado e o valor previsto.

```python
pred = lin_reg.predict(X_test)

mse = np.sqrt(mean_squared_error(y_test,pred))
print(f'Mean error: {mse:3.3} ({mse/np.mean(pred)*100:3.3}%)')
```

Nosso erro parece estar em torno de 2 pontos, o que √© ~17%. N√£o √© muito bom. Outro indicador da qualidade do modelo √© o **coeficiente de determina√ß√£o**, que pode ser obtido assim:

```python
score = lin_reg.score(X_train,y_train)
print('Model determination: ', score)
```
Se o valor for 0, isso significa que o modelo n√£o leva em conta os dados de entrada e atua como o *pior preditor linear*, que √© simplesmente um valor m√©dio do resultado. O valor de 1 significa que podemos prever perfeitamente todas as sa√≠das esperadas. No nosso caso, o coeficiente √© em torno de 0.06, o que √© bastante baixo.

Tamb√©m podemos plotar os dados de teste junto com a linha de regress√£o para ver melhor como a regress√£o funciona em nosso caso:

```python
plt.scatter(X_test,y_test)
plt.plot(X_test,pred)
```

<img alt="Regress√£o linear" src="images/linear-results.png" width="50%" />

## Regress√£o Polinomial

Outro tipo de Regress√£o Linear √© a Regress√£o Polinomial. Embora √†s vezes haja uma rela√ß√£o linear entre vari√°veis - quanto maior o volume da ab√≥bora, maior o pre√ßo - √†s vezes essas rela√ß√µes n√£o podem ser plotadas como um plano ou linha reta. 

‚úÖ Aqui est√£o [mais alguns exemplos](https://online.stat.psu.edu/stat501/lesson/9/9.8) de dados que poderiam usar Regress√£o Polinomial

D√™ mais uma olhada na rela√ß√£o entre Data e Pre√ßo. Este gr√°fico de dispers√£o parece que deve ser necessariamente analisado por uma linha reta? Os pre√ßos n√£o podem flutuar? Nesse caso, voc√™ pode tentar a regress√£o polinomial.

‚úÖ Polin√¥mios s√£o express√µes matem√°ticas que podem consistir em uma ou mais vari√°veis e coeficientes

A regress√£o polinomial cria uma linha curva para se ajustar melhor aos dados n√£o lineares. No nosso caso, se incluirmos uma vari√°vel `DayOfYear` elevada ao quadrado nos dados de entrada, devemos ser capazes de ajustar nossos dados com uma curva parab√≥lica, que ter√° um m√≠nimo em um certo ponto dentro do ano.

O Scikit-learn inclui uma √∫til [API de pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html?highlight=pipeline#sklearn.pipeline.make_pipeline) para combinar diferentes etapas do processamento de dados. Um **pipeline** √© uma cadeia de **estimadores**. No nosso caso, criaremos um pipeline que primeiro adiciona recursos polinomiais ao nosso modelo e, em seguida, treina a regress√£o:

```python
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline

pipeline = make_pipeline(PolynomialFeatures(2), LinearRegression())

pipeline.fit(X_train,y_train)
```

Usando `PolynomialFeatures(2)` means that we will include all second-degree polynomials from the input data. In our case it will just mean `DayOfYear`<sup>2</sup>, but given two input variables X and Y, this will add X<sup>2</sup>, XY and Y<sup>2</sup>. We may also use higher degree polynomials if we want.

Pipelines can be used in the same manner as the original `LinearRegression` object, i.e. we can `fit` the pipeline, and then use `predict` to get the prediction results. Here is the graph showing test data, and the approximation curve:

<img alt="Polynomial regression" src="images/poly-results.png" width="50%" />

Using Polynomial Regression, we can get slightly lower MSE and higher determination, but not significantly. We need to take into account other features!

> You can see that the minimal pumpkin prices are observed somewhere around Halloween. How can you explain this? 

üéÉ Congratulations, you just created a model that can help predict the price of pie pumpkins. You can probably repeat the same procedure for all pumpkin types, but that would be tedious. Let's learn now how to take pumpkin variety into account in our model!

## Categorical Features

In the ideal world, we want to be able to predict prices for different pumpkin varieties using the same model. However, the `Variety` column is somewhat different from columns like `Month`, because it contains non-numeric values. Such columns are called **categorical**.

[![ML for beginners - Categorical Feature Predictions with Linear Regression](https://img.youtube.com/vi/DYGliioIAE0/0.jpg)](https://youtu.be/DYGliioIAE0 "ML for beginners - Categorical Feature Predictions with Linear Regression")

> üé• Click the image above for a short video overview of using categorical features.

Here you can see how average price depends on variety:

<img alt="Average price by variety" src="images/price-by-variety.png" width="50%" />

To take variety into account, we first need to convert it to numeric form, or **encode** it. There are several way we can do it:

* Simple **numeric encoding** will build a table of different varieties, and then replace the variety name by an index in that table. This is not the best idea for linear regression, because linear regression takes the actual numeric value of the index, and adds it to the result, multiplying by some coefficient. In our case, the relationship between the index number and the price is clearly non-linear, even if we make sure that indices are ordered in some specific way.
* **One-hot encoding** will replace the `Variety` column by 4 different columns, one for each variety. Each column will contain `1` if the corresponding row is of a given variety, and `0` de outra forma. Isso significa que haver√° quatro coeficientes na regress√£o linear, um para cada variedade de ab√≥bora, respons√°vel pelo "pre√ßo inicial" (ou melhor, "pre√ßo adicional") para essa variedade em particular.

O c√≥digo abaixo mostra como podemos codificar uma variedade usando one-hot:

```python
pd.get_dummies(new_pumpkins['Variety'])
```

 ID | FAIRYTALE | MINIATURE | VARIEDADES MISTAS HEREDIT√ÅRIAS | TIPO TORTA
----|-----------|-----------|-------------------------------|----------
70 | 0 | 0 | 0 | 1
71 | 0 | 0 | 0 | 1
... | ... | ... | ... | ...
1738 | 0 | 1 | 0 | 0
1739 | 0 | 1 | 0 | 0
1740 | 0 | 1 | 0 | 0
1741 | 0 | 1 | 0 | 0
1742 | 0 | 1 | 0 | 0

Para treinar a regress√£o linear usando a variedade codificada one-hot como entrada, s√≥ precisamos inicializar os dados `X` and `y` corretamente:

```python
X = pd.get_dummies(new_pumpkins['Variety'])
y = new_pumpkins['Price']
```

O restante do c√≥digo √© o mesmo que usamos acima para treinar a Regress√£o Linear. Se voc√™ tentar, ver√° que o erro quadr√°tico m√©dio √© aproximadamente o mesmo, mas obtemos um coeficiente de determina√ß√£o muito mais alto (~77%). Para obter previs√µes ainda mais precisas, podemos levar em conta mais recursos categ√≥ricos, bem como recursos num√©ricos, como `Month` or `DayOfYear`. To get one large array of features, we can use `join`:

```python
X = pd.get_dummies(new_pumpkins['Variety']) \
        .join(new_pumpkins['Month']) \
        .join(pd.get_dummies(new_pumpkins['City'])) \
        .join(pd.get_dummies(new_pumpkins['Package']))
y = new_pumpkins['Price']
```

Aqui tamb√©m levamos em considera√ß√£o o tipo de `City` and `Package`, que nos d√° MSE 2.84 (10%) e determina√ß√£o 0.94!

## Juntando tudo

Para fazer o melhor modelo, podemos usar dados combinados (categ√≥ricos codificados one-hot + num√©ricos) do exemplo acima junto com a Regress√£o Polinomial. Aqui est√° o c√≥digo completo para sua conveni√™ncia:

```python
# set up training data
X = pd.get_dummies(new_pumpkins['Variety']) \
        .join(new_pumpkins['Month']) \
        .join(pd.get_dummies(new_pumpkins['City'])) \
        .join(pd.get_dummies(new_pumpkins['Package']))
y = new_pumpkins['Price']

# make train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# setup and train the pipeline
pipeline = make_pipeline(PolynomialFeatures(2), LinearRegression())
pipeline.fit(X_train,y_train)

# predict results for test data
pred = pipeline.predict(X_test)

# calculate MSE and determination
mse = np.sqrt(mean_squared_error(y_test,pred))
print(f'Mean error: {mse:3.3} ({mse/np.mean(pred)*100:3.3}%)')

score = pipeline.score(X_train,y_train)
print('Model determination: ', score)
```

Isso deve nos dar o melhor coeficiente de determina√ß√£o de quase 97%, e MSE=2.23 (~8% de erro de previs√£o).

| Modelo | MSE | Determina√ß√£o |
|-------|-----|---------------|
| `DayOfYear` Linear | 2.77 (17.2%) | 0.07 |
| `DayOfYear` Polynomial | 2.73 (17.0%) | 0.08 |
| `Variety` Linear | 5.24 (19.7%) | 0.77 |
| Todas as caracter√≠sticas Linear | 2.84 (10.5%) | 0.94 |
| Todas as caracter√≠sticas Polinomial | 2.23 (8.25%) | 0.97 |

üèÜ Muito bem! Voc√™ criou quatro modelos de Regress√£o em uma li√ß√£o e melhorou a qualidade do modelo para 97%. Na se√ß√£o final sobre Regress√£o, voc√™ aprender√° sobre Regress√£o Log√≠stica para determinar categorias. 

---
## üöÄDesafio

Teste v√°rias vari√°veis diferentes neste notebook para ver como a correla√ß√£o corresponde √† precis√£o do modelo.

## [Question√°rio p√≥s-aula](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/14/)

## Revis√£o e Autoestudo

Nesta li√ß√£o, aprendemos sobre Regress√£o Linear. Existem outros tipos importantes de Regress√£o. Leia sobre as t√©cnicas Stepwise, Ridge, Lasso e Elasticnet. Um bom curso para estudar e aprender mais √© o [curso de Aprendizado Estat√≠stico de Stanford](https://online.stanford.edu/courses/sohs-ystatslearning-statistical-learning)

## Tarefa 

[Construa um Modelo](assignment.md)

**Isen√ß√£o de responsabilidade**:  
Este documento foi traduzido utilizando servi√ßos de tradu√ß√£o autom√°tica baseados em IA. Embora nos esforcemos pela precis√£o, esteja ciente de que tradu√ß√µes autom√°ticas podem conter erros ou imprecis√µes. O documento original em seu idioma nativo deve ser considerado a fonte autoritativa. Para informa√ß√µes cr√≠ticas, recomenda-se a tradu√ß√£o profissional por um humano. N√£o nos responsabilizamos por quaisquer mal-entendidos ou interpreta√ß√µes err√¥neas decorrentes do uso desta tradu√ß√£o.