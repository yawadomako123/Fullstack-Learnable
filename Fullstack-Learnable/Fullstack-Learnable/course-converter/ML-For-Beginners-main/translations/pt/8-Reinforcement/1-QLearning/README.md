## Verificando a pol√≠tica

Como a Q-Table lista a "atratividade" de cada a√ß√£o em cada estado, √© bastante f√°cil us√°-la para definir a navega√ß√£o eficiente em nosso mundo. No caso mais simples, podemos selecionar a a√ß√£o correspondente ao maior valor da Q-Table: (c√≥digo bloco 9)

```python
def qpolicy_strict(m):
        x,y = m.human
        v = probs(Q[x,y])
        a = list(actions)[np.argmax(v)]
        return a

walk(m,qpolicy_strict)
```

> Se voc√™ tentar o c√≥digo acima v√°rias vezes, pode notar que √†s vezes ele "trava", e voc√™ precisa pressionar o bot√£o PARAR no notebook para interromp√™-lo. Isso acontece porque pode haver situa√ß√µes em que dois estados "apontam" um para o outro em termos de Q-Valor √≥timo, nesse caso, o agente acaba se movendo entre esses estados indefinidamente.

## üöÄDesafio

> **Tarefa 1:** Modifique o `walk` function to limit the maximum length of path by a certain number of steps (say, 100), and watch the code above return this value from time to time.

> **Task 2:** Modify the `walk` function so that it does not go back to the places where it has already been previously. This will prevent `walk` from looping, however, the agent can still end up being "trapped" in a location from which it is unable to escape.

## Navigation

A better navigation policy would be the one that we used during training, which combines exploitation and exploration. In this policy, we will select each action with a certain probability, proportional to the values in the Q-Table. This strategy may still result in the agent returning back to a position it has already explored, but, as you can see from the code below, it results in a very short average path to the desired location (remember that `print_statistics` para executar a simula√ß√£o 100 vezes): (c√≥digo bloco 10)

```python
def qpolicy(m):
        x,y = m.human
        v = probs(Q[x,y])
        a = random.choices(list(actions),weights=v)[0]
        return a

print_statistics(qpolicy)
```

Ap√≥s executar este c√≥digo, voc√™ deve obter um comprimento m√©dio de caminho muito menor do que antes, na faixa de 3-6.

## Investigando o processo de aprendizado

Como mencionamos, o processo de aprendizado √© um equil√≠brio entre explora√ß√£o e explora√ß√£o do conhecimento adquirido sobre a estrutura do espa√ßo do problema. Vimos que os resultados do aprendizado (a capacidade de ajudar um agente a encontrar um caminho curto para o objetivo) melhoraram, mas tamb√©m √© interessante observar como o comprimento m√©dio do caminho se comporta durante o processo de aprendizado:

Os aprendizados podem ser resumidos como:

- **O comprimento m√©dio do caminho aumenta**. O que vemos aqui √© que, a princ√≠pio, o comprimento m√©dio do caminho aumenta. Isso provavelmente se deve ao fato de que, quando n√£o sabemos nada sobre o ambiente, √© prov√°vel que fiquemos presos em estados ruins, √°gua ou lobo. √Ä medida que aprendemos mais e come√ßamos a usar esse conhecimento, podemos explorar o ambiente por mais tempo, mas ainda n√£o sabemos muito bem onde est√£o as ma√ß√£s.

- **O comprimento do caminho diminui, √† medida que aprendemos mais**. Uma vez que aprendemos o suficiente, torna-se mais f√°cil para o agente alcan√ßar o objetivo, e o comprimento do caminho come√ßa a diminuir. No entanto, ainda estamos abertos √† explora√ß√£o, ent√£o muitas vezes nos afastamos do melhor caminho e exploramos novas op√ß√µes, tornando o caminho mais longo do que o ideal.

- **Aumento abrupto do comprimento**. O que tamb√©m observamos neste gr√°fico √© que, em algum momento, o comprimento aumentou abruptamente. Isso indica a natureza estoc√°stica do processo e que, em algum ponto, podemos "estragar" os coeficientes da Q-Table ao sobrescrev√™-los com novos valores. Isso deve ser minimizado idealmente, diminuindo a taxa de aprendizado (por exemplo, no final do treinamento, ajustamos os valores da Q-Table apenas por um pequeno valor).

No geral, √© importante lembrar que o sucesso e a qualidade do processo de aprendizado dependem significativamente de par√¢metros, como taxa de aprendizado, decaimento da taxa de aprendizado e fator de desconto. Esses par√¢metros s√£o frequentemente chamados de **hiperpar√¢metros**, para distingui-los dos **par√¢metros**, que otimizamos durante o treinamento (por exemplo, coeficientes da Q-Table). O processo de encontrar os melhores valores de hiperpar√¢metros √© chamado de **otimiza√ß√£o de hiperpar√¢metros**, e merece um t√≥pico separado.

## [Quiz p√≥s-aula](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/46/)

## Tarefa 
[Um Mundo Mais Realista](assignment.md)

**Aviso Legal**:  
Este documento foi traduzido utilizando servi√ßos de tradu√ß√£o autom√°tica baseados em IA. Embora nos esforcemos pela precis√£o, esteja ciente de que tradu√ß√µes automatizadas podem conter erros ou imprecis√µes. O documento original em seu idioma nativo deve ser considerado a fonte autoritativa. Para informa√ß√µes cr√≠ticas, recomenda-se a tradu√ß√£o profissional por humanos. N√£o nos responsabilizamos por quaisquer mal-entendidos ou interpreta√ß√µes err√¥neas decorrentes do uso desta tradu√ß√£o.