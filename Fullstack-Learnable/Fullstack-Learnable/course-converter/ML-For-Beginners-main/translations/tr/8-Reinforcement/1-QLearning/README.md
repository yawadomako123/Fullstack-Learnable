## PolitikayÄ± kontrol etme

Q-Tablosu her durumdaki her eylemin "Ã§ekiciliÄŸini" listeler, bu nedenle dÃ¼nyamÄ±zda verimli navigasyonu tanÄ±mlamak oldukÃ§a kolaydÄ±r. En basit durumda, en yÃ¼ksek Q-Tablosu deÄŸerine karÅŸÄ±lÄ±k gelen eylemi seÃ§ebiliriz: (kod bloÄŸu 9)

```python
def qpolicy_strict(m):
        x,y = m.human
        v = probs(Q[x,y])
        a = list(actions)[np.argmax(v)]
        return a

walk(m,qpolicy_strict)
```

> YukarÄ±daki kodu birkaÃ§ kez denerseniz, bazen "takÄ±ldÄ±ÄŸÄ±nÄ±" ve kesmek iÃ§in not defterindeki DURDUR dÃ¼ÄŸmesine basmanÄ±z gerektiÄŸini fark edebilirsiniz. Bu, iki durumun optimal Q-DeÄŸeri aÃ§Ä±sÄ±ndan birbirine "iÅŸaret ettiÄŸi" durumlar olabileceÄŸinden, bu durumda ajan bu durumlar arasÄ±nda sonsuz bir ÅŸekilde hareket etmeye baÅŸlar.

## ğŸš€Meydan Okuma

> **GÃ¶rev 1:** `walk` function to limit the maximum length of path by a certain number of steps (say, 100), and watch the code above return this value from time to time.

> **Task 2:** Modify the `walk` function so that it does not go back to the places where it has already been previously. This will prevent `walk` from looping, however, the agent can still end up being "trapped" in a location from which it is unable to escape.

## Navigation

A better navigation policy would be the one that we used during training, which combines exploitation and exploration. In this policy, we will select each action with a certain probability, proportional to the values in the Q-Table. This strategy may still result in the agent returning back to a position it has already explored, but, as you can see from the code below, it results in a very short average path to the desired location (remember that `print_statistics` simÃ¼lasyonu 100 kez Ã§alÄ±ÅŸtÄ±racak ÅŸekilde deÄŸiÅŸtirin: (kod bloÄŸu 10)

```python
def qpolicy(m):
        x,y = m.human
        v = probs(Q[x,y])
        a = random.choices(list(actions),weights=v)[0]
        return a

print_statistics(qpolicy)
```

Bu kodu Ã§alÄ±ÅŸtÄ±rdÄ±ktan sonra, Ã¶nceki ortalama yol uzunluÄŸundan Ã§ok daha kÃ¼Ã§Ã¼k bir ortalama yol uzunluÄŸu elde etmelisiniz, 3-6 aralÄ±ÄŸÄ±nda.

## Ã–ÄŸrenme sÃ¼recini araÅŸtÄ±rma

BelirttiÄŸimiz gibi, Ã¶ÄŸrenme sÃ¼reci, problem alanÄ±nÄ±n yapÄ±sÄ± hakkÄ±nda elde edilen bilgilerin keÅŸfi ve keÅŸfi arasÄ±nda bir dengedir. Ã–ÄŸrenme sonuÃ§larÄ±nÄ±n (bir ajanÄ±n hedefe kÄ±sa bir yol bulma yeteneÄŸi) iyileÅŸtiÄŸini gÃ¶rdÃ¼k, ancak Ã¶ÄŸrenme sÃ¼recinde ortalama yol uzunluÄŸunun nasÄ±l davrandÄ±ÄŸÄ±nÄ± gÃ¶zlemlemek de ilginÃ§tir:

Ã–ÄŸrenilenler ÅŸu ÅŸekilde Ã¶zetlenebilir:

- **Ortalama yol uzunluÄŸu artar**. Burada gÃ¶rdÃ¼ÄŸÃ¼mÃ¼z ÅŸey, baÅŸlangÄ±Ã§ta ortalama yol uzunluÄŸunun arttÄ±ÄŸÄ±dÄ±r. Bu, Ã§evre hakkÄ±nda hiÃ§bir ÅŸey bilmediÄŸimizde, kÃ¶tÃ¼ durumlara, suya veya kurda yakalanma olasÄ±lÄ±ÄŸÄ±mÄ±zÄ±n yÃ¼ksek olmasÄ± nedeniyle olabilir. Daha fazla bilgi edindikÃ§e ve bu bilgiyi kullanmaya baÅŸladÄ±kÃ§a, Ã§evreyi daha uzun sÃ¼re keÅŸfedebiliriz, ancak elma nerede olduÄŸunu hala Ã§ok iyi bilmiyoruz.

- **Ã–ÄŸrendikÃ§e yol uzunluÄŸu azalÄ±r**. Yeterince Ã¶ÄŸrendiÄŸimizde, ajanÄ±n hedefe ulaÅŸmasÄ± daha kolay hale gelir ve yol uzunluÄŸu azalmaya baÅŸlar. Ancak, keÅŸfe hala aÃ§Ä±ÄŸÄ±z, bu yÃ¼zden genellikle en iyi yoldan saparÄ±z ve yeni seÃ§enekleri keÅŸfederiz, bu da yolu optimalden daha uzun hale getirir.

- **Uzunluk ani bir ÅŸekilde artar**. Bu grafikte ayrÄ±ca, bir noktada uzunluÄŸun ani bir ÅŸekilde arttÄ±ÄŸÄ±nÄ± gÃ¶zlemliyoruz. Bu, sÃ¼recin stokastik doÄŸasÄ±nÄ± ve bir noktada Q-Tablosu katsayÄ±larÄ±nÄ± yeni deÄŸerlerle Ã¼zerine yazarak "bozabileceÄŸimizi" gÃ¶sterir. Bu, ideal olarak Ã¶ÄŸrenme oranÄ±nÄ± azaltarak en aza indirilmelidir (Ã¶rneÄŸin, eÄŸitimin sonuna doÄŸru, Q-Tablosu deÄŸerlerini sadece kÃ¼Ã§Ã¼k bir deÄŸerle ayarlayarak).

Genel olarak, Ã¶ÄŸrenme sÃ¼recinin baÅŸarÄ±sÄ± ve kalitesinin, Ã¶ÄŸrenme oranÄ±, Ã¶ÄŸrenme oranÄ± dÃ¼ÅŸÃ¼ÅŸÃ¼ ve indirim faktÃ¶rÃ¼ gibi parametrelere Ã¶nemli Ã¶lÃ§Ã¼de baÄŸlÄ± olduÄŸunu hatÄ±rlamak Ã¶nemlidir. Bunlar genellikle **hiperparametreler** olarak adlandÄ±rÄ±lÄ±r, Ã§Ã¼nkÃ¼ eÄŸitim sÄ±rasÄ±nda optimize ettiÄŸimiz **parametrelerden** (Ã¶rneÄŸin, Q-Tablosu katsayÄ±larÄ±) farklÄ±dÄ±r. En iyi hiperparametre deÄŸerlerini bulma sÃ¼recine **hiperparametre optimizasyonu** denir ve ayrÄ± bir konuyu hak eder.

## [Ders sonrasÄ± quiz](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/46/)

## Ã–dev
[Daha GerÃ§ekÃ§i Bir DÃ¼nya](assignment.md)

**Feragatname**:
Bu belge, makine tabanlÄ± yapay zeka Ã§eviri hizmetleri kullanÄ±larak Ã§evrilmiÅŸtir. DoÄŸruluk iÃ§in Ã§aba sarf etsek de, otomatik Ã§evirilerin hata veya yanlÄ±ÅŸlÄ±klar iÃ§erebileceÄŸini lÃ¼tfen unutmayÄ±n. Orijinal belge, kendi dilinde yetkili kaynak olarak kabul edilmelidir. Kritik bilgiler iÃ§in profesyonel insan Ã§evirisi Ã¶nerilir. Bu Ã§evirinin kullanÄ±mÄ±ndan kaynaklanan herhangi bir yanlÄ±ÅŸ anlama veya yanlÄ±ÅŸ yorumlamadan sorumlu deÄŸiliz.