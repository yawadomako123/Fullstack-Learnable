# 训练山地车

[OpenAI Gym](http://gym.openai.com) 的设计方式使得所有环境都提供相同的 API - 即相同的方法 `reset`、`step` 和 `render`，以及相同的 **动作空间** 和 **观察空间** 抽象。因此，应该可以在不同环境中以最小的代码更改来适应相同的强化学习算法。

## 山地车环境

[山地车环境](https://gym.openai.com/envs/MountainCar-v0/) 包含一辆卡在山谷中的车：
目标是驶出山谷并夺取旗帜，每一步执行以下动作之一：

| 值 | 含义 |
|---|---|
| 0 | 向左加速 |
| 1 | 不加速 |
| 2 | 向右加速 |

这个问题的主要技巧是，汽车的发动机不够强大，无法一次性爬上山。因此，唯一成功的方法是来回驾驶以积累动量。

观察空间仅由两个值组成：

| 数字 | 观察值  | 最小值 | 最大值 |
|-----|--------------|-----|-----|
|  0  | 车的位置 | -1.2| 0.6 |
|  1  | 车的速度 | -0.07 | 0.07 |

山地车的奖励系统相当棘手：

 * 如果代理在山顶（位置 = 0.5）到达旗帜，则奖励为0。
 * 如果代理的位置小于0.5，则奖励为-1。

如果车的位置超过0.5，或者剧集长度超过200，剧集将终止。
## 说明

调整我们的强化学习算法以解决山地车问题。从现有的 [notebook.ipynb](../../../../8-Reinforcement/2-Gym/notebook.ipynb) 代码开始，替换新的环境，改变状态离散化函数，并尝试以最小的代码修改使现有算法进行训练。通过调整超参数来优化结果。

> **注意**：可能需要调整超参数才能使算法收敛。
## 评分标准

| 标准 | 模范 | 充分 | 需要改进 |
| -------- | --------- | -------- | ----------------- |
|          | Q-Learning 算法已成功从 CartPole 示例中适应，最小代码修改即可解决在200步内捕获旗帜的问题。 | 采用了一个新的 Q-Learning 算法，但有良好的文档记录；或者采用了现有算法，但未达到预期结果 | 学生未能成功采用任何算法，但在解决方案方面迈出了实质性步骤（实现状态离散化、Q-表数据结构等） |

**免责声明**：
本文档已使用基于机器的人工智能翻译服务进行翻译。虽然我们努力确保准确性，但请注意，自动翻译可能包含错误或不准确之处。应将原始语言的文档视为权威来源。对于关键信息，建议使用专业人工翻译。对于因使用此翻译而产生的任何误解或误读，我们不承担任何责任。