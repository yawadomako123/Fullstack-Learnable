{
  "id": 100,
  "title": "ML For Beginners",
  "description": "Imported course: ML For Beginners",
  "category": "Machine Learning",
  "instructor": "Microsoft Team",
  "modules": [
    {
      "id": 1,
      "title": "Introduction",
      "orderIndex": 0,
      "lessons": [
        {
          "id": 2,
          "title": "Introduction",
          "content": "# Introduction to machine learning\n\nIn this section of the curriculum, you will be introduced to the base concepts underlying the field of machine learning, what it is, and learn about its history and the techniques researchers use to work with it.  Let's explore this new world of ML together!\n\n![globe](images/globe.jpg)\n> Photo by <a href=\"https://unsplash.com/@bill_oxford?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">Bill Oxford</a> on <a href=\"https://unsplash.com/s/photos/globe?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">Unsplash</a>\n  \n### Lessons\n\n1. [Introduction to machine learning](1-intro-to-ML/README.md)\n1. [The History of machine learning and AI](2-history-of-ML/README.md)\n1. [Fairness and machine learning](3-fairness/README.md)\n1. [Techniques of machine learning](4-techniques-of-ML/README.md)\n### Credits\n\n\"Introduction to Machine Learning\" was written with â™¥ï¸ by a team of folks including [Muhammad Sakib Khan Inan](https://twitter.com/Sakibinan), [Ornella Altunyan](https://twitter.com/ornelladotcom) and [Jen Looper](https://twitter.com/jenlooper)\n\n\"The History of Machine Learning\" was written with â™¥ï¸ by [Jen Looper](https://twitter.com/jenlooper) and [Amy Boyd](https://twitter.com/AmyKateNicho)\n\n\"Fairness and Machine Learning\" was written with â™¥ï¸ by [Tomomi Imura](https://twitter.com/girliemac) \n\n\"Techniques of Machine Learning\" was written with â™¥ï¸ by [Jen Looper](https://twitter.com/jenlooper) and [Chris Noring](https://twitter.com/softchris) ",
          "pdfUrl": "",
          "videoUrl": "",
          "orderIndex": 0
        }
      ],
      "quizzes": [],
      "subModules": [
        {
          "id": 3,
          "title": "Intro To ML",
          "orderIndex": 0,
          "lessons": [
            {
              "id": 4,
              "title": "Intro To ML",
              "content": "# Get Up and Running\n\n## Instructions\n\nIn this non-graded assignment, you should brush up on Python and get your environment up and running and able to run notebooks.\n\nTake this [Python Learning Path](https://docs.microsoft.com/learn/paths/python-language/?WT.mc_id=academic-77952-leestott), and then get your systems setup by going through these introductory videos:\n\nhttps://www.youtube.com/playlist?list=PLlrxD0HtieHhS8VzuMCfQD4uJ9yne1mE6\n",
              "pdfUrl": "",
              "videoUrl": "",
              "orderIndex": 0
            },
            {
              "id": 5,
              "title": "Intro To ML",
              "content": "# Introduction to machine learning\n\n## [Pre-lecture quiz](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/1/)\n\n---\n\n[![ML for beginners - Introduction to Machine Learning for Beginners](https://img.youtube.com/vi/6mSx_KJxcHI/0.jpg)](https://youtu.be/6mSx_KJxcHI \"ML for beginners - Introduction to Machine Learning for Beginners\")\n\n> ðŸŽ¥ Click the image above for a short video working through this lesson.\n\nWelcome to this course on classical machine learning for beginners! Whether you're completely new to this topic, or an experienced ML practitioner looking to brush up on an area, we're happy to have you join us! We want to create a friendly launching spot for your ML study and would be happy to evaluate, respond to, and incorporate your [feedback](https://github.com/microsoft/ML-For-Beginners/discussions).\n\n[![Introduction to ML](https://img.youtube.com/vi/h0e2HAPTGF4/0.jpg)](https://youtu.be/h0e2HAPTGF4 \"Introduction to ML\")\n\n> ðŸŽ¥ Click the image above for a video: MIT's John Guttag introduces machine learning\n\n---\n## Getting started with machine learning\n\nBefore starting with this curriculum, you need to have your computer set up and ready to run notebooks locally.\n\n- **Configure your machine with these videos**. Use the following links to learn [how to install Python](https://youtu.be/CXZYvNRIAKM) in your system and [setup a text editor](https://youtu.be/EU8eayHWoZg) for development.\n- **Learn Python**. It's also recommended to have a basic understanding of [Python](https://docs.microsoft.com/learn/paths/python-language/?WT.mc_id=academic-77952-leestott), a programming language useful for data scientists that we use in this course.\n- **Learn Node.js and JavaScript**. We also use JavaScript a few times in this course when building web apps, so you will need to have [node](https://nodejs.org) and [npm](https://www.npmjs.com/) installed, as well as [Visual Studio Code](https://code.visualstudio.com/) available for both Python and JavaScript development.\n- **Create a GitHub account**. Since you found us here on [GitHub](https://github.com), you might already have an account, but if not, create one and then fork this curriculum to use on your own. (Feel free to give us a star, too ðŸ˜Š)\n- **Explore Scikit-learn**. Familiarize yourself with [Scikit-learn](https://scikit-learn.org/stable/user_guide.html), a set of ML libraries that we reference in these lessons.\n\n---\n## What is machine learning?\n\nThe term 'machine learning' is one of the most popular and frequently used terms of today. There is a nontrivial possibility that you have heard this term at least once if you have some sort of familiarity with technology, no matter what domain you work in. The mechanics of machine learning, however, are a mystery to most people. For a machine learning beginner, the subject can sometimes feel overwhelming. Therefore, it is important to understand what machine learning actually is, and to learn about it step by step, through practical examples.\n\n---\n## The hype curve\n\n![ml hype curve](images/hype.png)\n\n> Google Trends shows the recent 'hype curve' of the term 'machine learning'\n\n---\n## A mysterious universe\n\nWe live in a universe full of fascinating mysteries. Great scientists such as Stephen Hawking, Albert Einstein, and many more have devoted their lives to searching for meaningful information that uncovers the mysteries of the world around us. This is the human condition of learning: a human child learns new things and uncovers the structure of their world year by year as they grow to adulthood.\n\n---\n## The child's brain\n\nA child's brain and senses perceive the facts of their surroundings and gradually learn the hidden patterns of life which help the child to craft logical rules to identify learned patterns. The learning process of the human brain makes humans the most sophisticated living creature of this world. Learning continuously by discovering hidden patterns and then innovating on those patterns enables us to make ourselves better and better throughout our lifetime. This learning capacity and evolving capability is related to a concept called [brain plasticity](https://www.simplypsychology.org/brain-plasticity.html). Superficially, we can draw some motivational similarities between the learning process of the human brain and the concepts of machine learning.\n\n---\n## The human brain\n\nThe [human brain](https://www.livescience.com/29365-human-brain.html) perceives things from the real world, processes the perceived information, makes rational decisions, and performs certain actions based on circumstances. This is what we called behaving intelligently. When we program a facsimile of the intelligent behavioral process to a machine, it is called artificial intelligence (AI).\n\n---\n## Some terminology\n\nAlthough the terms can be confused, machine learning (ML) is an important subset of artificial intelligence. **ML is concerned with using specialized algorithms to uncover meaningful information and find hidden patterns from perceived data to corroborate the rational decision-making process**.\n\n---\n## AI, ML, Deep Learning\n\n![AI, ML, deep learning, data science](images/ai-ml-ds.png)\n\n> A diagram showing the relationships between AI, ML, deep learning, and data science. Infographic by [Jen Looper](https://twitter.com/jenlooper) inspired by [this graphic](https://softwareengineering.stackexchange.com/questions/366996/distinction-between-ai-ml-neural-networks-deep-learning-and-data-mining)\n\n---\n## Concepts to cover\n\nIn this curriculum, we are going to cover only the core concepts of machine learning that a beginner must know. We cover what we call 'classical machine learning' primarily using Scikit-learn, an excellent library many students use to learn the basics.  To understand broader concepts of artificial intelligence or deep learning, a strong fundamental knowledge of machine learning is indispensable, and so we would like to offer it here.\n\n---\n## In this course you will learn:\n\n- core concepts of machine learning\n- the history of ML\n- ML and fairness\n- regression ML techniques\n- classification ML techniques\n- clustering ML techniques\n- natural language processing ML techniques\n- time series forecasting ML techniques\n- reinforcement learning\n- real-world applications for ML\n\n---\n## What we will not cover\n\n- deep learning\n- neural networks\n- AI\n\nTo make for a better learning experience, we will avoid the complexities of neural networks, 'deep learning' - many-layered model-building using neural networks - and AI, which we will discuss in a different curriculum. We also will offer a forthcoming data science curriculum to focus on that aspect of this larger field.\n\n---\n## Why study machine learning?\n\nMachine learning, from a systems perspective, is defined as the creation of automated systems that can learn hidden patterns from data to aid in making intelligent decisions.\n\nThis motivation is loosely inspired by how the human brain learns certain things based on the data it perceives from the outside world.\n\nâœ… Think for a minute why a business would want to try to use machine learning strategies vs. creating a hard-coded rules-based engine.\n\n---\n## Applications of machine learning\n\nApplications of machine learning are now almost everywhere, and are as ubiquitous as the data that is flowing around our societies, generated by our smart phones, connected devices, and other systems. Considering the immense potential of state-of-the-art machine learning algorithms, researchers have been exploring their capability to solve multi-dimensional and multi-disciplinary real-life problems with great positive outcomes.\n\n---\n## Examples of applied ML\n\n**You can use machine learning in many ways**:\n\n- To predict the likelihood of disease from a patient's medical history or reports.\n- To leverage weather data to predict weather events.\n- To understand the sentiment of a text.\n- To detect fake news to stop the spread of propaganda.\n\nFinance, economics, earth science, space exploration, biomedical engineering, cognitive science, and even fields in the humanities have adapted machine learning to solve the arduous, data-processing heavy problems of their domain.\n\n---\n## Conclusion\n\nMachine learning automates the process of pattern-discovery by finding meaningful insights from real-world or generated data. It has proven itself to be highly valuable in business, health, and financial applications, among others.\n\nIn the near future, understanding the basics of machine learning is going to be a must for people from any domain due to its widespread adoption.\n\n---\n# ðŸš€ Challenge\n\nSketch, on paper or using an online app like [Excalidraw](https://excalidraw.com/), your understanding of the differences between AI, ML, deep learning, and data science. Add some ideas of problems that each of these techniques are good at solving.\n\n# [Post-lecture quiz](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/2/)\n\n---\n# Review & Self Study\n\nTo learn more about how you can work with ML algorithms in the cloud, follow this [Learning Path](https://docs.microsoft.com/learn/paths/create-no-code-predictive-models-azure-machine-learning/?WT.mc_id=academic-77952-leestott).\n\nTake a [Learning Path](https://docs.microsoft.com/learn/modules/introduction-to-machine-learning/?WT.mc_id=academic-77952-leestott) about the basics of ML.\n\n---\n# Assignment\n\n[Get up and running](assignment.md)\n",
              "pdfUrl": "",
              "videoUrl": "",
              "orderIndex": 1
            }
          ],
          "quizzes": [],
          "subModules": []
        },
        {
          "id": 6,
          "title": "History Of ML",
          "orderIndex": 1,
          "lessons": [
            {
              "id": 7,
              "title": "History Of ML",
              "content": "# Create a timeline\n\n## Instructions\n\nUsing [this repo](https://github.com/Digital-Humanities-Toolkit/timeline-builder), create a timeline of some aspect of the history of algorithms, mathematics, statistics, AI, or ML, or a combination of these. You can focus on one person, one idea, or a long timespan of thought. Make sure to add multimedia elements.\n\n## Rubric\n\n| Criteria | Exemplary                                         | Adequate                                | Needs Improvement                                                |\n| -------- | ------------------------------------------------- | --------------------------------------- | ---------------------------------------------------------------- |\n|          | A deployed timeline is presented as a GitHub page | The code is incomplete and not deployed | The timeline is incomplete, not well researched and not deployed |\n",
              "pdfUrl": "",
              "videoUrl": "",
              "orderIndex": 0
            },
            {
              "id": 8,
              "title": "History Of ML",
              "content": "# History of machine learning\n\n![Summary of History of machine learning in a sketchnote](../../sketchnotes/ml-history.png)\n> Sketchnote by [Tomomi Imura](https://www.twitter.com/girlie_mac)\n\n## [Pre-lecture quiz](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/3/)\n\n---\n\n[![ML for beginners - History of Machine Learning](https://img.youtube.com/vi/N6wxM4wZ7V0/0.jpg)](https://youtu.be/N6wxM4wZ7V0 \"ML for beginners - History of Machine Learning\")\n\n> ðŸŽ¥ Click the image above for a short video working through this lesson.\n\nIn this lesson, we will walk through the major milestones in the history of machine learning and artificial intelligence.\n\nThe history of artificial intelligence (AI) as a field is intertwined with the history of machine learning, as the algorithms and computational advances that underpin ML fed into the development of AI. It is useful to remember that, while these fields as distinct areas of inquiry began to crystallize in the 1950s, important [algorithmic, statistical, mathematical, computational and technical discoveries](https://wikipedia.org/wiki/Timeline_of_machine_learning) predated and overlapped this era. In fact, people have been thinking about these questions for [hundreds of years](https://wikipedia.org/wiki/History_of_artificial_intelligence): this article discusses the historical intellectual underpinnings of the idea of a 'thinking machine.'\n\n---\n## Notable discoveries\n\n- 1763, 1812 [Bayes Theorem](https://wikipedia.org/wiki/Bayes%27_theorem) and its predecessors. This theorem and its applications underlie inference, describing the probability of an event occurring based on prior knowledge.\n- 1805 [Least Square Theory](https://wikipedia.org/wiki/Least_squares) by French mathematician Adrien-Marie Legendre. This theory, which you will learn about  in our Regression unit, helps in data fitting.\n- 1913 [Markov Chains](https://wikipedia.org/wiki/Markov_chain), named after Russian mathematician Andrey Markov, is used to describe a sequence of possible events based on a previous state.\n- 1957 [Perceptron](https://wikipedia.org/wiki/Perceptron) is a type of linear classifier invented by American psychologist Frank Rosenblatt that underlies advances in deep learning.\n\n---\n\n- 1967 [Nearest Neighbor](https://wikipedia.org/wiki/Nearest_neighbor) is an algorithm originally designed to map routes. In an ML context it is used to  detect patterns.\n- 1970 [Backpropagation](https://wikipedia.org/wiki/Backpropagation) is used to train [feedforward neural networks](https://wikipedia.org/wiki/Feedforward_neural_network).\n- 1982 [Recurrent Neural Networks](https://wikipedia.org/wiki/Recurrent_neural_network) are artificial neural networks derived from feedforward neural networks that create temporal graphs.\n\nâœ… Do a little research. What other dates stand out as pivotal in the history of ML and AI?\n\n---\n## 1950: Machines that think\n\nAlan Turing, a truly remarkable person who was voted [by the public in 2019](https://wikipedia.org/wiki/Icons:_The_Greatest_Person_of_the_20th_Century) as the greatest scientist of the 20th century, is credited as helping to lay the foundation for the concept of a 'machine that can think.' He grappled with naysayers and his own need for empirical evidence of this concept in part by creating the [Turing Test](https://www.bbc.com/news/technology-18475646), which you will explore in our NLP lessons.\n\n---\n## 1956: Dartmouth Summer Research Project\n\n\"The Dartmouth Summer Research Project on artificial intelligence was a seminal event for artificial intelligence as a field,\" and it was here that the term 'artificial intelligence' was coined ([source](https://250.dartmouth.edu/highlights/artificial-intelligence-ai-coined-dartmouth)).\n\n> Every aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it.\n\n---\n\nThe lead researcher, mathematics professor John McCarthy, hoped \"to proceed on the basis of the conjecture that every aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it.\" The participants included another luminary in the field, Marvin Minsky.\n\nThe workshop is credited with having initiated and encouraged several discussions including \"the rise of symbolic methods, systems focussed on limited domains (early expert systems), and deductive systems versus inductive systems.\" ([source](https://wikipedia.org/wiki/Dartmouth_workshop)).\n\n---\n## 1956 - 1974: \"The golden years\"\n\nFrom the 1950s through the mid '70s, optimism ran high in the hope that AI could solve many problems. In 1967, Marvin Minsky stated confidently that \"Within a generation ... the problem of creating 'artificial intelligence' will substantially be solved.\" (Minsky, Marvin (1967), Computation: Finite and Infinite Machines, Englewood Cliffs, N.J.: Prentice-Hall)\n\nnatural language processing research flourished, search was refined and made more powerful, and the concept of 'micro-worlds' was created, where simple tasks were completed using plain language instructions.\n\n---\n\nResearch was well funded by government agencies, advances were made in computation and algorithms, and prototypes of intelligent machines were built. Some of these machines include:\n\n* [Shakey the robot](https://wikipedia.org/wiki/Shakey_the_robot), who could maneuver and decide how to perform tasks 'intelligently'.\n\n    ![Shakey, an intelligent robot](images/shakey.jpg)\n    > Shakey in 1972\n\n---\n\n* Eliza, an early 'chatterbot', could converse with people and act as a primitive 'therapist'. You'll learn more about Eliza in the NLP lessons.\n\n    ![Eliza, a bot](images/eliza.png)\n    > A version of Eliza, a chatbot\n\n---\n\n* \"Blocks world\" was an example of a micro-world where blocks could be stacked and sorted, and experiments in teaching machines to make decisions could be tested. Advances built with libraries such as [SHRDLU](https://wikipedia.org/wiki/SHRDLU) helped propel language processing forward.\n\n    [![blocks world with SHRDLU](https://img.youtube.com/vi/QAJz4YKUwqw/0.jpg)](https://www.youtube.com/watch?v=QAJz4YKUwqw \"blocks world with SHRDLU\")\n\n    > ðŸŽ¥ Click the image above for a video: Blocks world with SHRDLU\n\n---\n## 1974 - 1980: \"AI Winter\"\n\nBy the mid 1970s, it had become apparent that the complexity of making 'intelligent machines' had been understated and that its promise, given the available compute power, had been overblown. Funding dried up and confidence in the field slowed. Some issues that impacted confidence included:\n---\n- **Limitations**. Compute power was too limited.\n- **Combinatorial explosion**. The amount of parameters needed to be trained grew exponentially as more was asked of computers, without a parallel evolution of compute power and capability.\n- **Paucity of data**. There was a paucity of data that hindered the process of testing, developing, and refining algorithms.\n- **Are we asking the right questions?**. The very questions that were being asked began to be questioned. Researchers began to field criticism about their approaches:\n  - Turing tests came into question by means, among other ideas, of the 'chinese room theory' which posited that, \"programming a digital computer may make it appear to understand language but could not produce real understanding.\" ([source](https://plato.stanford.edu/entries/chinese-room/))\n  - The ethics of introducing artificial intelligences such as the \"therapist\" ELIZA into society was challenged.\n\n---\n\nAt the same time, various AI schools of thought began to form. A dichotomy was established between [\"scruffy\" vs. \"neat AI\"](https://wikipedia.org/wiki/Neats_and_scruffies) practices. _Scruffy_ labs tweaked programs for hours until they had the desired results. _Neat_ labs \"focused on logic and formal problem solving\". ELIZA and SHRDLU were well-known _scruffy_ systems. In the 1980s, as demand emerged to make ML systems reproducible, the _neat_ approach gradually took the forefront as its results are more explainable.\n\n---\n## 1980s Expert systems\n\nAs the field grew, its benefit to business became clearer, and in the 1980s so did the proliferation of 'expert systems'. \"Expert systems were among the first truly successful forms of artificial intelligence (AI) software.\" ([source](https://wikipedia.org/wiki/Expert_system)).\n\nThis type of system is actually _hybrid_, consisting partially of a rules engine defining business requirements, and an inference engine that leveraged the rules system to deduce new facts.\n\nThis era also saw increasing attention paid to neural networks.\n\n---\n## 1987 - 1993: AI 'Chill'\n\nThe proliferation of specialized expert systems hardware had the unfortunate effect of becoming too specialized. The rise of personal computers also competed with these large, specialized, centralized systems. The democratization of computing had begun, and it eventually paved the way for the modern explosion of big data.\n\n---\n## 1993 - 2011\n\nThis epoch saw a new era for ML and AI to be able to solve some of the problems that had been caused earlier by the lack of data and compute power. The amount of data began to rapidly increase and become more widely available, for better and for worse, especially with the advent of the smartphone around 2007. Compute power expanded exponentially, and algorithms evolved alongside. The field began to gain maturity as the freewheeling days of the past began to crystallize into a true discipline.\n\n---\n## Now\n\nToday machine learning and AI touch almost every part of our lives. This era calls for careful understanding of the risks and potentials effects of these algorithms on human lives. As Microsoft's Brad Smith has stated, \"Information technology raises issues that go to the heart of fundamental human-rights protections like privacy and freedom of expression. These issues heighten responsibility for tech companies that create these products. In our view, they also call for thoughtful government regulation and for the development of norms around acceptable uses\" ([source](https://www.technologyreview.com/2019/12/18/102365/the-future-of-ais-impact-on-society/)).\n\n---\n\nIt remains to be seen what the future holds, but it is important to understand these computer systems and the software and algorithms that they run. We hope that this curriculum will help you to gain a better understanding so that you can decide for yourself.\n\n[![The history of deep learning](https://img.youtube.com/vi/mTtDfKgLm54/0.jpg)](https://www.youtube.com/watch?v=mTtDfKgLm54 \"The history of deep learning\")\n> ðŸŽ¥ Click the image above for a video: Yann LeCun discusses the history of deep learning in this lecture\n\n---\n## ðŸš€Challenge\n\nDig into one of these historical moments and learn more about the people behind them. There are fascinating characters, and no scientific discovery was ever created in a cultural vacuum. What do you discover?\n\n## [Post-lecture quiz](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/4/)\n\n---\n## Review & Self Study\n\nHere are items to watch and listen to:\n\n[This podcast where Amy Boyd discusses the evolution of AI](http://runasradio.com/Shows/Show/739)\n\n[![The history of AI by Amy Boyd](https://img.youtube.com/vi/EJt3_bFYKss/0.jpg)](https://www.youtube.com/watch?v=EJt3_bFYKss \"The history of AI by Amy Boyd\")\n\n---\n\n## Assignment\n\n[Create a timeline](assignment.md)\n",
              "pdfUrl": "",
              "videoUrl": "",
              "orderIndex": 1
            }
          ],
          "quizzes": [],
          "subModules": []
        },
        {
          "id": 9,
          "title": "Fairness",
          "orderIndex": 2,
          "lessons": [
            {
              "id": 10,
              "title": "Fairness",
              "content": "# Explore the Responsible AI Toolbox\n\n## Instructions\n\nIn this lesson you learned about the Responsible AI Toolbox, an \"open-source, community-driven project to help data scientists to analyze and improve AI systems.\" For this assignment, explore one of RAI Toolbox's [notebooks](https://github.com/microsoft/responsible-ai-toolbox/blob/main/notebooks/responsibleaidashboard/getting-started.ipynb) and report your findings in a paper or presentation.\n\n## Rubric\n\n| Criteria | Exemplary | Adequate | Needs Improvement |\n| -------- | --------- | -------- | ----------------- |\n|          |  A paper or powerpoint presentation is presented discussing Fairlearn's systems, the notebook that was run, and the conclusions drawn from running it        |   A paper is presented without conclusions       |  No paper is presented                 |\n",
              "pdfUrl": "",
              "videoUrl": "",
              "orderIndex": 0
            },
            {
              "id": 11,
              "title": "Fairness",
              "content": "# Building Machine Learning solutions with responsible AI\n \n![Summary of responsible AI in Machine Learning in a sketchnote](../../sketchnotes/ml-fairness.png)\n> Sketchnote by [Tomomi Imura](https://www.twitter.com/girlie_mac)\n\n## [Pre-lecture quiz](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/5/)\n \n## Introduction\n\nIn this curriculum, you will start to discover how machine learning can and is impacting our everyday lives. Even now, systems and models are involved in daily decision-making tasks, such as health care diagnoses, loan approvals or detecting fraud. So, it is important that these models work well to provide outcomes that are trustworthy. Just as any software application, AI systems are going to miss expectations or have an undesirable outcome. That is why it is essential to be about to understand and explain the behavior of an AI model. \n\nImagine what can happen when the data you are using to build these models lacks certain demographics, such as race, gender, political view, religion, or disproportionally represents such demographics. What about when the modelâ€™s output is interpreted to favor some demographic? What is the consequence for the application? In addition, what happens when the model has an adverse outcome and is harmful to people? Who is accountable for the AI systems behavior? These are some questions we will explore in this curriculum. \n\nIn this lesson, you will: \n\n- Raise your awareness of the importance of fairness in machine learning and fairness-related harms.\n- Become familiar with the practice of exploring outliers and unusual scenarios to ensure reliability and safety\n- Gain understanding on the need to empower everyone by designing inclusive systems\n- Explore how vital it is to protect privacy and security of data and people\n- See the importance of having a glass box approach to explain the behavior of AI models\n- Be mindful of how accountability is essential to build trust in AI systems\n\n## Prerequisite\n\nAs a prerequisite, please take the \"Responsible AI Principles\" Learn Path and watch the video below on the topic:\n\nLearn more about Responsible AI by following this [Learning Path](https://docs.microsoft.com/learn/modules/responsible-ai-principles/?WT.mc_id=academic-77952-leestott)\n\n[![Microsoft's Approach to Responsible AI](https://img.youtube.com/vi/dnC8-uUZXSc/0.jpg)](https://youtu.be/dnC8-uUZXSc \"Microsoft's Approach to Responsible AI\")\n\n> ðŸŽ¥ Click the image above for a video: Microsoft's Approach to Responsible AI\n\n## Fairness\n\nAI systems should treat everyone fairly and avoid affecting similar groups of people in different ways. For example, when AI systems provide guidance on medical treatment, loan applications, or employment, they should make the same recommendations to everyone with similar symptoms, financial circumstances, or professional qualifications. Each of us as humans carries around inherited biases that affect our decisions and actions. These biases can be evident in the data that we use to train AI systems. Such manipulation can sometimes happen unintentionally. It is often difficult to consciously know when you are introducing bias in data. \n\n**â€œUnfairnessâ€** encompasses negative impacts, or â€œharmsâ€, for a group of people, such as those defined in terms of race, gender, age, or disability status. The main fairness-related harms can be classified as: \n\n- **Allocation**, if a gender or ethnicity for example is favored over another.\n- **Quality of service**. If you train the data for one specific scenario but reality is much more complex, it leads to a poor performing service.  For instance, a hand soap dispenser that could not seem to be able to sense people with dark skin. [Reference](https://gizmodo.com/why-cant-this-soap-dispenser-identify-dark-skin-1797931773)\n- **Denigration**. To unfairly criticize and label something or someone. For example, an image labeling technology infamously mislabeled images of dark-skinned people as gorillas.\n- **Over- or under- representation**. The idea is that a certain group is not seen in a certain profession, and any service or function that keeps promoting that is contributing to harm.\n- **Stereotyping**. Associating a given group with pre-assigned attributes.  For example, a language translation system betweem English and Turkish may have inaccuraces due to words with stereotypical associations to gender.\n\n![translation to Turkish](images/gender-bias-translate-en-tr.png)\n> translation to Turkish\n\n![translation back to English](images/gender-bias-translate-tr-en.png)\n> translation back to English\n\nWhen designing and testing AI systems, we need to ensure that AI is fair and not programmed to make biased or discriminatory decisions, which human beings are also prohibited from making. Guaranteeing fairness in AI and machine learning remains a complex sociotechnical challenge. \n\n### Reliability and safety\n\nTo build trust, AI systems need to be reliable, safe, and consistent under normal and unexpected conditions. It is important to know how AI systems will behavior in a variety of situations, especially when they are outliers. When building AI solutions, there needs to be a substantial amount of focus on how to handle a wide variety of circumstances that the AI solutions would encounter. For example, a self-driving car needs to put people's safety as a top priority. As a result, the AI powering the car need to consider all the possible scenarios that the car could come across such as night, thunderstorms or blizzards, kids running across the street, pets, road constructions etc. How well an AI system can handle a wild range of conditions reliably and safely reflects the level of anticipation the data scientist or AI developer considered during the design or testing of the system.  \n\n> [ðŸŽ¥ Click the here for a video: ](https://www.microsoft.com/videoplayer/embed/RE4vvIl)\n\n### Inclusiveness\n\nAI systems should be designed to engage and empower everyone. When designing and implementing AI systems data scientists and AI developers identify and address potential barriers in the system that could unintentionally exclude people. For example, there are 1 billion people with disabilities around the world. With the advancement of AI, they can access a wide range of information and opportunities more easily in their daily lives. By addressing the barriers, it creates opportunities to innovate and develop AI products with better experiences that benefit everyone. \n\n> [ðŸŽ¥ Click the here for a video: inclusiveness in AI](https://www.microsoft.com/videoplayer/embed/RE4vl9v)\n\n### Security and privacy \n\nAI systems should be safe and respect peopleâ€™s privacy. People have less trust in systems that put their privacy, information, or lives at risk. When training machine learning models, we rely on data to produce the best results. In doing so, the origin of the data and integrity must be considered. For example, was the data user submitted or publicly available? Next, while working with the data, it is crucial to develop AI systems that can protect confidential information and resist attacks. As AI becomes more prevalent, protecting privacy and securing important personal and business information is becoming more critical and complex. Privacy and data security issues require especially close attention for AI because access to data is essential for AI systems to make accurate and informed predictions and decisions about people. \n\n> [ðŸŽ¥ Click the here for a video: security in AI](https://www.microsoft.com/videoplayer/embed/RE4voJF)\n\n- As an industry we have made significant advancements in Privacy & security, fueled significantly by regulations like the GDPR (General Data Protection Regulation). \n- Yet with AI systems we must acknowledge the tension between the need for more personal data to make systems more personal and effective â€“ and privacy. \n- Just like with the birth of connected computers with the internet, we are also seeing a huge uptick in the number of security issues related to AI. \n- At the same time, we have seen AI being used to improve security. As an example, most modern anti-virus scanners are driven by AI heuristics today. \n- We need to ensure that our Data Science processes blend harmoniously with the latest privacy and security practices. \n\n\n### Transparency\nAI systems should be understandable. A crucial part of transparency is explaining the behavior of AI systems and their components. Improving the understanding of AI systems requires that stakeholders comprehend how and why they function so that they can identify potential performance issues, safety and privacy concerns, biases, exclusionary practices, or unintended outcomes. We also believe that those who use AI systems should be honest and forthcoming about when, why, and how they choose to deploy them. As well as the limitations of the systems they use. For example, if a bank uses an AI system to support its consumer lending decisions, it is important to examine the outcomes and understand which data influences the systemâ€™s recommendations. Governments are starting to regulate AI across industries, so data scientists and organizations must explain if an AI system meets regulatory requirements, especially when there is an undesirable outcome. \n\n> [ðŸŽ¥ Click the here for a video: transparency in AI](https://www.microsoft.com/videoplayer/embed/RE4voJF)\n\n- Because AI systems are so complex, it is hard to understand how they work and interpret the results. \n- This lack of understanding affects the way these systems are managed, operationalized, and documented. \n- This lack of understanding more importantly affects the decisions made using the results these systems produce. \n\n### Accountability \n \nThe people who design and deploy AI systems must be accountable for how their systems operate. The need for accountability is particularly crucial with sensitive use technologies like facial recognition. Recently, there has been a growing demand for facial recognition technology, especially from law enforcement organizations who see the potential of the technology in uses like finding missing children. However, these technologies could potentially be used by a government to put their citizensâ€™ fundamental freedoms at risk by, for example, enabling continuous surveillance of specific individuals. Hence, data scientists and organizations need to be responsible for how their AI system impacts individuals or society.\n\n[![Leading AI Researcher Warns of Mass Surveillance Through Facial Recognition](images/accountability.png)](https://www.youtube.com/watch?v=Wldt8P5V6D0 \"Microsoft's Approach to Responsible AI\")\n\n> ðŸŽ¥ Click the image above for a video: Warnings of Mass Surveillance Through Facial Recognition \n\nUltimately one of the biggest questions for our generation, as the first generation that is bringing AI to society, is how to ensure that computers will remain accountable to people and how to ensure that the people that design computers remain accountable to everyone else.\n\n## Impact assessment \n\nBefore training a machine learning model, it is important to conduct an impact assessmet to understand the purpose of the AI system; what the intended use is; where it will be deployed; and who will be interacting with the system.  These are helpful for reviewer(s) or testers evaluating the system to know what factors to take into consideration when identifying potential risks and expected consequences.\n\nThe following are areas of focus when conducting an impact assessment:\n\n* **Adverse impact on individuals**.  Being aware of any restriction or requirements, unsupported use or any known limitations hindering the system's performance is vital to ensure that the system is not used in a way that could cause harm to individuals.\n* **Data requirements**.  Gaining an understanding of how and where the system will use data enables reviewers to explore any data requirements you would need to be mindful of (e.g., GDPR or HIPPA data regulations).  In addition, examine whether the source or quantity of data is substantial for training.\n* **Summary of impact**.  Gather a list of potential harms that could  arise from using the system.  Throughout the ML lifecycle, review if the issues identified are mitigated or addressed.\n* **Applicable goals** for each of the six core principles.  Assess if the goals from each of the principles are met and if there are any gaps.\n\n\n## Debugging with responsible AI  \n\nSimilar to debugging a software application, debugging an AI system is a necessary process of identifying and resolving issues in the system.  There are many factors that would affect a model not performing as expected or responsibly.  Most traditional model performance metrics are quantitative aggregates of a model's performance, which are not sufficient to analyze how a model violates the responsible AI principles. Furthermore, a machine learning model is a black box that makes it difficult to understand what drives its outcome or provide explanation when it makes a mistake.  Later in this course, we will learn how to use the Responsible AI dashboard to help debug AI systems.  The dashboard provides a holistic tool for data scientists and AI developers to perform:\n\n* **Error analysis**.  To identify the error distribution of the model that can affect the system's fairness or reliability.\n* **Model overview**. To discover where there are disparities in the model's performance across data cohorts.\n* **Data analysis**.  To understand the data distribution and identify any potential bias in the data that could lead to fairness, inclusiveness, and reliability issues.\n* **Model interpretability**. To understand what affects or influences the model's predictions. This helps in explaining the model's behavior, which is important for transparency and accountability.\n\n\n## ðŸš€ Challenge \n \nTo prevent harms from being introduced in the first place, we should: \n\n- have a diversity of backgrounds and perspectives among the people working on systems \n- invest in datasets that reflect the diversity of our society \n- develop better methods throughout the machine learning lifecycle for detecting and correcting responible AI when it occurs \n\nThink about real-life scenarios where a model's untrustworthiness is evident in model-building and usage. What else should we consider? \n\n## [Post-lecture quiz](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/6/)\n## Review & Self Study \n \nIn this lesson, you have learned some basics of the concepts of fairness and unfairness in machine learning.  \n \nWatch this workshop to dive deeper into the topics: \n\n- In pursuit of responsible AI: Bringing principles to practice by Besmira Nushi, Mehrnoosh Sameki and Amit Sharma\n\n[![Responsible AI Toolbox: An open-source framework for building responsible AI](https://img.youtube.com/vi/tGgJCrA-MZU/0.jpg)](https://www.youtube.com/watch?v=tGgJCrA-MZU \"RAI Toolbox: An open-source framework for building responsible AI\")\n\n> ðŸŽ¥ Click the image above for a video: RAI Toolbox: An open-source framework for building responsible AI by Besmira Nushi, Mehrnoosh Sameki, and Amit Sharma\n\nAlso, read: \n\n- Microsoftâ€™s RAI resource center: [Responsible AI Resources â€“ Microsoft AI](https://www.microsoft.com/ai/responsible-ai-resources?activetab=pivot1%3aprimaryr4) \n\n- Microsoftâ€™s FATE research group: [FATE: Fairness, Accountability, Transparency, and Ethics in AI - Microsoft Research](https://www.microsoft.com/research/theme/fate/) \n\nRAI Toolbox: \n\n- [Responsible AI Toolbox GitHub repository](https://github.com/microsoft/responsible-ai-toolbox)\n\nRead about Azure Machine Learning's tools to ensure fairness:\n\n- [Azure Machine Learning](https://docs.microsoft.com/azure/machine-learning/concept-fairness-ml?WT.mc_id=academic-77952-leestott) \n\n## Assignment\n\n[Explore RAI Toolbox](assignment.md) \n",
              "pdfUrl": "",
              "videoUrl": "",
              "orderIndex": 1
            }
          ],
          "quizzes": [],
          "subModules": []
        },
        {
          "id": 12,
          "title": "Techniques Of ML",
          "orderIndex": 3,
          "lessons": [
            {
              "id": 13,
              "title": "Techniques Of ML",
              "content": "# Interview a data scientist\n\n## Instructions\n\nIn your company, in a user group, or among your friends or fellow students, talk to someone who works professionally as a data scientist. Write a short paper (500 words) about their daily occupations. Are they specialists, or do they work 'full stack'?\n\n## Rubric\n\n| Criteria | Exemplary                                                                            | Adequate                                                           | Needs Improvement     |\n| -------- | ------------------------------------------------------------------------------------ | ------------------------------------------------------------------ | --------------------- |\n|          | An essay of the correct length, with attributed sources, is presented as a .doc file | The essay is poorly attributed or shorter than the required length | No essay is presented |\n",
              "pdfUrl": "",
              "videoUrl": "",
              "orderIndex": 0
            },
            {
              "id": 14,
              "title": "Techniques Of ML",
              "content": "# Techniques of Machine Learning\n\nThe process of building, using, and maintaining machine learning models and the data they use is a very different process from many other development workflows. In this lesson, we will demystify the process, and outline the main techniques you need to know. You will:\n\n- Understand the processes underpinning machine learning at a high level.\n- Explore base concepts such as 'models', 'predictions', and 'training data'.\n\n## [Pre-lecture quiz](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/7/)\n\n[![ML for beginners - Techniques of Machine Learning](https://img.youtube.com/vi/4NGM0U2ZSHU/0.jpg)](https://youtu.be/4NGM0U2ZSHU \"ML for beginners - Techniques of Machine Learning\")\n\n> ðŸŽ¥ Click the image above for a short video working through this lesson.\n\n## Introduction\n\nOn a high level, the craft of creating machine learning (ML) processes is comprised of a number of steps:\n\n1. **Decide on the question**. Most ML processes start by asking a question that cannot be answered by a simple conditional program or rules-based engine. These questions often revolve around predictions based on a collection of data.\n2. **Collect and prepare data**. To be able to answer your question, you need data. The quality and, sometimes, quantity of your data will determine how well you can answer your initial question. Visualizing data is an important aspect of this phase. This phase also includes splitting the data into a training and testing group to build a model.\n3. **Choose a training method**. Depending on your question and the nature of your data, you need to choose how you want to train a model to best reflect your data and make accurate predictions against it. This is the part of your ML process that requires specific expertise and, often, a considerable amount of experimentation.\n4. **Train the model**. Using your training data, you'll use various algorithms to train a model to recognize patterns in the data. The model might leverage internal weights that can be adjusted to privilege certain parts of the data over others to build a better model.\n5. **Evaluate the model**. You use never before seen data (your testing data) from your collected set to see how the model is performing.\n6. **Parameter tuning**. Based on the performance of your model, you can redo the process using different parameters, or variables, that control the behavior of the algorithms used to train the model.\n7. **Predict**. Use new inputs to test the accuracy of your model.\n\n## What question to ask\n\nComputers are particularly skilled at discovering hidden patterns in data. This utility is very helpful for researchers who have questions about a given domain that cannot be easily answered by creating a conditionally-based rules engine. Given an actuarial task, for example, a data scientist might be able to construct handcrafted rules around the mortality of smokers vs non-smokers.\n\nWhen many other variables are brought into the equation, however, a ML model might prove more efficient to predict future mortality rates based on past health history. A more cheerful example might be making weather predictions for the month of April in a given location based on data that includes latitude, longitude, climate change, proximity to the ocean, patterns of the jet stream, and more.\n\nâœ… This [slide deck](https://www2.cisl.ucar.edu/sites/default/files/2021-10/0900%20June%2024%20Haupt_0.pdf) on weather models offers a historical perspective for using ML in weather analysis.  \n\n## Pre-building tasks\n\nBefore starting to build your model, there are several tasks you need to complete. To test your question and form a hypothesis based on a model's predictions, you need to identify and configure several elements.\n\n### Data\n\nTo be able to answer your question with any kind of certainty, you need a good amount of data of the right type. There are two things you need to do at this point:\n\n- **Collect data**. Keeping in mind the previous lesson on fairness in data analysis, collect your data with care. Be aware of the sources of this data, any inherent biases it might have, and document its origin.\n- **Prepare data**. There are several steps in the data preparation process. You might need to collate data and normalize it if it comes from diverse sources. You can improve the data's quality and quantity through various methods such as converting strings to numbers (as we do in [Clustering](../../5-Clustering/1-Visualize/README.md)). You might also generate new data, based on the original (as we do in [Classification](../../4-Classification/1-Introduction/README.md)). You can clean and edit the data (as we will prior to the [Web App](../../3-Web-App/README.md) lesson). Finally, you might also need to randomize it and shuffle it, depending on your training techniques.\n\nâœ… After collecting and processing your data, take a moment to see if its shape will allow you to address your intended question. It may be that the data will not perform well in your given task, as we discover in our [Clustering](../../5-Clustering/1-Visualize/README.md) lessons!\n\n### Features and Target\n\nA [feature](https://www.datasciencecentral.com/profiles/blogs/an-introduction-to-variable-and-feature-selection) is a measurable property of your data. In many datasets it is expressed as a column heading like 'date' 'size' or 'color'. Your feature variable, usually represented as `X` in code, represent the input variable which will be used to train model.\n\nA target is a thing you are trying to predict. Target usually represented as `y` in code, represents the answer to the question you are trying to ask of your data: in December, what **color** pumpkins will be cheapest? in San Francisco, what neighborhoods will have the best real estate **price**? Sometimes target is also referred as label attribute.\n\n### Selecting your feature variable\n\nðŸŽ“ **Feature Selection and Feature Extraction** How do you know which variable to choose when building a model? You'll probably go through a process of feature selection or feature extraction to choose the right variables for the most performant model. They're not the same thing, however: \"Feature extraction creates new features from functions of the original features, whereas feature selection returns a subset of the features.\" ([source](https://wikipedia.org/wiki/Feature_selection))\n\n### Visualize your data\n\nAn important aspect of the data scientist's toolkit is the power to visualize data using several excellent libraries such as Seaborn or MatPlotLib. Representing your data visually might allow you to uncover hidden correlations that you can leverage. Your visualizations might also help you to uncover bias or unbalanced data (as we discover in [Classification](../../4-Classification/2-Classifiers-1/README.md)).\n\n### Split your dataset\n\nPrior to training, you need to split your dataset into two or more parts of unequal size that still represent the data well.\n\n- **Training**. This part of the dataset is fit to your model to train it. This set constitutes the majority of the original dataset.\n- **Testing**. A test dataset is an independent group of data, often gathered from the original data, that you use to confirm the performance of the built model.\n- **Validating**. A validation set is a smaller independent group of examples that you use to tune the model's hyperparameters, or architecture, to improve the model. Depending on your data's size and the question you are asking, you might not need to build this third set (as we note in [Time Series Forecasting](../../7-TimeSeries/1-Introduction/README.md)).\n\n## Building a model\n\nUsing your training data, your goal is to build a model, or a statistical representation of your data, using various algorithms to **train** it. Training a model exposes it to data and allows it to make assumptions about perceived patterns it discovers, validates, and accepts or rejects.\n\n### Decide on a training method\n\nDepending on your question and the nature of your data, you will choose a method to train it. Stepping through [Scikit-learn's documentation](https://scikit-learn.org/stable/user_guide.html) - which we use in this course - you can explore many ways to train a model. Depending on your experience, you might have to try several different methods to build the best model. You are likely to go through a process whereby data scientists evaluate the performance of a model by feeding it unseen data, checking for accuracy, bias, and other quality-degrading issues, and selecting the most appropriate training method for the task at hand.\n\n### Train a model\n\nArmed with your training data, you are ready to 'fit' it to create a model. You will notice that in many ML libraries you will find the code 'model.fit' - it is at this time that you send in your feature variable as an array of values (usually 'X') and a target variable (usually 'y').\n\n### Evaluate the model\n\nOnce the training process is complete (it can take many iterations, or 'epochs', to train a large model), you will be able to evaluate the model's quality by using test data to gauge its performance. This data is a subset of the original data that the model has not previously analyzed. You can print out a table of metrics about your model's quality.\n\nðŸŽ“ **Model fitting**\n\nIn the context of machine learning, model fitting refers to the accuracy of the model's underlying function as it attempts to analyze data with which it is not familiar.\n\nðŸŽ“ **Underfitting** and **overfitting** are common problems that degrade the quality of the model, as the model fits either not well enough or too well. This causes the model to make predictions either too closely aligned or too loosely aligned with its training data. An overfit model predicts training data too well because it has learned the data's details and noise too well. An underfit model is not accurate as it can neither accurately analyze its training data nor data it has not yet 'seen'.\n\n![overfitting model](images/overfitting.png)\n> Infographic by [Jen Looper](https://twitter.com/jenlooper)\n\n## Parameter tuning\n\nOnce your initial training is complete, observe the quality of the model and consider improving it by tweaking its 'hyperparameters'. Read more about the process [in the documentation](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-tune-hyperparameters?WT.mc_id=academic-77952-leestott).\n\n## Prediction\n\nThis is the moment where you can use completely new data to test your model's accuracy. In an 'applied' ML setting, where you are building web assets to use the model in production, this process might involve gathering user input (a button press, for example) to set a variable and send it to the model for inference, or evaluation.\n\nIn these lessons, you will discover how to use these steps to prepare, build, test, evaluate, and predict - all the gestures of a data scientist and more, as you progress in your journey to become a 'full stack' ML engineer.\n\n---\n\n## ðŸš€Challenge\n\nDraw a flow chart reflecting the steps of a ML practitioner. Where do you see yourself right now in the process? Where do you predict you will find difficulty? What seems easy to you?\n\n## [Post-lecture quiz](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/8/)\n\n## Review & Self Study\n\nSearch online for interviews with data scientists who discuss their daily work. Here is [one](https://www.youtube.com/watch?v=Z3IjgbbCEfs).\n\n## Assignment\n\n[Interview a data scientist](assignment.md)\n",
              "pdfUrl": "",
              "videoUrl": "",
              "orderIndex": 1
            }
          ],
          "quizzes": [],
          "subModules": []
        }
      ]
    },
    {
      "id": 15,
      "title": "Regression",
      "orderIndex": 1,
      "lessons": [
        {
          "id": 16,
          "title": "Regression",
          "content": "# Regression models for machine learning\n## Regional topic: Regression models for pumpkin prices in North America ðŸŽƒ\n\nIn North America, pumpkins are often carved into scary faces for Halloween. Let's discover more about these fascinating vegetables!\n\n![jack-o-lanterns](./images/jack-o-lanterns.jpg)\n> Photo by <a href=\"https://unsplash.com/@teutschmann?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">Beth Teutschmann</a> on <a href=\"https://unsplash.com/s/photos/jack-o-lanterns?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">Unsplash</a>\n  \n## What you will learn\n\n[![Introduction to Regression](https://img.youtube.com/vi/5QnJtDad4iQ/0.jpg)](https://youtu.be/5QnJtDad4iQ \"Regression Introduction video - Click to Watch!\")\n> ðŸŽ¥ Click the image above for a quick introduction video to this lesson\n\nThe lessons in this section cover types of regression in the context of machine learning. Regression models can help determine the _relationship_ between variables. This type of model can predict values such as length, temperature, or age, thus uncovering relationships between variables as it analyzes data points.\n\nIn this series of lessons, you'll discover the differences between linear and logistic regression, and when you should prefer one over the other.\n\n[![ML for beginners - Introduction to Regression models for Machine Learning](https://img.youtube.com/vi/XA3OaoW86R8/0.jpg)](https://youtu.be/XA3OaoW86R8 \"ML for beginners - Introduction to Regression models for Machine Learning\")\n\n> ðŸŽ¥ Click the image above for a short video introducing regression models.\n\nIn this group of lessons, you will get set up to begin machine learning tasks, including configuring Visual Studio Code to manage notebooks, the common environment for data scientists. You will discover Scikit-learn, a library for machine learning, and you will build your first models, focusing on Regression models in this chapter.\n\n> There are useful low-code tools that can help you learn about working with regression models. Try [Azure ML for this task](https://docs.microsoft.com/learn/modules/create-regression-model-azure-machine-learning-designer/?WT.mc_id=academic-77952-leestott)\n\n### Lessons\n\n1. [Tools of the trade](1-Tools/README.md)\n2. [Managing data](2-Data/README.md)\n3. [Linear and polynomial regression](3-Linear/README.md)\n4. [Logistic regression](4-Logistic/README.md)\n\n---\n### Credits\n\n\"ML with regression\" was written with â™¥ï¸ by [Jen Looper](https://twitter.com/jenlooper)\n\nâ™¥ï¸ Quiz contributors include: [Muhammad Sakib Khan Inan](https://twitter.com/Sakibinan) and [Ornella Altunyan](https://twitter.com/ornelladotcom)\n\nThe pumpkin dataset is suggested by [this project on Kaggle](https://www.kaggle.com/usda/a-year-of-pumpkin-prices) and its data is sourced from the [Specialty Crops Terminal Markets Standard Reports](https://www.marketnews.usda.gov/mnp/fv-report-config-step1?type=termPrice) distributed by the United States Department of Agriculture. We have added some points around color based on variety to normalize the distribution. This data is in the public domain.\n",
          "pdfUrl": "",
          "videoUrl": "",
          "orderIndex": 0
        }
      ],
      "quizzes": [],
      "subModules": [
        {
          "id": 17,
          "title": "Tools",
          "orderIndex": 0,
          "lessons": [
            {
              "id": 18,
              "title": "Tools",
              "content": "# Regression with Scikit-learn\n\n## Instructions\n\nTake a look at the [Linnerud dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_linnerud.html#sklearn.datasets.load_linnerud) in Scikit-learn. This dataset has multiple [targets](https://scikit-learn.org/stable/datasets/toy_dataset.html#linnerrud-dataset): 'It consists of three exercise (data) and three physiological (target) variables collected from twenty middle-aged men in a fitness club'.\n\nIn your own words, describe how to create a Regression model that would plot the relationship between the waistline and how many situps are accomplished. Do the same for the other datapoints in this dataset.\n\n## Rubric\n\n| Criteria                       | Exemplary                           | Adequate                      | Needs Improvement          |\n| ------------------------------ | ----------------------------------- | ----------------------------- | -------------------------- |\n| Submit a descriptive paragraph | Well-written paragraph is submitted | A few sentences are submitted | No description is supplied |\n",
              "pdfUrl": "",
              "videoUrl": "",
              "orderIndex": 0
            },
            {
              "id": 19,
              "title": "Tools",
              "content": "# Get started with Python and Scikit-learn for regression models\n\n![Summary of regressions in a sketchnote](../../sketchnotes/ml-regression.png)\n\n> Sketchnote by [Tomomi Imura](https://www.twitter.com/girlie_mac)\n\n## [Pre-lecture quiz](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/9/)\n\n> ### [This lesson is available in R!](./solution/R/lesson_1.html)\n\n## Introduction\n\nIn these four lessons, you will discover how to build regression models. We will discuss what these are for shortly. But before you do anything, make sure you have the right tools in place to start the process!\n\nIn this lesson, you will learn how to:\n\n- Configure your computer for local machine learning tasks.\n- Work with Jupyter notebooks.\n- Use Scikit-learn, including installation.\n- Explore linear regression with a hands-on exercise.\n\n## Installations and configurations\n\n[![ML for beginners - Setup your tools ready to build Machine Learning models](https://img.youtube.com/vi/-DfeD2k2Kj0/0.jpg)](https://youtu.be/-DfeD2k2Kj0 \"ML for beginners -Setup your tools ready to build Machine Learning models\")\n\n> ðŸŽ¥ Click the image above for a short video working through configuring your computer for ML.\n\n1. **Install Python**. Ensure that [Python](https://www.python.org/downloads/) is installed on your computer. You will use Python for many data science and machine learning tasks. Most computer systems already include a Python installation. There are useful [Python Coding Packs](https://code.visualstudio.com/learn/educators/installers?WT.mc_id=academic-77952-leestott) available as well, to ease the setup for some users.\n\n   Some usages of Python, however, require one version of the software, whereas others require a different version. For this reason, it's useful to work within a [virtual environment](https://docs.python.org/3/library/venv.html).\n\n2. **Install Visual Studio Code**. Make sure you have Visual Studio Code installed on your computer. Follow these instructions to [install Visual Studio Code](https://code.visualstudio.com/) for the basic installation. You are going to use Python in Visual Studio Code in this course, so you might want to brush up on how to [configure Visual Studio Code](https://docs.microsoft.com/learn/modules/python-install-vscode?WT.mc_id=academic-77952-leestott) for Python development.\n\n   > Get comfortable with Python by working through this collection of [Learn modules](https://docs.microsoft.com/users/jenlooper-2911/collections/mp1pagggd5qrq7?WT.mc_id=academic-77952-leestott)\n   >\n   > [![Setup Python with Visual Studio Code](https://img.youtube.com/vi/yyQM70vi7V8/0.jpg)](https://youtu.be/yyQM70vi7V8 \"Setup Python with Visual Studio Code\")\n   >\n   > ðŸŽ¥ Click the image above for a video: using Python within VS Code.\n\n3. **Install Scikit-learn**, by following [these instructions](https://scikit-learn.org/stable/install.html). Since you need to ensure that you use Python 3, it's recommended that you use a virtual environment. Note, if you are installing this library on a M1 Mac, there are special instructions on the page linked above.\n\n1. **Install Jupyter Notebook**. You will need to [install the Jupyter package](https://pypi.org/project/jupyter/).\n\n## Your ML authoring environment\n\nYou are going to use **notebooks** to develop your Python code and create machine learning models. This type of file is a common tool for data scientists, and they can be identified by their suffix or extension `.ipynb`.\n\nNotebooks are an interactive environment that allow the developer to both code and add notes and write documentation around the code which is quite helpful for experimental or research-oriented projects.\n\n[![ML for beginners - Set up Jupyter Notebooks to start building regression models](https://img.youtube.com/vi/7E-jC8FLA2E/0.jpg)](https://youtu.be/7E-jC8FLA2E \"ML for beginners - Set up Jupyter Notebooks to start building regression models\")\n\n> ðŸŽ¥ Click the image above for a short video working through this exercise.\n\n### Exercise - work with a notebook\n\nIn this folder, you will find the file _notebook.ipynb_.\n\n1. Open _notebook.ipynb_ in Visual Studio Code.\n\n   A Jupyter server will start with Python 3+ started. You will find areas of the notebook that can be `run`, pieces of code. You can run a code block, by selecting the icon that looks like a play button.\n\n1. Select the `md` icon and add a bit of markdown, and the following text **# Welcome to your notebook**.\n\n   Next, add some Python code.\n\n1. Type **print('hello notebook')** in the code block.\n1. Select the arrow to run the code.\n\n   You should see the printed statement:\n\n    ```output\n    hello notebook\n    ```\n\n![VS Code with a notebook open](images/notebook.jpg)\n\nYou can interleaf your code with comments to self-document the notebook.\n\nâœ… Think for a minute how different a web developer's working environment is versus that of a data scientist.\n\n## Up and running with Scikit-learn\n\nNow that Python is set up in your local environment, and you are comfortable with Jupyter notebooks, let's get equally comfortable with Scikit-learn (pronounce it `sci` as in `science`). Scikit-learn provides an [extensive API](https://scikit-learn.org/stable/modules/classes.html#api-ref) to help you perform ML tasks.\n\nAccording to their [website](https://scikit-learn.org/stable/getting_started.html), \"Scikit-learn is an open source machine learning library that supports supervised and unsupervised learning. It also provides various tools for model fitting, data preprocessing, model selection and evaluation, and many other utilities.\"\n\nIn this course, you will use Scikit-learn and other tools to build machine learning models to perform what we call 'traditional machine learning' tasks. We have deliberately avoided neural networks and deep learning, as they are better covered in our forthcoming 'AI for Beginners' curriculum.\n\nScikit-learn makes it straightforward to build models and evaluate them for use. It is primarily focused on using numeric data and contains several ready-made datasets for use as learning tools. It also includes pre-built models for students to try. Let's explore the process of loading prepackaged data and using a built in estimator  first ML model with Scikit-learn with some basic data.\n\n## Exercise - your first Scikit-learn notebook\n\n> This tutorial was inspired by the [linear regression example](https://scikit-learn.org/stable/auto_examples/linear_model/plot_ols.html#sphx-glr-auto-examples-linear-model-plot-ols-py) on Scikit-learn's web site.\n\n\n[![ML for beginners - Your First Linear Regression Project in Python](https://img.youtube.com/vi/2xkXL5EUpS0/0.jpg)](https://youtu.be/2xkXL5EUpS0 \"ML for beginners - Your First Linear Regression Project in Python\")\n\n> ðŸŽ¥ Click the image above for a short video working through this exercise.\n\nIn the _notebook.ipynb_ file associated to this lesson, clear out all the cells by pressing the 'trash can' icon.\n\nIn this section, you will work with a small dataset about diabetes that is built into Scikit-learn for learning purposes. Imagine that you wanted to test a treatment for diabetic patients. Machine Learning models might help you determine which patients would respond better to the treatment, based on combinations of variables. Even a very basic regression model, when visualized, might show information about variables that would help you organize your theoretical clinical trials.\n\nâœ… There are many types of regression methods, and which one you pick depends on the answer you're looking for. If you want to predict the probable height for a person of a given age, you'd use linear regression, as you're seeking a **numeric value**. If you're interested in discovering whether a type of cuisine should be considered vegan or not, you're looking for a **category assignment** so you would use logistic regression. You'll learn more about logistic regression later. Think a bit about some questions you can ask of data, and which of these methods would be more appropriate.\n\nLet's get started on this task.\n\n### Import libraries\n\nFor this task we will import some libraries:\n\n- **matplotlib**. It's a useful [graphing tool](https://matplotlib.org/) and we will use it to create a line plot.\n- **numpy**. [numpy](https://numpy.org/doc/stable/user/whatisnumpy.html) is a useful library for handling numeric data in Python.\n- **sklearn**. This is the [Scikit-learn](https://scikit-learn.org/stable/user_guide.html) library.\n\nImport some libraries to help with your tasks.\n\n1. Add imports by typing the following code:\n\n   ```python\n   import matplotlib.pyplot as plt\n   import numpy as np\n   from sklearn import datasets, linear_model, model_selection\n   ```\n\n   Above you are importing `matplotlib`, `numpy` and you are importing `datasets`, `linear_model` and `model_selection` from `sklearn`. `model_selection` is used for splitting data into training and test sets.\n\n### The diabetes dataset\n\nThe built-in [diabetes dataset](https://scikit-learn.org/stable/datasets/toy_dataset.html#diabetes-dataset) includes 442 samples of data around diabetes, with 10 feature variables, some of which include:\n\n- age: age in years\n- bmi: body mass index\n- bp: average blood pressure\n- s1 tc: T-Cells (a type of white blood cells)\n\nâœ… This dataset includes the concept of 'sex' as a feature variable important to research around diabetes. Many medical datasets include this type of binary classification. Think a bit about how categorizations such as this might exclude certain parts of a population from treatments.\n\nNow, load up the X and y data.\n\n> ðŸŽ“ Remember, this is supervised learning, and we need a named 'y' target.\n\nIn a new code cell, load the diabetes dataset by calling `load_diabetes()`. The input `return_X_y=True` signals that `X` will be a data matrix, and `y` will be the regression target.\n\n1. Add some print commands to show the shape of the data matrix and its first element:\n\n    ```python\n    X, y = datasets.load_diabetes(return_X_y=True)\n    print(X.shape)\n    print(X[0])\n    ```\n\n    What you are getting back as a response, is a tuple. What you are doing is to assign the two first values of the tuple to `X` and `y` respectively. Learn more [about tuples](https://wikipedia.org/wiki/Tuple).\n\n    You can see that this data has 442 items shaped in arrays of 10 elements:\n\n    ```text\n    (442, 10)\n    [ 0.03807591  0.05068012  0.06169621  0.02187235 -0.0442235  -0.03482076\n    -0.04340085 -0.00259226  0.01990842 -0.01764613]\n    ```\n\n    âœ… Think a bit about the relationship between the data and the regression target. Linear regression predicts relationships between feature X and target variable y. Can you find the [target](https://scikit-learn.org/stable/datasets/toy_dataset.html#diabetes-dataset) for the diabetes dataset in the documentation? What is this dataset demonstrating, given that target?\n\n2. Next, select a portion of this dataset to plot by selecting the 3rd column of the dataset. You can do this by using the `:` operator to select all rows, and then selecting the 3rd column using the index (2). You can also reshape the data to be a 2D array - as required for plotting - by using `reshape(n_rows, n_columns)`. If one of the parameter is -1, the corresponding dimension is calculated automatically.\n\n   ```python\n   X = X[:, 2]\n   X = X.reshape((-1,1))\n   ```\n\n   âœ… At any time, print out the data to check its shape.\n\n3. Now that you have data ready to be plotted, you can see if a machine can help determine a logical split between the numbers in this dataset. To do this, you need to split both the data (X) and the target (y) into test and training sets. Scikit-learn has a straightforward way to do this; you can split your test data at a given point.\n\n   ```python\n   X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.33)\n   ```\n\n4. Now you are ready to train your model! Load up the linear regression model and train it with your X and y training sets using `model.fit()`:\n\n    ```python\n    model = linear_model.LinearRegression()\n    model.fit(X_train, y_train)\n    ```\n\n    âœ… `model.fit()` is a function you'll see in many ML libraries such as TensorFlow\n\n5. Then, create a prediction using test data, using the function `predict()`. This will be used to draw the line between data groups\n\n    ```python\n    y_pred = model.predict(X_test)\n    ```\n\n6. Now it's time to show the data in a plot. Matplotlib is a very useful tool for this task. Create a scatterplot of all the X and y test data, and use the prediction to draw a line in the most appropriate place, between the model's data groupings.\n\n    ```python\n    plt.scatter(X_test, y_test,  color='black')\n    plt.plot(X_test, y_pred, color='blue', linewidth=3)\n    plt.xlabel('Scaled BMIs')\n    plt.ylabel('Disease Progression')\n    plt.title('A Graph Plot Showing Diabetes Progression Against BMI')\n    plt.show()\n    ```\n\n   ![a scatterplot showing datapoints around diabetes](./images/scatterplot.png)\n\n   âœ… Think a bit about what's going on here. A straight line is running through many small dots of data, but what is it doing exactly? Can you see how you should be able to use this line to predict where a new, unseen data point should fit in relationship to the plot's y axis? Try to put into words the practical use of this model.\n\nCongratulations, you built your first linear regression model, created a prediction with it, and displayed it in a plot!\n\n---\n## ðŸš€Challenge\n\nPlot a different variable from this dataset. Hint: edit this line: `X = X[:,2]`. Given this dataset's target, what are you able to discover about the progression of diabetes as a disease?\n## [Post-lecture quiz](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/10/)\n\n## Review & Self Study\n\nIn this tutorial, you worked with simple linear regression, rather than univariate or multiple linear regression. Read a little about the differences between these methods, or take a look at [this video](https://www.coursera.org/lecture/quantifying-relationships-regression-models/linear-vs-nonlinear-categorical-variables-ai2Ef)\n\nRead more about the concept of regression and think about what kinds of questions can be answered by this technique. Take this [tutorial](https://docs.microsoft.com/learn/modules/train-evaluate-regression-models?WT.mc_id=academic-77952-leestott) to deepen your understanding.\n\n## Assignment\n\n[A different dataset](assignment.md)\n",
              "pdfUrl": "",
              "videoUrl": "",
              "orderIndex": 1
            }
          ],
          "quizzes": [],
          "subModules": []
        },
        {
          "id": 20,
          "title": "Data",
          "orderIndex": 1,
          "lessons": [
            {
              "id": 21,
              "title": "Data",
              "content": "# Exploring Visualizations\n\nThere are several different libraries that are available for data visualization. Create some visualizations using the Pumpkin data in this lesson with matplotlib and seaborn in a sample notebook. Which libraries are easier to work with?\n## Rubric\n\n| Criteria | Exemplary | Adequate | Needs Improvement |\n| -------- | --------- | -------- | ----------------- |\n|          | A notebook is submitted with two explorations/visualizations         |   A notebook is submitted with one explorations/visualizations       |  A notebook is not submitted                 |\n",
              "pdfUrl": "",
              "videoUrl": "",
              "orderIndex": 0
            },
            {
              "id": 22,
              "title": "Data",
              "content": "# Build a regression model using Scikit-learn: prepare and visualize data\n\n![Data visualization infographic](./images/data-visualization.png)\n\nInfographic by [Dasani Madipalli](https://twitter.com/dasani_decoded)\n\n## [Pre-lecture quiz](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/11/)\n\n> ### [This lesson is available in R!](./solution/R/lesson_2.html)\n\n## Introduction\n\nNow that you are set up with the tools you need to start tackling machine learning model building with Scikit-learn, you are ready to start asking questions of your data. As you work with data and apply ML solutions, it's very important to understand how to ask the right question to properly unlock the potentials of your dataset.\n\nIn this lesson, you will learn:\n\n- How to prepare your data for model-building.\n- How to use Matplotlib for data visualization.\n\n## Asking the right question of your data\n\nThe question you need answered will determine what type of ML algorithms you will leverage. And the quality of the answer you get back will be heavily dependent on the nature of your data.\n\nTake a look at the [data](https://github.com/microsoft/ML-For-Beginners/blob/main/2-Regression/data/US-pumpkins.csv) provided for this lesson. You can open this .csv file in VS Code. A quick skim immediately shows that there are blanks and a mix of strings and numeric data. There's also a strange column called 'Package' where the data is a mix between 'sacks', 'bins' and other values. The data, in fact, is a bit of a mess.\n\n[![ML for beginners - How to Analyze and Clean a Dataset](https://img.youtube.com/vi/5qGjczWTrDQ/0.jpg)](https://youtu.be/5qGjczWTrDQ \"ML for beginners - How to Analyze and Clean a Dataset\")\n\n> ðŸŽ¥ Click the image above for a short video working through preparing the data for this lesson.\n\nIn fact, it is not very common to be gifted a dataset that is completely ready to use to create a ML model out of the box. In this lesson, you will learn how to prepare a raw dataset using standard Python libraries. You will also learn various techniques to visualize the data.\n\n## Case study: 'the pumpkin market'\n\nIn this folder you will find a .csv file in the root `data` folder called [US-pumpkins.csv](https://github.com/microsoft/ML-For-Beginners/blob/main/2-Regression/data/US-pumpkins.csv) which includes 1757 lines of data about the market for pumpkins, sorted into groupings by city. This is raw data extracted from the [Specialty Crops Terminal Markets Standard Reports](https://www.marketnews.usda.gov/mnp/fv-report-config-step1?type=termPrice) distributed by the United States Department of Agriculture.\n\n### Preparing data\n\nThis data is in the public domain. It can be downloaded in many separate files, per city, from the USDA web site. To avoid too many separate files, we have concatenated all the city data into one spreadsheet, thus we have already _prepared_ the data a bit. Next, let's take a closer look at the data.\n\n### The pumpkin data - early conclusions\n\nWhat do you notice about this data? You already saw that there is a mix of strings, numbers, blanks and strange values that you need to make sense of.\n\nWhat question can you ask of this data, using a Regression technique? What about \"Predict the price of a pumpkin for sale during a given month\". Looking again at the data, there are some changes you need to make to create the data structure necessary for the task.\n## Exercise - analyze the pumpkin data\n\nLet's use [Pandas](https://pandas.pydata.org/), (the name stands for `Python Data Analysis`) a tool very useful for shaping data, to analyze and prepare this pumpkin data.\n\n### First, check for missing dates\n\nYou will first need to take steps to check for missing dates:\n\n1. Convert the dates to a month format (these are US dates, so the format is `MM/DD/YYYY`).\n2. Extract the month to a new column.\n\nOpen the _notebook.ipynb_ file in Visual Studio Code and import the spreadsheet in to a new Pandas dataframe.\n\n1. Use the `head()` function to view the first five rows.\n\n    ```python\n    import pandas as pd\n    pumpkins = pd.read_csv('../data/US-pumpkins.csv')\n    pumpkins.head()\n    ```\n\n    âœ… What function would you use to view the last five rows?\n\n1. Check if there is missing data in the current dataframe:\n\n    ```python\n    pumpkins.isnull().sum()\n    ```\n\n    There is missing data, but maybe it won't matter for the task at hand.\n\n1. To make your dataframe easier to work with, select only the columns you need, using the `loc` function which extracts from the original dataframe a group of rows (passed as first parameter) and columns (passed as second parameter). The expression `:` in the case below means \"all rows\".\n\n    ```python\n    columns_to_select = ['Package', 'Low Price', 'High Price', 'Date']\n    pumpkins = pumpkins.loc[:, columns_to_select]\n    ```\n\n### Second, determine average price of pumpkin\n\nThink about how to determine the average price of a pumpkin in a given month. What columns would you pick for this task? Hint: you'll need 3 columns.\n\nSolution: take the average of the `Low Price` and `High Price` columns to populate the new Price column, and convert the Date column to only show the month. Fortunately, according to the check above, there is no missing data for dates or prices.\n\n1. To calculate the average, add the following code:\n\n    ```python\n    price = (pumpkins['Low Price'] + pumpkins['High Price']) / 2\n\n    month = pd.DatetimeIndex(pumpkins['Date']).month\n\n    ```\n\n   âœ… Feel free to print any data you'd like to check using `print(month)`.\n\n2. Now, copy your converted data into a fresh Pandas dataframe:\n\n    ```python\n    new_pumpkins = pd.DataFrame({'Month': month, 'Package': pumpkins['Package'], 'Low Price': pumpkins['Low Price'],'High Price': pumpkins['High Price'], 'Price': price})\n    ```\n\n    Printing out your dataframe will show you a clean, tidy dataset on which you can build your new regression model.\n\n### But wait! There's something odd here\n\nIf you look at the `Package` column, pumpkins are sold in many different configurations. Some are sold in '1 1/9 bushel' measures, and some in '1/2 bushel' measures, some per pumpkin, some per pound, and some in big boxes with varying widths.\n\n> Pumpkins seem very hard to weigh consistently\n\nDigging into the original data, it's interesting that anything with `Unit of Sale` equalling 'EACH' or 'PER BIN' also have the `Package` type per inch, per bin, or 'each'. Pumpkins seem to be very hard to weigh consistently, so let's filter them by selecting only pumpkins with the string 'bushel' in their `Package` column.\n\n1. Add a filter at the top of the file, under the initial .csv import:\n\n    ```python\n    pumpkins = pumpkins[pumpkins['Package'].str.contains('bushel', case=True, regex=True)]\n    ```\n\n    If you print the data now, you can see that you are only getting the 415 or so rows of data containing pumpkins by the bushel.\n\n### But wait! There's one more thing to do\n\nDid you notice that the bushel amount varies per row? You need to normalize the pricing so that you show the pricing per bushel, so do some math to standardize it.\n\n1. Add these lines after the block creating the new_pumpkins dataframe:\n\n    ```python\n    new_pumpkins.loc[new_pumpkins['Package'].str.contains('1 1/9'), 'Price'] = price/(1 + 1/9)\n\n    new_pumpkins.loc[new_pumpkins['Package'].str.contains('1/2'), 'Price'] = price/(1/2)\n    ```\n\nâœ… According to [The Spruce Eats](https://www.thespruceeats.com/how-much-is-a-bushel-1389308), a bushel's weight depends on the type of produce, as it's a volume measurement. \"A bushel of tomatoes, for example, is supposed to weigh 56 pounds... Leaves and greens take up more space with less weight, so a bushel of spinach is only 20 pounds.\" It's all pretty complicated! Let's not bother with making a bushel-to-pound conversion, and instead price by the bushel. All this study of bushels of pumpkins, however, goes to show how very important it is to understand the nature of your data!\n\nNow, you can analyze the pricing per unit based on their bushel measurement. If you print out the data one more time, you can see how it's standardized.\n\nâœ… Did you notice that pumpkins sold by the half-bushel are very expensive? Can you figure out why? Hint: little pumpkins are way pricier than big ones, probably because there are so many more of them per bushel, given the unused space taken by one big hollow pie pumpkin.\n\n## Visualization Strategies\n\nPart of the data scientist's role is to demonstrate the quality and nature of the data they are working with. To do this, they often create interesting visualizations, or plots, graphs, and charts, showing different aspects of data. In this way, they are able to visually show relationships and gaps that are otherwise hard to uncover.\n\n[![ML for beginners - How to Visualize Data with Matplotlib](https://img.youtube.com/vi/SbUkxH6IJo0/0.jpg)](https://youtu.be/SbUkxH6IJo0 \"ML for beginners - How to Visualize Data with Matplotlib\")\n\n> ðŸŽ¥ Click the image above for a short video working through visualizing the data for this lesson.\n\nVisualizations can also help determine the machine learning technique most appropriate for the data. A scatterplot that seems to follow a line, for example, indicates that the data is a good candidate for a linear regression exercise.\n\nOne data visualization library that works well in Jupyter notebooks is [Matplotlib](https://matplotlib.org/) (which you also saw in the previous lesson).\n\n> Get more experience with data visualization in [these tutorials](https://docs.microsoft.com/learn/modules/explore-analyze-data-with-python?WT.mc_id=academic-77952-leestott).\n\n## Exercise - experiment with Matplotlib\n\nTry to create some basic plots to display the new dataframe you just created. What would a basic line plot show?\n\n1. Import Matplotlib at the top of the file, under the Pandas import:\n\n    ```python\n    import matplotlib.pyplot as plt\n    ```\n\n1. Rerun the entire notebook to refresh.\n1. At the bottom of the notebook, add a cell to plot the data as a box:\n\n    ```python\n    price = new_pumpkins.Price\n    month = new_pumpkins.Month\n    plt.scatter(price, month)\n    plt.show()\n    ```\n\n    ![A scatterplot showing price to month relationship](./images/scatterplot.png)\n\n    Is this a useful plot? Does anything about it surprise you?\n\n    It's not particularly useful as all it does is display in your data as a spread of points in a given month.\n\n### Make it useful\n\nTo get charts to display useful data, you usually need to group the data somehow. Let's try creating a plot where the y axis shows the months and the data demonstrates the distribution of data.\n\n1. Add a cell to create a grouped bar chart:\n\n    ```python\n    new_pumpkins.groupby(['Month'])['Price'].mean().plot(kind='bar')\n    plt.ylabel(\"Pumpkin Price\")\n    ```\n\n    ![A bar chart showing price to month relationship](./images/barchart.png)\n\n    This is a more useful data visualization! It seems to indicate that the highest price for pumpkins occurs in September and October. Does that meet your expectation? Why or why not?\n\n---\n\n## ðŸš€Challenge\n\nExplore the different types of visualization that Matplotlib offers. Which types are most appropriate for regression problems?\n\n## [Post-lecture quiz](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/12/)\n\n## Review & Self Study\n\nTake a look at the many ways to visualize data. Make a list of the various libraries available and note which are best for given types of tasks, for example 2D visualizations vs. 3D visualizations. What do you discover?\n\n## Assignment\n\n[Exploring visualization](assignment.md)\n",
              "pdfUrl": "",
              "videoUrl": "",
              "orderIndex": 1
            }
          ],
          "quizzes": [],
          "subModules": []
        },
        {
          "id": 23,
          "title": "Linear",
          "orderIndex": 2,
          "lessons": [
            {
              "id": 24,
              "title": "Linear",
              "content": "# Create a Regression Model\n\n## Instructions\n\nIn this lesson you were shown how to build a model using both Linear and Polynomial Regression. Using this knowledge, find a dataset or use one of Scikit-learn's built-in sets to build a fresh model. Explain in your notebook why you chose the technique you did, and demonstrate your model's accuracy. If it is not accurate, explain why.\n\n## Rubric\n\n| Criteria | Exemplary                                                    | Adequate                   | Needs Improvement               |\n| -------- | ------------------------------------------------------------ | -------------------------- | ------------------------------- |\n|          | presents a complete notebook with a well-documented solution | the solution is incomplete | the solution is flawed or buggy |\n",
              "pdfUrl": "",
              "videoUrl": "",
              "orderIndex": 0
            },
            {
              "id": 25,
              "title": "Linear",
              "content": "# Build a regression model using Scikit-learn: regression four ways\n\n![Linear vs polynomial regression infographic](./images/linear-polynomial.png)\n> Infographic by [Dasani Madipalli](https://twitter.com/dasani_decoded)\n## [Pre-lecture quiz](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/13/)\n\n> ### [This lesson is available in R!](./solution/R/lesson_3.html)\n### Introduction \n\nSo far you have explored what regression is with sample data gathered from the pumpkin pricing dataset that we will use throughout this lesson. You have also visualized it using Matplotlib.\n\nNow you are ready to dive deeper into regression for ML. While visualization allows you to make sense of data, the real power of Machine Learning comes from _training models_. Models are trained on historic data to automatically capture data dependencies, and they allow you to predict outcomes for new data, which the model has not seem before.\n\nIn this lesson, you will learn more about two types of regression: _basic linear regression_ and _polynomial regression_, along with some of the math underlying these techniques. Those models will allow us to predict pumpkin prices depending on different input data. \n\n[![ML for beginners - Understanding Linear Regression](https://img.youtube.com/vi/CRxFT8oTDMg/0.jpg)](https://youtu.be/CRxFT8oTDMg \"ML for beginners - Understanding Linear Regression\")\n\n> ðŸŽ¥ Click the image above for a short video overview of linear regression.\n\n> Throughout this curriculum, we assume minimal knowledge of math, and seek to make it accessible for students coming from other fields, so watch for notes, ðŸ§® callouts, diagrams, and other learning tools to aid in comprehension.\n\n### Prerequisite\n\nYou should be familiar by now with the structure of the pumpkin data that we are examining. You can find it preloaded and pre-cleaned in this lesson's _notebook.ipynb_ file. In the file, the pumpkin price is displayed per bushel in a new data frame.  Make sure you can run these notebooks in kernels in Visual Studio Code.\n\n### Preparation\n\nAs a reminder, you are loading this data so as to ask questions of it. \n\n- When is the best time to buy pumpkins? \n- What price can I expect of a case of miniature pumpkins?\n- Should I buy them in half-bushel baskets or by the 1 1/9 bushel box?\nLet's keep digging into this data.\n\nIn the previous lesson, you created a Pandas data frame and populated it with part of the original dataset, standardizing the pricing by the bushel. By doing that, however, you were only able to gather about 400 datapoints and only for the fall months. \n\nTake a look at the data that we preloaded in this lesson's accompanying notebook. The data is preloaded and an initial scatterplot is charted to show month data. Maybe we can get a little more detail about the nature of the data by cleaning it more.\n\n## A linear regression line\n\nAs you learned in Lesson 1, the goal of a linear regression exercise is to be able to plot a line to:\n\n- **Show variable relationships**. Show the relationship between variables\n- **Make predictions**. Make accurate predictions on where a new datapoint would fall in relationship to that line. \n \nIt is typical of **Least-Squares Regression** to draw this type of line. The term 'least-squares' means that all the datapoints surrounding the regression line are squared and then added up. Ideally, that final sum is as small as possible, because we want a low number of errors, or `least-squares`. \n\nWe do so since we want to model a line that has the least cumulative distance from all of our data points. We also square the terms before adding them since we are concerned with its magnitude rather than its direction.\n\n> **ðŸ§® Show me the math** \n> \n> This line, called the _line of best fit_ can be expressed by [an equation](https://en.wikipedia.org/wiki/Simple_linear_regression): \n> \n> ```\n> Y = a + bX\n> ```\n>\n> `X` is the 'explanatory variable'. `Y` is the 'dependent variable'. The slope of the line is `b` and `a` is the y-intercept, which refers to the value of `Y` when `X = 0`. \n>\n>![calculate the slope](images/slope.png)\n>\n> First, calculate the slope `b`. Infographic by [Jen Looper](https://twitter.com/jenlooper)\n>\n> In other words, and referring to our pumpkin data's original question: \"predict the price of a pumpkin per bushel by month\", `X` would refer to the price and `Y` would refer to the month of sale. \n>\n>![complete the equation](images/calculation.png)\n>\n> Calculate the value of Y. If you're paying around $4, it must be April! Infographic by [Jen Looper](https://twitter.com/jenlooper)\n>\n> The math that calculates the line must demonstrate the slope of the line, which is also dependent on the intercept, or where `Y` is situated when `X = 0`.\n>\n> You can observe the method of calculation for these values on the [Math is Fun](https://www.mathsisfun.com/data/least-squares-regression.html) web site. Also visit [this Least-squares calculator](https://www.mathsisfun.com/data/least-squares-calculator.html) to watch how the numbers' values impact the line.\n\n## Correlation\n\nOne more term to understand is the **Correlation Coefficient** between given X and Y variables. Using a scatterplot, you can quickly visualize this coefficient. A plot with datapoints scattered in a neat line have high correlation, but a plot with datapoints scattered everywhere between X and Y have a low correlation.\n\nA good linear regression model will be one that has a high (nearer to 1 than 0) Correlation Coefficient using the Least-Squares Regression method with a line of regression.\n\nâœ… Run the notebook accompanying this lesson and look at the Month to Price scatterplot. Does the data associating Month to Price for pumpkin sales seem to have high or low correlation, according to your visual interpretation of the scatterplot? Does that change if you use more fine-grained measure instead of `Month`, eg. *day of the year* (i.e. number of days since the beginning of the year)?\n\nIn the code below, we will assume that we have cleaned up the data, and obtained a data frame called `new_pumpkins`, similar to the following:\n\nID | Month | DayOfYear | Variety | City | Package | Low Price | High Price | Price\n---|-------|-----------|---------|------|---------|-----------|------------|-------\n70 | 9 | 267 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 15.0 | 15.0 | 13.636364\n71 | 9 | 267 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 18.0 | 18.0 | 16.363636\n72 | 10 | 274 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 18.0 | 18.0 | 16.363636\n73 | 10 | 274 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 17.0 | 17.0 | 15.454545\n74 | 10 | 281 | PIE TYPE | BALTIMORE | 1 1/9 bushel cartons | 15.0 | 15.0 | 13.636364\n\n> The code to clean the data is available in [`notebook.ipynb`](notebook.ipynb). We have performed the same cleaning steps as in the previous lesson, and have calculated `DayOfYear` column using the following expression: \n\n```python\nday_of_year = pd.to_datetime(pumpkins['Date']).apply(lambda dt: (dt-datetime(dt.year,1,1)).days)\n```\n\nNow that you have an understanding of the math behind linear regression, let's create a Regression model to see if we can predict which package of pumpkins will have the best pumpkin prices. Someone buying pumpkins for a holiday pumpkin patch might want this information to be able to optimize their purchases of pumpkin packages for the patch.\n\n## Looking for Correlation\n\n[![ML for beginners - Looking for Correlation: The Key to Linear Regression](https://img.youtube.com/vi/uoRq-lW2eQo/0.jpg)](https://youtu.be/uoRq-lW2eQo \"ML for beginners - Looking for Correlation: The Key to Linear Regression\")\n\n> ðŸŽ¥ Click the image above for a short video overview of correlation.\n\nFrom the previous lesson you have probably seen that the average price for different months looks like this:\n\n<img alt=\"Average price by month\" src=\"../2-Data/images/barchart.png\" width=\"50%\"/>\n\nThis suggests that there should be some correlation, and we can try training linear regression model to predict the relationship between `Month` and `Price`, or between `DayOfYear` and `Price`. Here is the scatter plot that shows the latter relationship:\n\n<img alt=\"Scatter plot of Price vs. Day of Year\" src=\"images/scatter-dayofyear.png\" width=\"50%\" /> \n\nLet's see if there is a correlation using the `corr` function:\n\n```python\nprint(new_pumpkins['Month'].corr(new_pumpkins['Price']))\nprint(new_pumpkins['DayOfYear'].corr(new_pumpkins['Price']))\n```\n\nIt looks like the correlation is pretty small, -0.15 by `Month` and -0.17 by the `DayOfMonth`, but there could be another important relationship. It looks like there are different clusters of prices corresponding to different pumpkin varieties. To confirm this hypothesis, let's plot each pumpkin category using a different color. By passing an `ax` parameter to the `scatter` plotting function we can plot all points on the same graph:\n\n```python\nax=None\ncolors = ['red','blue','green','yellow']\nfor i,var in enumerate(new_pumpkins['Variety'].unique()):\n    df = new_pumpkins[new_pumpkins['Variety']==var]\n    ax = df.plot.scatter('DayOfYear','Price',ax=ax,c=colors[i],label=var)\n```\n\n<img alt=\"Scatter plot of Price vs. Day of Year\" src=\"images/scatter-dayofyear-color.png\" width=\"50%\" /> \n\nOur investigation suggests that variety has more effect on the overall price than the actual selling date. We can see this with a bar graph:\n\n```python\nnew_pumpkins.groupby('Variety')['Price'].mean().plot(kind='bar')\n```\n\n<img alt=\"Bar graph of price vs variety\" src=\"images/price-by-variety.png\" width=\"50%\" /> \n\nLet us focus for the moment only on one pumpkin variety, the 'pie type', and see what effect the date has on the price:\n\n```python\npie_pumpkins = new_pumpkins[new_pumpkins['Variety']=='PIE TYPE']\npie_pumpkins.plot.scatter('DayOfYear','Price') \n```\n<img alt=\"Scatter plot of Price vs. Day of Year\" src=\"images/pie-pumpkins-scatter.png\" width=\"50%\" /> \n\nIf we now calculate the correlation between `Price` and `DayOfYear` using `corr` function, we will get something like `-0.27` - which means that training a predictive model makes sense.\n\n> Before training a linear regression model, it is important to make sure that our data is clean. Linear regression does not work well with missing values, thus it makes sense to get rid of all empty cells:\n\n```python\npie_pumpkins.dropna(inplace=True)\npie_pumpkins.info()\n```\n\nAnother approach would be to fill those empty values with mean values from the corresponding column.\n\n## Simple Linear Regression\n\n[![ML for beginners - Linear and Polynomial Regression using Scikit-learn](https://img.youtube.com/vi/e4c_UP2fSjg/0.jpg)](https://youtu.be/e4c_UP2fSjg \"ML for beginners - Linear and Polynomial Regression using Scikit-learn\")\n\n> ðŸŽ¥ Click the image above for a short video overview of linear and polynomial regression.\n\nTo train our Linear Regression model, we will use the **Scikit-learn** library.\n\n```python\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\n```\n\nWe start by separating input values (features) and the expected output (label) into separate numpy arrays:\n\n```python\nX = pie_pumpkins['DayOfYear'].to_numpy().reshape(-1,1)\ny = pie_pumpkins['Price']\n```\n\n> Note that we had to perform `reshape` on the input data in order for the Linear Regression package to understand it correctly. Linear Regression expects a 2D-array as an input, where each row of the array corresponds to a vector of input features. In our case, since we have only one input - we need an array with shape N&times;1, where N is the dataset size.\n\nThen, we need to split the data into train and test datasets, so that we can validate our model after training:\n\n```python\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n```\n\nFinally, training the actual Linear Regression model takes only two lines of code. We define the `LinearRegression` object, and fit it to our data using the `fit` method:\n\n```python\nlin_reg = LinearRegression()\nlin_reg.fit(X_train,y_train)\n```\n\nThe `LinearRegression` object after `fit`-ting contains all the coefficients of the regression, which can be accessed using `.coef_` property. In our case, there is just one coefficient, which should be around `-0.017`. It means that prices seem to drop a bit with time, but not too much, around 2 cents per day. We can also access the intersection point of the regression with Y-axis using `lin_reg.intercept_` - it will be around `21` in our case, indicating the price at the beginning of the year.\n\nTo see how accurate our model is, we can predict prices on a test dataset, and then measure how close our predictions are to the expected values. This can be done using mean square error (MSE) metrics, which is the mean of all squared differences between expected and predicted value.\n\n```python\npred = lin_reg.predict(X_test)\n\nmse = np.sqrt(mean_squared_error(y_test,pred))\nprint(f'Mean error: {mse:3.3} ({mse/np.mean(pred)*100:3.3}%)')\n```\n\nOur error seems to be around 2 points, which is ~17%. Not too good. Another indicator of model quality is the **coefficient of determination**, which can be obtained like this:\n\n```python\nscore = lin_reg.score(X_train,y_train)\nprint('Model determination: ', score)\n```\nIf the value is 0, it means that the model does not take input data into account, and acts as the *worst linear predictor*, which is simply a mean value of the result. The value of 1 means that we can perfectly predict all expected outputs. In our case, the coefficient is around 0.06, which is quite low.\n\nWe can also plot the test data together with the regression line to better see how regression works in our case:\n\n```python\nplt.scatter(X_test,y_test)\nplt.plot(X_test,pred)\n```\n\n<img alt=\"Linear regression\" src=\"images/linear-results.png\" width=\"50%\" />\n\n## Polynomial Regression\n\nAnother type of Linear Regression is Polynomial Regression. While sometimes there's a linear relationship between variables - the bigger the pumpkin in volume, the higher the price - sometimes these relationships can't be plotted as a plane or straight line. \n\nâœ… Here are [some more examples](https://online.stat.psu.edu/stat501/lesson/9/9.8) of data that could use Polynomial Regression\n\nTake another look at the relationship between Date and Price. Does this scatterplot seem like it should necessarily be analyzed by a straight line? Can't prices fluctuate? In this case, you can try polynomial regression.\n\nâœ… Polynomials are mathematical expressions that might consist of one or more variables and coefficients\n\nPolynomial regression creates a curved line to better fit nonlinear data. In our case, if we include a squared `DayOfYear` variable into input data, we should be able to fit our data with a parabolic curve, which will have a minimum at a certain point within the year.\n\nScikit-learn includes a helpful [pipeline API](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html?highlight=pipeline#sklearn.pipeline.make_pipeline) to combine different steps of data processing together. A **pipeline** is a chain of **estimators**. In our case, we will create a pipeline that first adds polynomial features to our model, and then trains the regression:\n\n```python\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import make_pipeline\n\npipeline = make_pipeline(PolynomialFeatures(2), LinearRegression())\n\npipeline.fit(X_train,y_train)\n```\n\nUsing `PolynomialFeatures(2)` means that we will include all second-degree polynomials from the input data. In our case it will just mean `DayOfYear`<sup>2</sup>, but given two input variables X and Y, this will add X<sup>2</sup>, XY and Y<sup>2</sup>. We may also use higher degree polynomials if we want.\n\nPipelines can be used in the same manner as the original `LinearRegression` object, i.e. we can `fit` the pipeline, and then use `predict` to get the prediction results. Here is the graph showing test data, and the approximation curve:\n\n<img alt=\"Polynomial regression\" src=\"images/poly-results.png\" width=\"50%\" />\n\nUsing Polynomial Regression, we can get slightly lower MSE and higher determination, but not significantly. We need to take into account other features!\n\n> You can see that the minimal pumpkin prices are observed somewhere around Halloween. How can you explain this? \n\nðŸŽƒ Congratulations, you just created a model that can help predict the price of pie pumpkins. You can probably repeat the same procedure for all pumpkin types, but that would be tedious. Let's learn now how to take pumpkin variety into account in our model!\n\n## Categorical Features\n\nIn the ideal world, we want to be able to predict prices for different pumpkin varieties using the same model. However, the `Variety` column is somewhat different from columns like `Month`, because it contains non-numeric values. Such columns are called **categorical**.\n\n[![ML for beginners - Categorical Feature Predictions with Linear Regression](https://img.youtube.com/vi/DYGliioIAE0/0.jpg)](https://youtu.be/DYGliioIAE0 \"ML for beginners - Categorical Feature Predictions with Linear Regression\")\n\n> ðŸŽ¥ Click the image above for a short video overview of using categorical features.\n\nHere you can see how average price depends on variety:\n\n<img alt=\"Average price by variety\" src=\"images/price-by-variety.png\" width=\"50%\" />\n\nTo take variety into account, we first need to convert it to numeric form, or **encode** it. There are several way we can do it:\n\n* Simple **numeric encoding** will build a table of different varieties, and then replace the variety name by an index in that table. This is not the best idea for linear regression, because linear regression takes the actual numeric value of the index, and adds it to the result, multiplying by some coefficient. In our case, the relationship between the index number and the price is clearly non-linear, even if we make sure that indices are ordered in some specific way.\n* **One-hot encoding** will replace the `Variety` column by 4 different columns, one for each variety. Each column will contain `1` if the corresponding row is of a given variety, and `0` otherwise. This means that there will be four coefficients in linear regression, one for each pumpkin variety, responsible for \"starting price\" (or rather \"additional price\") for that particular variety.\n\nThe code below shows how we can one-hot encode a variety:\n\n```python\npd.get_dummies(new_pumpkins['Variety'])\n```\n\n ID | FAIRYTALE | MINIATURE | MIXED HEIRLOOM VARIETIES | PIE TYPE\n----|-----------|-----------|--------------------------|----------\n70 | 0 | 0 | 0 | 1\n71 | 0 | 0 | 0 | 1\n... | ... | ... | ... | ...\n1738 | 0 | 1 | 0 | 0\n1739 | 0 | 1 | 0 | 0\n1740 | 0 | 1 | 0 | 0\n1741 | 0 | 1 | 0 | 0\n1742 | 0 | 1 | 0 | 0\n\nTo train linear regression using one-hot encoded variety as input, we just need to initialize `X` and `y` data correctly:\n\n```python\nX = pd.get_dummies(new_pumpkins['Variety'])\ny = new_pumpkins['Price']\n```\n\nThe rest of the code is the same as what we used above to train Linear Regression. If you try it, you will see that the mean squared error is about the same, but we get much higher coefficient of determination (~77%). To get even more accurate predictions, we can take more categorical features into account, as well as numeric features, such as `Month` or `DayOfYear`. To get one large array of features, we can use `join`:\n\n```python\nX = pd.get_dummies(new_pumpkins['Variety']) \\\n        .join(new_pumpkins['Month']) \\\n        .join(pd.get_dummies(new_pumpkins['City'])) \\\n        .join(pd.get_dummies(new_pumpkins['Package']))\ny = new_pumpkins['Price']\n```\n\nHere we also take into account `City` and `Package` type, which gives us MSE 2.84 (10%), and determination 0.94!\n\n## Putting it all together\n\nTo make the best model, we can use combined (one-hot encoded categorical + numeric) data from the above example together with Polynomial Regression. Here is the complete code for your convenience:\n\n```python\n# set up training data\nX = pd.get_dummies(new_pumpkins['Variety']) \\\n        .join(new_pumpkins['Month']) \\\n        .join(pd.get_dummies(new_pumpkins['City'])) \\\n        .join(pd.get_dummies(new_pumpkins['Package']))\ny = new_pumpkins['Price']\n\n# make train-test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n\n# setup and train the pipeline\npipeline = make_pipeline(PolynomialFeatures(2), LinearRegression())\npipeline.fit(X_train,y_train)\n\n# predict results for test data\npred = pipeline.predict(X_test)\n\n# calculate MSE and determination\nmse = np.sqrt(mean_squared_error(y_test,pred))\nprint(f'Mean error: {mse:3.3} ({mse/np.mean(pred)*100:3.3}%)')\n\nscore = pipeline.score(X_train,y_train)\nprint('Model determination: ', score)\n```\n\nThis should give us the best determination coefficient of almost 97%, and MSE=2.23 (~8% prediction error).\n\n| Model | MSE | Determination |\n|-------|-----|---------------|\n| `DayOfYear` Linear | 2.77 (17.2%) | 0.07 |\n| `DayOfYear` Polynomial | 2.73 (17.0%) | 0.08 |\n| `Variety` Linear | 5.24 (19.7%) | 0.77 |\n| All features Linear | 2.84 (10.5%) | 0.94 |\n| All features Polynomial | 2.23 (8.25%) | 0.97 |\n\nðŸ† Well done! You created four Regression models in one lesson, and improved the model quality to 97%. In the final section on Regression, you will learn about Logistic Regression to determine categories. \n\n---\n## ðŸš€Challenge\n\nTest several different variables in this notebook to see how correlation corresponds to model accuracy.\n\n## [Post-lecture quiz](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/14/)\n\n## Review & Self Study\n\nIn this lesson we learned about Linear Regression. There are other important types of Regression. Read about Stepwise, Ridge, Lasso and Elasticnet techniques. A good course to study to learn more is the [Stanford Statistical Learning course](https://online.stanford.edu/courses/sohs-ystatslearning-statistical-learning)\n\n## Assignment \n\n[Build a Model](assignment.md)\n",
              "pdfUrl": "",
              "videoUrl": "",
              "orderIndex": 1
            }
          ],
          "quizzes": [],
          "subModules": []
        },
        {
          "id": 26,
          "title": "Logistic",
          "orderIndex": 3,
          "lessons": [
            {
              "id": 27,
              "title": "Logistic",
              "content": "# Retrying some Regression\n\n## Instructions\n\nIn the lesson, you used a subset of the pumpkin data. Now, go back to the original data and try to use all of it, cleaned and standardized, to build a Logistic Regression model.\n## Rubric\n\n| Criteria | Exemplary                                                               | Adequate                                                     | Needs Improvement                                           |\n| -------- | ----------------------------------------------------------------------- | ------------------------------------------------------------ | ----------------------------------------------------------- |\n|          | A notebook is presented with a well-explained and well-performing model | A notebook is presented with a model that performs minimally | A notebook is presented with a sub-performing model or none |\n",
              "pdfUrl": "",
              "videoUrl": "",
              "orderIndex": 0
            },
            {
              "id": 28,
              "title": "Logistic",
              "content": "# Logistic regression to predict categories\n\n![Logistic vs. linear regression infographic](./images/linear-vs-logistic.png)\n\n## [Pre-lecture quiz](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/15/)\n\n> ### [This lesson is available in R!](./solution/R/lesson_4.html)\n\n## Introduction\n\nIn this final lesson on Regression, one of the basic _classic_ ML techniques, we will take a look at Logistic Regression. You would use this technique to discover patterns to predict binary categories. Is this candy chocolate or not? Is this disease contagious or not? Will this customer choose this product or not? \n\nIn this lesson, you will learn:\n\n- A new library for data visualization\n- Techniques for logistic regression\n\nâœ… Deepen your understanding of working with this type of regression in this [Learn module](https://docs.microsoft.com/learn/modules/train-evaluate-classification-models?WT.mc_id=academic-77952-leestott)\n\n## Prerequisite\n\nHaving worked with the pumpkin data, we are now familiar enough with it to realize that there's one binary category that we can work with: `Color`.\n\nLet's build a logistic regression model to predict that, given some variables, _what color a given pumpkin is likely to be_ (orange ðŸŽƒ or white ðŸ‘»).\n\n> Why are we talking about binary classification in a lesson grouping about regression? Only for linguistic convenience, as logistic regression is [really a classification method](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression), albeit a linear-based one. Learn about other ways to classify data in the next lesson group.\n\n## Define the question\n\nFor our purposes, we will express this as a binary: 'White' or 'Not White'. There is also a 'striped' category in our dataset but there are few instances of it, so we will not use it. It disappears once we remove null values from the dataset, anyway.\n\n> ðŸŽƒ Fun fact, we sometimes call white pumpkins 'ghost' pumpkins. They aren't very easy to carve, so they aren't as popular as the orange ones but they are cool looking! So we could also reformulate our question as: 'Ghost' or 'Not Ghost'. ðŸ‘»\n\n## About logistic regression\n\nLogistic regression differs from linear regression, which you learned about previously, in a few important ways.\n\n[![ML for beginners - Understanding Logistic Regression for Machine Learning Classification](https://img.youtube.com/vi/KpeCT6nEpBY/0.jpg)](https://youtu.be/KpeCT6nEpBY \"ML for beginners - Understanding Logistic Regression for Machine Learning Classification\")\n\n> ðŸŽ¥ Click the image above for a short video overview of logistic regression.\n\n### Binary classification\n\nLogistic regression does not offer the same features as linear regression. The former offers a prediction about a binary category (\"white or not white\") whereas the latter is capable of predicting continual values, for example given the origin of a pumpkin and the time of harvest, _how much its price will rise_.\n\n![Pumpkin classification Model](./images/pumpkin-classifier.png)\n> Infographic by [Dasani Madipalli](https://twitter.com/dasani_decoded)\n\n### Other classifications\n\nThere are other types of logistic regression, including multinomial and ordinal:\n\n- **Multinomial**, which involves having more than one category - \"Orange, White, and Striped\".\n- **Ordinal**, which involves ordered categories, useful if we wanted to order our outcomes logically, like our pumpkins that are ordered by a finite number of sizes (mini,sm,med,lg,xl,xxl).\n\n![Multinomial vs ordinal regression](./images/multinomial-vs-ordinal.png)\n\n### Variables DO NOT have to correlate\n\nRemember how linear regression worked better with more correlated variables? Logistic regression is the opposite - the variables don't have to align. That works for this data which has somewhat weak correlations.\n\n### You need a lot of clean data\n\nLogistic regression will give more accurate results if you use more data; our small dataset is not optimal for this task, so keep that in mind.\n\n[![ML for beginners - Data Analysis and Preparation for Logistic Regression](https://img.youtube.com/vi/B2X4H9vcXTs/0.jpg)](https://youtu.be/B2X4H9vcXTs \"ML for beginners - Data Analysis and Preparation for Logistic Regression\")\n\n> ðŸŽ¥ Click the image above for a short video overview of preparing data for linear regression\n\nâœ… Think about the types of data that would lend themselves well to logistic regression\n\n## Exercise - tidy the data\n\nFirst, clean the data a bit, dropping null values and selecting only some of the columns:\n\n1. Add the following code:\n\n    ```python\n  \n    columns_to_select = ['City Name','Package','Variety', 'Origin','Item Size', 'Color']\n    pumpkins = full_pumpkins.loc[:, columns_to_select]\n\n    pumpkins.dropna(inplace=True)\n    ```\n\n    You can always take a peek at your new dataframe:\n\n    ```python\n    pumpkins.info\n    ```\n\n### Visualization - categorical plot\n\nBy now you have loaded up the [starter notebook](./notebook.ipynb) with pumpkin data once again and cleaned it so as to preserve a dataset containing a few variables, including `Color`. Let's visualize the dataframe in the notebook using a different library: [Seaborn](https://seaborn.pydata.org/index.html), which is built on Matplotlib which we used earlier. \n\nSeaborn offers some neat ways to visualize your data. For example, you can compare distributions of the data for each `Variety` and `Color` in a categorical plot.\n\n1. Create such a plot by using the `catplot` function, using our pumpkin data `pumpkins`, and specifying a color mapping for each pumpkin category (orange or white):\n\n    ```python\n    import seaborn as sns\n    \n    palette = {\n    'ORANGE': 'orange',\n    'WHITE': 'wheat',\n    }\n\n    sns.catplot(\n    data=pumpkins, y=\"Variety\", hue=\"Color\", kind=\"count\",\n    palette=palette, \n    )\n    ```\n\n    ![A grid of visualized data](images/pumpkins_catplot_1.png)\n\n    By observing the data, you can see how the Color data relates to Variety.\n\n    âœ… Given this categorical plot, what are some interesting explorations you can envision?\n\n### Data pre-processing: feature and label encoding\nOur pumpkins dataset contains string values for all its columns. Working with categorical data is intuitive for humans but not for machines. Machine learning algorithms work well with numbers. That's why encoding is a very important step in the data pre-processing phase, since it enables us to turn categorical data into numerical data, without losing any information. Good encoding leads to building a good model.\n\nFor feature encoding there are two main types of encoders:\n\n1. Ordinal encoder: it suits well for ordinal variables, which are categorical variables where their data follows a logical ordering, like the `Item Size` column in our dataset. It creates a mapping such that each category is represented by a number, which is the order of the category in the column.\n\n    ```python\n    from sklearn.preprocessing import OrdinalEncoder\n\n    item_size_categories = [['sml', 'med', 'med-lge', 'lge', 'xlge', 'jbo', 'exjbo']]\n    ordinal_features = ['Item Size']\n    ordinal_encoder = OrdinalEncoder(categories=item_size_categories)\n    ```\n\n2. Categorical encoder: it suits well for nominal variables, which are categorical variables where their data does not follow a logical ordering, like all the features different from `Item Size` in our dataset. It is a one-hot encoding, which means that each category is represented by a binary column: the encoded variable is equal to 1 if the pumpkin belongs to that Variety and 0 otherwise.\n\n    ```python\n    from sklearn.preprocessing import OneHotEncoder\n\n    categorical_features = ['City Name', 'Package', 'Variety', 'Origin']\n    categorical_encoder = OneHotEncoder(sparse_output=False)\n    ```\nThen, `ColumnTransformer` is used to combine multiple encoders into a single step and apply them to the appropriate columns.\n\n```python\n    from sklearn.compose import ColumnTransformer\n    \n    ct = ColumnTransformer(transformers=[\n        ('ord', ordinal_encoder, ordinal_features),\n        ('cat', categorical_encoder, categorical_features)\n        ])\n    \n    ct.set_output(transform='pandas')\n    encoded_features = ct.fit_transform(pumpkins)\n```\nOn the other hand, to encode the label, we use the scikit-learn `LabelEncoder` class, which is a utility class to help normalize labels such that they contain only values between 0 and n_classes-1 (here, 0 and 1).\n\n```python\n    from sklearn.preprocessing import LabelEncoder\n\n    label_encoder = LabelEncoder()\n    encoded_label = label_encoder.fit_transform(pumpkins['Color'])\n```\nOnce we have encoded the features and the label, we can merge them into a new dataframe `encoded_pumpkins`.\n\n```python\n    encoded_pumpkins = encoded_features.assign(Color=encoded_label)\n```\nâœ… What are the advantages of using an ordinal encoder for the `Item Size` column?\n\n### Analyse relationships between variables\n\nNow that we have pre-processed our data, we can analyse the relationships between the features and the label to grasp an idea of how well the model will be able to predict the label given the features.\nThe best way to perform this kind of analysis is plotting the data. We'll be using again the Seaborn `catplot` function, to visualize the relationships between `Item Size`,  `Variety` and `Color` in a categorical plot. To better plot the data we'll be using the encoded `Item Size` column and the unencoded `Variety` column.\n\n```python\n    palette = {\n    'ORANGE': 'orange',\n    'WHITE': 'wheat',\n    }\n    pumpkins['Item Size'] = encoded_pumpkins['ord__Item Size']\n\n    g = sns.catplot(\n        data=pumpkins,\n        x=\"Item Size\", y=\"Color\", row='Variety',\n        kind=\"box\", orient=\"h\",\n        sharex=False, margin_titles=True,\n        height=1.8, aspect=4, palette=palette,\n    )\n    g.set(xlabel=\"Item Size\", ylabel=\"\").set(xlim=(0,6))\n    g.set_titles(row_template=\"{row_name}\")\n```\n![A catplot of visualized data](images/pumpkins_catplot_2.png)\n\n### Use a swarm plot\n\nSince Color is a binary category (White or Not), it needs 'a [specialized approach](https://seaborn.pydata.org/tutorial/categorical.html?highlight=bar) to visualization'. There are other ways to visualize the relationship of this category with other variables. \n\nYou can visualize variables side-by-side with Seaborn plots.\n\n1. Try a 'swarm' plot to show the distribution of values:\n\n    ```python\n    palette = {\n    0: 'orange',\n    1: 'wheat'\n    }\n    sns.swarmplot(x=\"Color\", y=\"ord__Item Size\", data=encoded_pumpkins, palette=palette)\n    ```\n\n    ![A swarm of visualized data](images/swarm_2.png)\n\n**Watch Out**: the code above might generate a warning, since seaborn fails to represent such amount of datapoints into a swam plot. A possible solution is decreasing the size of the marker, by using the 'size' parameter. However, be aware that this affects the readability of the plot.\n\n\n> **ðŸ§® Show Me The Math**\n>\n> Logistic regression relies on the concept of 'maximum likelihood' using [sigmoid functions](https://wikipedia.org/wiki/Sigmoid_function). A 'Sigmoid Function' on a plot looks like an 'S' shape. It takes a value and maps it to somewhere between 0 and 1. Its curve is also called a 'logistic curve'. Its formula looks like this:\n>\n> ![logistic function](images/sigmoid.png)\n>\n> where the sigmoid's midpoint finds itself at x's 0 point, L is the curve's maximum value, and k is the curve's steepness. If the outcome of the function is more than 0.5, the label in question will be given the class '1' of the binary choice. If not, it will be classified as '0'.\n\n## Build your model\n\nBuilding a model to find these binary classification is surprisingly straightforward in Scikit-learn.\n\n[![ML for beginners - Logistic Regression for classification of data](https://img.youtube.com/vi/MmZS2otPrQ8/0.jpg)](https://youtu.be/MmZS2otPrQ8 \"ML for beginners - Logistic Regression for classification of data\")\n\n> ðŸŽ¥ Click the image above for a short video overview of building a linear regression model\n\n1. Select the variables you want to use in your classification model and split the training and test sets calling `train_test_split()`:\n\n    ```python\n    from sklearn.model_selection import train_test_split\n    \n    X = encoded_pumpkins[encoded_pumpkins.columns.difference(['Color'])]\n    y = encoded_pumpkins['Color']\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n    \n    ```\n\n2. Now you can train your model, by calling `fit()` with your training data, and print out its result:\n\n    ```python\n    from sklearn.metrics import f1_score, classification_report \n    from sklearn.linear_model import LogisticRegression\n\n    model = LogisticRegression()\n    model.fit(X_train, y_train)\n    predictions = model.predict(X_test)\n\n    print(classification_report(y_test, predictions))\n    print('Predicted labels: ', predictions)\n    print('F1-score: ', f1_score(y_test, predictions))\n    ```\n\n    Take a look at your model's scoreboard. It's not bad, considering you have only about 1000 rows of data:\n\n    ```output\n                       precision    recall  f1-score   support\n    \n                    0       0.94      0.98      0.96       166\n                    1       0.85      0.67      0.75        33\n    \n        accuracy                                0.92       199\n        macro avg           0.89      0.82      0.85       199\n        weighted avg        0.92      0.92      0.92       199\n    \n        Predicted labels:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0\n        0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n        1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0\n        0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1 1 0\n        0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n        0 0 0 1 0 0 0 0 0 0 0 0 1 1]\n        F1-score:  0.7457627118644068\n    ```\n\n## Better comprehension via a confusion matrix\n\nWhile you can get a scoreboard report [terms](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html?highlight=classification_report#sklearn.metrics.classification_report) by printing out the items above, you might be able to understand your model more easily by using a [confusion matrix](https://scikit-learn.org/stable/modules/model_evaluation.html#confusion-matrix) to help us understand how the model is performing.\n\n> ðŸŽ“ A '[confusion matrix](https://wikipedia.org/wiki/Confusion_matrix)' (or 'error matrix') is a table that expresses your model's true vs. false positives and negatives, thus gauging the accuracy of predictions.\n\n1. To use a confusion metrics, call `confusion_matrix()`:\n\n    ```python\n    from sklearn.metrics import confusion_matrix\n    confusion_matrix(y_test, predictions)\n    ```\n\n    Take a look at your model's confusion matrix:\n\n    ```output\n    array([[162,   4],\n           [ 11,  22]])\n    ```\n\nIn Scikit-learn, confusion matrices Rows (axis 0) are actual labels and columns (axis 1) are predicted labels.\n\n|       |   0   |   1   |\n| :---: | :---: | :---: |\n|   0   |  TN   |  FP   |\n|   1   |  FN   |  TP   |\n\nWhat's going on here? Let's say our model is asked to classify pumpkins between two binary categories, category 'white' and category 'not-white'.\n\n- If your model predicts a pumpkin as not white and it belongs to category 'not-white' in reality we call it a true negative, shown by the top left number.\n- If your model predicts a pumpkin as white and it belongs to category 'not-white' in reality we call it a false negative, shown by the bottom left number. \n- If your model predicts a pumpkin as not white and it belongs to category 'white' in reality we call it a false positive, shown by the top right number. \n- If your model predicts a pumpkin as white and it belongs to category 'white' in reality we call it a true positive, shown by the bottom right number.\n\nAs you might have guessed it's preferable to have a larger number of true positives and true negatives and a lower number of false positives and false negatives, which implies that the model performs better.\n\nHow does the confusion matrix relate to precision and recall? Remember, the classification report printed above showed precision (0.85) and recall (0.67).\n\nPrecision = tp / (tp + fp) = 22 / (22 + 4) = 0.8461538461538461\n\nRecall = tp / (tp + fn) = 22 / (22 + 11) = 0.6666666666666666\n\nâœ… Q: According to the confusion matrix, how did the model do? A: Not bad; there are a good number of true negatives but also a few false negatives. \n\nLet's revisit the terms we saw earlier with the help of the confusion matrix's mapping of TP/TN and FP/FN:\n\nðŸŽ“ Precision: TP/(TP + FP) The fraction of relevant instances among the retrieved instances (e.g. which labels were well-labeled)\n\nðŸŽ“ Recall: TP/(TP + FN) The fraction of relevant instances that were retrieved, whether well-labeled or not\n\nðŸŽ“ f1-score: (2 * precision * recall)/(precision + recall) A weighted average of the precision and recall, with best being 1 and worst being 0\n\nðŸŽ“ Support: The number of occurrences of each label retrieved\n\nðŸŽ“ Accuracy: (TP + TN)/(TP + TN + FP + FN) The percentage of labels predicted accurately for a sample.\n\nðŸŽ“ Macro Avg: The calculation of the unweighted mean metrics for each label, not taking label imbalance into account.\n\nðŸŽ“ Weighted Avg: The calculation of the mean metrics for each label, taking label imbalance into account by weighting them by their support (the number of true instances for each label).\n\nâœ… Can you think which metric you should watch if you want your model to reduce the number of false negatives?\n\n## Visualize the ROC curve of this model\n\n[![ML for beginners - Analyzing Logistic Regression Performance with ROC Curves](https://img.youtube.com/vi/GApO575jTA0/0.jpg)](https://youtu.be/GApO575jTA0 \"ML for beginners - Analyzing Logistic Regression Performance with ROC Curves\")\n\n> ðŸŽ¥ Click the image above for a short video overview of ROC curves\n\nLet's do one more visualization to see the so-called 'ROC' curve:\n\n```python\nfrom sklearn.metrics import roc_curve, roc_auc_score\nimport matplotlib\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ny_scores = model.predict_proba(X_test)\nfpr, tpr, thresholds = roc_curve(y_test, y_scores[:,1])\n\nfig = plt.figure(figsize=(6, 6))\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot(fpr, tpr)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.show()\n```\n\nUsing Matplotlib, plot the model's [Receiving Operating Characteristic](https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html?highlight=roc) or ROC. ROC curves are often used to get a view of the output of a classifier in terms of its true vs. false positives. \"ROC curves typically feature true positive rate on the Y axis, and false positive rate on the X axis.\" Thus, the steepness of the curve and the space between the midpoint line and the curve matter: you want a curve that quickly heads up and over the line. In our case, there are false positives to start with, and then the line heads up and over properly:\n\n![ROC](./images/ROC_2.png)\n\nFinally, use Scikit-learn's [`roc_auc_score` API](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html?highlight=roc_auc#sklearn.metrics.roc_auc_score) to compute the actual 'Area Under the Curve' (AUC):\n\n```python\nauc = roc_auc_score(y_test,y_scores[:,1])\nprint(auc)\n```\nThe result is `0.9749908725812341`. Given that the AUC ranges from 0 to 1, you want a big score, since a model that is 100% correct in its predictions will have an AUC of 1; in this case, the model is _pretty good_. \n\nIn future lessons on classifications, you will learn how to iterate to improve your model's scores. But for now, congratulations! You've completed these regression lessons!\n\n---\n## ðŸš€Challenge\n\nThere's a lot more to unpack regarding logistic regression! But the best way to learn is to experiment. Find a dataset that lends itself to this type of analysis and build a model with it. What do you learn? tip: try [Kaggle](https://www.kaggle.com/search?q=logistic+regression+datasets) for interesting datasets.\n\n## [Post-lecture quiz](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/16/)\n\n## Review & Self Study\n\nRead the first few pages of [this paper from Stanford](https://web.stanford.edu/~jurafsky/slp3/5.pdf) on some practical uses for logistic regression. Think about tasks that are better suited for one or the other type of regression tasks that we have studied up to this point. What would work best?\n\n## Assignment \n\n[Retrying this regression](assignment.md)\n",
              "pdfUrl": "",
              "videoUrl": "",
              "orderIndex": 1
            }
          ],
          "quizzes": [],
          "subModules": []
        }
      ]
    },
    {
      "id": 29,
      "title": "Web App",
      "orderIndex": 2,
      "lessons": [
        {
          "id": 30,
          "title": "Web App",
          "content": "# Build a web app to use your ML model\n\nIn this section of the curriculum, you will be introduced to an applied ML topic: how to save your Scikit-learn model as a file that can be used to make predictions within a web application. Once the model is saved, you'll learn how to use it in a web app built in Flask. You'll first create a model using some data that's all about UFO sightings! Then, you'll build a web app that will allow you to input a number of seconds with a latitude and a longitude value to predict which country reported seeing a UFO.\n\n![UFO Parking](images/ufo.jpg)\n\nPhoto by <a href=\"https://unsplash.com/@mdherren?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">Michael Herren</a> on <a href=\"https://unsplash.com/s/photos/ufo?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">Unsplash</a>\n\n## Lessons\n\n1. [Build a Web App](1-Web-App/README.md)\n\n## Credits\n\n\"Build a Web App\" was written with â™¥ï¸ by [Jen Looper](https://twitter.com/jenlooper).\n\nâ™¥ï¸ The quizzes were written by Rohan Raj.\n\nThe dataset is sourced from [Kaggle](https://www.kaggle.com/NUFORC/ufo-sightings).\n\nThe web app architecture was suggested in part by [this article](https://towardsdatascience.com/how-to-easily-deploy-machine-learning-models-using-flask-b95af8fe34d4) and [this repo](https://github.com/abhinavsagar/machine-learning-deployment) by Abhinav Sagar.\n",
          "pdfUrl": "",
          "videoUrl": "",
          "orderIndex": 0
        }
      ],
      "quizzes": [],
      "subModules": [
        {
          "id": 31,
          "title": "Web App",
          "orderIndex": 0,
          "lessons": [
            {
              "id": 32,
              "title": "Web App",
              "content": "# Try a different model\n\n## Instructions\n\nNow that you have built one web app using a trained Regression model, use one of the models from an earlier Regression lesson to redo this web app. You can keep the style or design it differently to reflect the pumpkin data. Be careful to change the inputs to reflect your model's training method.\n\n## Rubric\n\n| Criteria                   | Exemplary                                                 | Adequate                                                  | Needs Improvement                      |\n| -------------------------- | --------------------------------------------------------- | --------------------------------------------------------- | -------------------------------------- |\n| | The web app runs as expected and is deployed to the cloud | The web app contains flaws or exhibits unexpected results | The web app does not function properly |\n",
              "pdfUrl": "",
              "videoUrl": "",
              "orderIndex": 0
            },
            {
              "id": 33,
              "title": "Web App",
              "content": "# Build a Web App to use a ML Model\n\nIn this lesson, you will train an ML model on a data set that's out of this world: _UFO sightings over the past century_, sourced from NUFORC's database.\n\nYou will learn:\n\n- How to 'pickle' a trained model\n- How to use that model in a Flask app\n\nWe will continue our use of notebooks to clean data and train our model, but you can take the process one step further by exploring using a model 'in the wild', so to speak: in a web app.\n\nTo do this, you need to build a web app using Flask.\n\n## [Pre-lecture quiz](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/17/)\n\n## Building an app\n\nThere are several ways to build web apps to consume machine learning models. Your web architecture may influence the way your model is trained. Imagine that you are working in a business where the data science group has trained a model that they want you to use in an app.\n\n### Considerations\n\nThere are many questions you need to ask:\n\n- **Is it a web app or a mobile app?** If you are building a mobile app or need to use the model in an IoT context, you could use [TensorFlow Lite](https://www.tensorflow.org/lite/) and use the model in an Android or iOS app.\n- **Where will the model reside?** In the cloud or locally?\n- **Offline support.** Does the app have to work offline?\n- **What technology was used to train the model?** The chosen technology may influence the tooling you need to use.\n    - **Using TensorFlow.** If you are training a model using TensorFlow, for example, that ecosystem provides the ability to convert a TensorFlow model for use in a web app by using [TensorFlow.js](https://www.tensorflow.org/js/).\n    - **Using PyTorch.** If you are building a model using a library such as [PyTorch](https://pytorch.org/), you have the option to export it in [ONNX](https://onnx.ai/) (Open Neural Network Exchange) format for use in JavaScript web apps that can use the [Onnx Runtime](https://www.onnxruntime.ai/). This option will be explored in a future lesson for a Scikit-learn-trained model.\n    - **Using Lobe.ai or Azure Custom Vision.** If you are using an ML SaaS (Software as a Service) system such as [Lobe.ai](https://lobe.ai/) or [Azure Custom Vision](https://azure.microsoft.com/services/cognitive-services/custom-vision-service/?WT.mc_id=academic-77952-leestott) to train a model, this type of software provides ways to export the model for many platforms, including building a bespoke API to be queried in the cloud by your online application.\n\nYou also have the opportunity to build an entire Flask web app that would be able to train the model itself in a web browser. This can also be done using TensorFlow.js in a JavaScript context.\n\nFor our purposes, since we have been working with Python-based notebooks, let's explore the steps you need to take to export a trained model from such a notebook to a format readable by a Python-built web app.\n\n## Tool\n\nFor this task, you need two tools: Flask and Pickle, both of which run on Python.\n\nâœ… What's [Flask](https://palletsprojects.com/p/flask/)? Defined as a 'micro-framework' by its creators, Flask provides the basic features of web frameworks using Python and a templating engine to build web pages. Take a look at [this Learn module](https://docs.microsoft.com/learn/modules/python-flask-build-ai-web-app?WT.mc_id=academic-77952-leestott) to practice building with Flask.\n\nâœ… What's [Pickle](https://docs.python.org/3/library/pickle.html)? Pickle ðŸ¥’ is a Python module that serializes and de-serializes a Python object structure. When you 'pickle' a model, you serialize or flatten its structure for use on the web. Be careful: pickle is not intrinsically secure, so be careful if prompted to 'un-pickle' a file. A pickled file has the suffix `.pkl`.\n\n## Exercise - clean your data\n\nIn this lesson you'll use data from 80,000 UFO sightings, gathered by [NUFORC](https://nuforc.org) (The National UFO Reporting Center). This data has some interesting descriptions of UFO sightings, for example:\n\n- **Long example description.** \"A man emerges from a beam of light that shines on a grassy field at night and he runs towards the Texas Instruments parking lot\".\n- **Short example description.** \"the lights chased us\".\n\nThe [ufos.csv](./data/ufos.csv) spreadsheet includes columns about the `city`, `state` and `country` where the sighting occurred, the object's `shape` and its `latitude` and `longitude`.\n\nIn the blank [notebook](notebook.ipynb) included in this lesson:\n\n1. import `pandas`, `matplotlib`, and `numpy` as you did in previous lessons and import the ufos spreadsheet. You can take a look at a sample data set:\n\n    ```python\n    import pandas as pd\n    import numpy as np\n    \n    ufos = pd.read_csv('./data/ufos.csv')\n    ufos.head()\n    ```\n\n1. Convert the ufos data to a small dataframe with fresh titles. Check the unique values in the `Country` field.\n\n    ```python\n    ufos = pd.DataFrame({'Seconds': ufos['duration (seconds)'], 'Country': ufos['country'],'Latitude': ufos['latitude'],'Longitude': ufos['longitude']})\n    \n    ufos.Country.unique()\n    ```\n\n1. Now, you can reduce the amount of data we need to deal with by dropping any null values and only importing sightings between 1-60 seconds:\n\n    ```python\n    ufos.dropna(inplace=True)\n    \n    ufos = ufos[(ufos['Seconds'] >= 1) & (ufos['Seconds'] <= 60)]\n    \n    ufos.info()\n    ```\n\n1. Import Scikit-learn's `LabelEncoder` library to convert the text values for countries to a number:\n\n    âœ… LabelEncoder encodes data alphabetically\n\n    ```python\n    from sklearn.preprocessing import LabelEncoder\n    \n    ufos['Country'] = LabelEncoder().fit_transform(ufos['Country'])\n    \n    ufos.head()\n    ```\n\n    Your data should look like this:\n\n    ```output\n    \tSeconds\tCountry\tLatitude\tLongitude\n    2\t20.0\t3\t\t53.200000\t-2.916667\n    3\t20.0\t4\t\t28.978333\t-96.645833\n    14\t30.0\t4\t\t35.823889\t-80.253611\n    23\t60.0\t4\t\t45.582778\t-122.352222\n    24\t3.0\t\t3\t\t51.783333\t-0.783333\n    ```\n\n## Exercise - build your model\n\nNow you can get ready to train a model by dividing the data into the training and testing group.\n\n1. Select the three features you want to train on as your X vector, and the y vector will be the `Country`. You want to be able to input `Seconds`, `Latitude` and `Longitude` and get a country id to return.\n\n    ```python\n    from sklearn.model_selection import train_test_split\n    \n    Selected_features = ['Seconds','Latitude','Longitude']\n    \n    X = ufos[Selected_features]\n    y = ufos['Country']\n    \n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n    ```\n\n1. Train your model using logistic regression:\n\n    ```python\n    from sklearn.metrics import accuracy_score, classification_report\n    from sklearn.linear_model import LogisticRegression\n    model = LogisticRegression()\n    model.fit(X_train, y_train)\n    predictions = model.predict(X_test)\n    \n    print(classification_report(y_test, predictions))\n    print('Predicted labels: ', predictions)\n    print('Accuracy: ', accuracy_score(y_test, predictions))\n    ```\n\nThe accuracy isn't bad **(around 95%)**, unsurprisingly, as `Country` and `Latitude/Longitude` correlate.\n\nThe model you created isn't very revolutionary as you should be able to infer a `Country` from its `Latitude` and `Longitude`, but it's a good exercise to try to train from raw data that you cleaned, exported, and then use this model in a web app.\n\n## Exercise - 'pickle' your model\n\nNow, it's time to _pickle_ your model! You can do that in a few lines of code. Once it's _pickled_, load your pickled model and test it against a sample data array containing values for seconds, latitude and longitude,\n\n```python\nimport pickle\nmodel_filename = 'ufo-model.pkl'\npickle.dump(model, open(model_filename,'wb'))\n\nmodel = pickle.load(open('ufo-model.pkl','rb'))\nprint(model.predict([[50,44,-12]]))\n```\n\nThe model returns **'3'**, which is the country code for the UK. Wild! ðŸ‘½\n\n## Exercise - build a Flask app\n\nNow you can build a Flask app to call your model and return similar results, but in a more visually pleasing way.\n\n1. Start by creating a folder called **web-app** next to the _notebook.ipynb_ file where your _ufo-model.pkl_ file resides.\n\n1. In that folder create three more folders: **static**, with a folder **css** inside it, and **templates**. You should now have the following files and directories:\n\n    ```output\n    web-app/\n      static/\n        css/\n      templates/\n    notebook.ipynb\n    ufo-model.pkl\n    ```\n\n    âœ… Refer to the solution folder for a view of the finished app\n\n1. The first file to create in _web-app_ folder is **requirements.txt** file. Like _package.json_ in a JavaScript app, this file lists dependencies required by the app. In **requirements.txt** add the lines:\n\n    ```text\n    scikit-learn\n    pandas\n    numpy\n    flask\n    ```\n\n1. Now, run this file by navigating to _web-app_:\n\n    ```bash\n    cd web-app\n    ```\n\n1. In your terminal type `pip install`, to install the libraries listed in _requirements.txt_:\n\n    ```bash\n    pip install -r requirements.txt\n    ```\n\n1. Now, you're ready to create three more files to finish the app:\n\n    1. Create **app.py** in the root.\n    2. Create **index.html** in _templates_ directory.\n    3. Create **styles.css** in _static/css_ directory.\n\n1. Build out the _styles.css_ file with a few styles:\n\n    ```css\n    body {\n    \twidth: 100%;\n    \theight: 100%;\n    \tfont-family: 'Helvetica';\n    \tbackground: black;\n    \tcolor: #fff;\n    \ttext-align: center;\n    \tletter-spacing: 1.4px;\n    \tfont-size: 30px;\n    }\n    \n    input {\n    \tmin-width: 150px;\n    }\n    \n    .grid {\n    \twidth: 300px;\n    \tborder: 1px solid #2d2d2d;\n    \tdisplay: grid;\n    \tjustify-content: center;\n    \tmargin: 20px auto;\n    }\n    \n    .box {\n    \tcolor: #fff;\n    \tbackground: #2d2d2d;\n    \tpadding: 12px;\n    \tdisplay: inline-block;\n    }\n    ```\n\n1. Next, build out the _index.html_ file:\n\n    ```html\n    <!DOCTYPE html>\n    <html>\n      <head>\n        <meta charset=\"UTF-8\">\n        <title>ðŸ›¸ UFO Appearance Prediction! ðŸ‘½</title>\n        <link rel=\"stylesheet\" href=\"{{ url_for('static', filename='css/styles.css') }}\">\n      </head>\n    \n      <body>\n        <div class=\"grid\">\n    \n          <div class=\"box\">\n    \n            <p>According to the number of seconds, latitude and longitude, which country is likely to have reported seeing a UFO?</p>\n    \n            <form action=\"{{ url_for('predict')}}\" method=\"post\">\n              <input type=\"number\" name=\"seconds\" placeholder=\"Seconds\" required=\"required\" min=\"0\" max=\"60\" />\n              <input type=\"text\" name=\"latitude\" placeholder=\"Latitude\" required=\"required\" />\n              <input type=\"text\" name=\"longitude\" placeholder=\"Longitude\" required=\"required\" />\n              <button type=\"submit\" class=\"btn\">Predict country where the UFO is seen</button>\n            </form>\n    \n            <p>{{ prediction_text }}</p>\n    \n          </div>\n    \n        </div>\n    \n      </body>\n    </html>\n    ```\n\n    Take a look at the templating in this file. Notice the 'mustache' syntax around variables that will be provided by the app, like the prediction text: `{{}}`. There's also a form that posts a prediction to the `/predict` route.\n\n    Finally, you're ready to build the python file that drives the consumption of the model and the display of predictions:\n\n1. In `app.py` add:\n\n    ```python\n    import numpy as np\n    from flask import Flask, request, render_template\n    import pickle\n    \n    app = Flask(__name__)\n    \n    model = pickle.load(open(\"./ufo-model.pkl\", \"rb\"))\n    \n    \n    @app.route(\"/\")\n    def home():\n        return render_template(\"index.html\")\n    \n    \n    @app.route(\"/predict\", methods=[\"POST\"])\n    def predict():\n    \n        int_features = [int(x) for x in request.form.values()]\n        final_features = [np.array(int_features)]\n        prediction = model.predict(final_features)\n    \n        output = prediction[0]\n    \n        countries = [\"Australia\", \"Canada\", \"Germany\", \"UK\", \"US\"]\n    \n        return render_template(\n            \"index.html\", prediction_text=\"Likely country: {}\".format(countries[output])\n        )\n    \n    \n    if __name__ == \"__main__\":\n        app.run(debug=True)\n    ```\n\n    > ðŸ’¡ Tip: when you add [`debug=True`](https://www.askpython.com/python-modules/flask/flask-debug-mode) while running the web app using Flask, any changes you make to your application will be reflected immediately without the need to restart the server. Beware! Don't enable this mode in a production app.\n\nIf you run `python app.py` or `python3 app.py` - your web server starts up, locally, and you can fill out a short form to get an answer to your burning question about where UFOs have been sighted!\n\nBefore doing that, take a look at the parts of `app.py`:\n\n1. First, dependencies are loaded and the app starts.\n1. Then, the model is imported.\n1. Then, index.html is rendered on the home route.\n\nOn the `/predict` route, several things happen when the form is posted:\n\n1. The form variables are gathered and converted to a numpy array. They are then sent to the model and a prediction is returned.\n2. The Countries that we want displayed are re-rendered as readable text from their predicted country code, and that value is sent back to index.html to be rendered in the template.\n\nUsing a model this way, with Flask and a pickled model, is relatively straightforward. The hardest thing is to understand what shape the data is that must be sent to the model to get a prediction. That all depends on how the model was trained. This one has three data points to be input in order to get a prediction.\n\nIn a professional setting, you can see how good communication is necessary between the folks who train the model and those who consume it in a web or mobile app. In our case, it's only one person, you!\n\n---\n\n## ðŸš€ Challenge\n\nInstead of working in a notebook and importing the model to the Flask app, you could train the model right within the Flask app! Try converting your Python code in the notebook, perhaps after your data is cleaned, to train the model from within the app on a route called `train`. What are the pros and cons of pursuing this method?\n\n## [Post-lecture quiz](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/18/)\n\n## Review & Self Study\n\nThere are many ways to build a web app to consume ML models. Make a list of the ways you could use JavaScript or Python to build a web app to leverage machine learning. Consider architecture: should the model stay in the app or live in the cloud? If the latter, how would you access it? Draw out an architectural model for an applied ML web solution.\n\n## Assignment\n\n[Try a different model](assignment.md)\n",
              "pdfUrl": "",
              "videoUrl": "",
              "orderIndex": 1
            }
          ],
          "quizzes": [],
          "subModules": []
        }
      ]
    },
    {
      "id": 34,
      "title": "Classification",
      "orderIndex": 3,
      "lessons": [
        {
          "id": 35,
          "title": "Classification",
          "content": "# Getting started with classification\n\n## Regional topic: Delicious Asian and Indian Cuisines ðŸœ\n\nIn Asia and India, food traditions are extremely diverse, and very delicious! Let's look at data about regional cuisines to try to understand their ingredients.\n\n![Thai food seller](./images/thai-food.jpg)\n> Photo by <a href=\"https://unsplash.com/@changlisheng?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">Lisheng Chang</a> on <a href=\"https://unsplash.com/s/photos/asian-food?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">Unsplash</a>\n  \n## What you will learn\n\nIn this section, you will build on your earlier study of Regression and learn about other classifiers that you can use to better understand the data.\n\n> There are useful low-code tools that can help you learn about working with classification models. Try [Azure ML for this task](https://docs.microsoft.com/learn/modules/create-classification-model-azure-machine-learning-designer/?WT.mc_id=academic-77952-leestott)\n\n## Lessons\n\n1. [Introduction to classification](1-Introduction/README.md)\n2. [More classifiers](2-Classifiers-1/README.md)\n3. [Yet other classifiers](3-Classifiers-2/README.md)\n4. [Applied ML: build a web app](4-Applied/README.md)\n\n## Credits\n\n\"Getting started with classification\" was written with â™¥ï¸ by [Cassie Breviu](https://www.twitter.com/cassiebreviu) and [Jen Looper](https://www.twitter.com/jenlooper)\n\nThe delicious cuisines dataset was sourced from [Kaggle](https://www.kaggle.com/hoandan/asian-and-indian-cuisines).\n",
          "pdfUrl": "",
          "videoUrl": "",
          "orderIndex": 0
        }
      ],
      "quizzes": [],
      "subModules": [
        {
          "id": 36,
          "title": "Introduction",
          "orderIndex": 0,
          "lessons": [
            {
              "id": 37,
              "title": "Introduction",
              "content": "# Explore classification methods\n\n## Instructions\n\nIn [Scikit-learn documentation](https://scikit-learn.org/stable/supervised_learning.html) you'll find a large list of ways to classify data. Do a little scavenger hunt in these docs: your goals is to look for classification methods and match a dataset in this curriculum, a question you can ask of it, and a technique of classification. Create a spreadsheet or table in a .doc file and explain how the dataset would work with the classification algorithm.\n\n## Rubric\n\n| Criteria | Exemplary                                                                                                                           | Adequate                                                                                                                            | Needs Improvement                                                                                                                                             |\n| -------- | ----------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n|          | a document is presented overviewing 5 algorithms alongside a classification technique. The overview is well-explained and detailed. | a document is presented overviewing 3 algorithms alongside a classification technique. The overview is well-explained and detailed. | a document is presented overviewing fewer than three algorithms alongside a classification technique and the overview is neither well-explained nor detailed. |\n",
              "pdfUrl": "",
              "videoUrl": "",
              "orderIndex": 0
            },
            {
              "id": 38,
              "title": "Introduction",
              "content": "# Introduction to classification\n\nIn these four lessons, you will explore a fundamental focus of classic machine learning - _classification_. We will walk through using various classification algorithms with a dataset about all the brilliant cuisines of Asia and India. Hope you're hungry!\n\n![just a pinch!](images/pinch.png)\n\n> Celebrate pan-Asian cuisines in these lessons! Image by [Jen Looper](https://twitter.com/jenlooper)\n\nClassification is a form of [supervised learning](https://wikipedia.org/wiki/Supervised_learning) that bears a lot in common with regression techniques. If machine learning is all about predicting values or names to things by using datasets, then classification generally falls into two groups: _binary classification_ and _multiclass classification_.\n\n[![Introduction to classification](https://img.youtube.com/vi/eg8DJYwdMyg/0.jpg)](https://youtu.be/eg8DJYwdMyg \"Introduction to classification\")\n\n> ðŸŽ¥ Click the image above for a video: MIT's John Guttag introduces classification\n\nRemember:\n\n- **Linear regression** helped you predict relationships between variables and make accurate predictions on where a new datapoint would fall in relationship to that line. So, you could predict _what price a pumpkin would be in September vs. December_, for example.\n- **Logistic regression** helped you discover \"binary categories\": at this price point, _is this pumpkin orange or not-orange_?\n\nClassification uses various algorithms to determine other ways of determining a data point's label or class. Let's work with this cuisine data to see whether, by observing a group of ingredients, we can determine its cuisine of origin.\n\n## [Pre-lecture quiz](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/19/)\n\n> ### [This lesson is available in R!](./solution/R/lesson_10.html)\n\n### Introduction\n\nClassification is one of the fundamental activities of the machine learning researcher and data scientist. From basic classification of a binary value (\"is this email spam or not?\"), to complex image classification and segmentation using computer vision, it's always useful to be able to sort data into classes and ask questions of it.\n\nTo state the process in a more scientific way, your classification method creates a predictive model that enables you to map the relationship between input variables to output variables.\n\n![binary vs. multiclass classification](images/binary-multiclass.png)\n\n> Binary vs. multiclass problems for classification algorithms to handle. Infographic by [Jen Looper](https://twitter.com/jenlooper)\n\nBefore starting the process of cleaning our data, visualizing it, and prepping it for our ML tasks, let's learn a bit about the various ways machine learning can be leveraged to classify data.\n\nDerived from [statistics](https://wikipedia.org/wiki/Statistical_classification), classification using classic machine learning uses features, such as `smoker`, `weight`, and `age` to determine _likelihood of developing X disease_. As a supervised learning technique similar to the regression exercises you performed earlier, your data is labeled and the ML algorithms use those labels to classify and predict classes (or 'features') of a dataset and assign them to a group or outcome.\n\nâœ… Take a moment to imagine a dataset about cuisines. What would a multiclass model be able to answer? What would a binary model be able to answer? What if you wanted to determine whether a given cuisine was likely to use fenugreek? What if you wanted to see if, given a present of a grocery bag full of star anise, artichokes, cauliflower, and horseradish, you could create a typical Indian dish?\n\n[![Crazy mystery baskets](https://img.youtube.com/vi/GuTeDbaNoEU/0.jpg)](https://youtu.be/GuTeDbaNoEU \"Crazy mystery baskets\")\n\n> ðŸŽ¥ Click the image above for a video.The whole premise of the show 'Chopped' is the 'mystery basket' where chefs have to make some dish out of a random choice of ingredients. Surely a ML model would have helped!\n\n## Hello 'classifier'\n\nThe question we want to ask of this cuisine dataset is actually a **multiclass question**, as we have several potential national cuisines to work with. Given a batch of ingredients, which of these many classes will the data fit?\n\nScikit-learn offers several different algorithms to use to classify data, depending on the kind of problem you want to solve. In the next two lessons, you'll learn about several of these algorithms.\n\n## Exercise - clean and balance your data\n\nThe first task at hand, before starting this project, is to clean and **balance** your data to get better results. Start with the blank _notebook.ipynb_ file in the root of this folder.\n\nThe first thing to install is [imblearn](https://imbalanced-learn.org/stable/). This is a Scikit-learn package that will allow you to better balance the data (you will learn more about this task in a minute).\n\n1. To install `imblearn`, run `pip install`, like so:\n\n    ```python\n    pip install imblearn\n    ```\n\n1. Import the packages you need to import your data and visualize it, also import `SMOTE` from `imblearn`.\n\n    ```python\n    import pandas as pd\n    import matplotlib.pyplot as plt\n    import matplotlib as mpl\n    import numpy as np\n    from imblearn.over_sampling import SMOTE\n    ```\n\n    Now you are set up to read import the data next.\n\n1. The next task will be to import the data:\n\n    ```python\n    df  = pd.read_csv('../data/cuisines.csv')\n    ```\n\n   Using `read_csv()` will read the content of the csv file _cusines.csv_ and place it in the variable `df`.\n\n1. Check the data's shape:\n\n    ```python\n    df.head()\n    ```\n\n   The first five rows look like this:\n\n    ```output\n    |     | Unnamed: 0 | cuisine | almond | angelica | anise | anise_seed | apple | apple_brandy | apricot | armagnac | ... | whiskey | white_bread | white_wine | whole_grain_wheat_flour | wine | wood | yam | yeast | yogurt | zucchini |\n    | --- | ---------- | ------- | ------ | -------- | ----- | ---------- | ----- | ------------ | ------- | -------- | --- | ------- | ----------- | ---------- | ----------------------- | ---- | ---- | --- | ----- | ------ | -------- |\n    | 0   | 65         | indian  | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 0      | 0        |\n    | 1   | 66         | indian  | 1      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 0      | 0        |\n    | 2   | 67         | indian  | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 0      | 0        |\n    | 3   | 68         | indian  | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 0      | 0        |\n    | 4   | 69         | indian  | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 1      | 0        |\n    ```\n\n1. Get info about this data by calling `info()`:\n\n    ```python\n    df.info()\n    ```\n\n    Your out resembles:\n\n    ```output\n    <class 'pandas.core.frame.DataFrame'>\n    RangeIndex: 2448 entries, 0 to 2447\n    Columns: 385 entries, Unnamed: 0 to zucchini\n    dtypes: int64(384), object(1)\n    memory usage: 7.2+ MB\n    ```\n\n## Exercise - learning about cuisines\n\nNow the work starts to become more interesting. Let's discover the distribution of data, per cuisine \n\n1. Plot the data as bars by calling `barh()`:\n\n    ```python\n    df.cuisine.value_counts().plot.barh()\n    ```\n\n    ![cuisine data distribution](images/cuisine-dist.png)\n\n    There are a finite number of cuisines, but the distribution of data is uneven. You can fix that! Before doing so, explore a little more. \n\n1. Find out how much data is available per cuisine and print it out:\n\n    ```python\n    thai_df = df[(df.cuisine == \"thai\")]\n    japanese_df = df[(df.cuisine == \"japanese\")]\n    chinese_df = df[(df.cuisine == \"chinese\")]\n    indian_df = df[(df.cuisine == \"indian\")]\n    korean_df = df[(df.cuisine == \"korean\")]\n    \n    print(f'thai df: {thai_df.shape}')\n    print(f'japanese df: {japanese_df.shape}')\n    print(f'chinese df: {chinese_df.shape}')\n    print(f'indian df: {indian_df.shape}')\n    print(f'korean df: {korean_df.shape}')\n    ```\n\n    the output looks like so:\n\n    ```output\n    thai df: (289, 385)\n    japanese df: (320, 385)\n    chinese df: (442, 385)\n    indian df: (598, 385)\n    korean df: (799, 385)\n    ```\n\n## Discovering ingredients\n\nNow you can dig deeper into the data and learn what are the typical ingredients per cuisine. You should clean out recurrent data that creates confusion between cuisines, so let's learn about this problem.\n\n1. Create a function `create_ingredient()` in Python to create an ingredient dataframe. This function will start by dropping an unhelpful column and sort through ingredients by their count:\n\n    ```python\n    def create_ingredient_df(df):\n        ingredient_df = df.T.drop(['cuisine','Unnamed: 0']).sum(axis=1).to_frame('value')\n        ingredient_df = ingredient_df[(ingredient_df.T != 0).any()]\n        ingredient_df = ingredient_df.sort_values(by='value', ascending=False,\n        inplace=False)\n        return ingredient_df\n    ```\n\n   Now you can use that function to get an idea of top ten most popular ingredients by cuisine.\n\n1. Call `create_ingredient()` and plot it calling `barh()`:\n\n    ```python\n    thai_ingredient_df = create_ingredient_df(thai_df)\n    thai_ingredient_df.head(10).plot.barh()\n    ```\n\n    ![thai](images/thai.png)\n\n1. Do the same for the japanese data:\n\n    ```python\n    japanese_ingredient_df = create_ingredient_df(japanese_df)\n    japanese_ingredient_df.head(10).plot.barh()\n    ```\n\n    ![japanese](images/japanese.png)\n\n1. Now for the chinese ingredients:\n\n    ```python\n    chinese_ingredient_df = create_ingredient_df(chinese_df)\n    chinese_ingredient_df.head(10).plot.barh()\n    ```\n\n    ![chinese](images/chinese.png)\n\n1. Plot the indian ingredients:\n\n    ```python\n    indian_ingredient_df = create_ingredient_df(indian_df)\n    indian_ingredient_df.head(10).plot.barh()\n    ```\n\n    ![indian](images/indian.png)\n\n1. Finally, plot the korean ingredients:\n\n    ```python\n    korean_ingredient_df = create_ingredient_df(korean_df)\n    korean_ingredient_df.head(10).plot.barh()\n    ```\n\n    ![korean](images/korean.png)\n\n1. Now, drop the most common ingredients that create confusion between distinct cuisines, by calling `drop()`: \n\n   Everyone loves rice, garlic and ginger!\n\n    ```python\n    feature_df= df.drop(['cuisine','Unnamed: 0','rice','garlic','ginger'], axis=1)\n    labels_df = df.cuisine #.unique()\n    feature_df.head()\n    ```\n\n## Balance the dataset\n\nNow that you have cleaned the data, use [SMOTE](https://imbalanced-learn.org/dev/references/generated/imblearn.over_sampling.SMOTE.html) - \"Synthetic Minority Over-sampling Technique\" - to balance it.\n\n1. Call `fit_resample()`, this strategy generates new samples by interpolation.\n\n    ```python\n    oversample = SMOTE()\n    transformed_feature_df, transformed_label_df = oversample.fit_resample(feature_df, labels_df)\n    ```\n\n    By balancing your data, you'll have better results when classifying it. Think about a binary classification. If most of your data is one class, a ML model is going to predict that class more frequently, just because there is more data for it. Balancing the data takes any skewed data and helps remove this imbalance. \n\n1. Now you can check the numbers of labels per ingredient:\n\n    ```python\n    print(f'new label count: {transformed_label_df.value_counts()}')\n    print(f'old label count: {df.cuisine.value_counts()}')\n    ```\n\n    Your output looks like so:\n\n    ```output\n    new label count: korean      799\n    chinese     799\n    indian      799\n    japanese    799\n    thai        799\n    Name: cuisine, dtype: int64\n    old label count: korean      799\n    indian      598\n    chinese     442\n    japanese    320\n    thai        289\n    Name: cuisine, dtype: int64\n    ```\n\n    The data is nice and clean, balanced, and very delicious! \n\n1. The last step is to save your balanced data, including labels and features, into a new dataframe that can be exported into a file:\n\n    ```python\n    transformed_df = pd.concat([transformed_label_df,transformed_feature_df],axis=1, join='outer')\n    ```\n\n1. You can take one more look at the data using `transformed_df.head()` and `transformed_df.info()`. Save a copy of this data for use in future lessons:\n\n    ```python\n    transformed_df.head()\n    transformed_df.info()\n    transformed_df.to_csv(\"../data/cleaned_cuisines.csv\")\n    ```\n\n    This fresh CSV can now be found in the root data folder.\n\n---\n\n## ðŸš€Challenge\n\nThis curriculum contains several interesting datasets. Dig through the `data` folders and see if any contain datasets that would be appropriate for binary or multi-class classification? What questions would you ask of this dataset?\n\n## [Post-lecture quiz](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/20/)\n\n## Review & Self Study\n\nExplore SMOTE's API. What use cases is it best used for? What problems does it solve?\n\n## Assignment \n\n[Explore classification methods](assignment.md)\n",
              "pdfUrl": "",
              "videoUrl": "",
              "orderIndex": 1
            }
          ],
          "quizzes": [],
          "subModules": []
        },
        {
          "id": 39,
          "title": "Classifiers 1",
          "orderIndex": 1,
          "lessons": [
            {
              "id": 40,
              "title": "Classifiers 1",
              "content": "# Study the solvers\n## Instructions\n\nIn this lesson you learned about the various solvers that pair algorithms with a machine learning process to create an accurate model. Walk through the solvers listed in the lesson and pick two. In your own words, compare and contrast these two solvers. What kind of problem do they address? How do they work with various data structures? Why would you pick one over another? \n## Rubric\n\n| Criteria | Exemplary                                                                                      | Adequate                                         | Needs Improvement            |\n| -------- | ---------------------------------------------------------------------------------------------- | ------------------------------------------------ | ---------------------------- |\n|          | A .doc file is presented with two paragraphs, one on each solver, comparing them thoughtfully. | A .doc file is presented with only one paragraph | The assignment is incomplete |\n",
              "pdfUrl": "",
              "videoUrl": "",
              "orderIndex": 0
            },
            {
              "id": 41,
              "title": "Classifiers 1",
              "content": "# Cuisine classifiers 1\n\nIn this lesson, you will use the dataset you saved from the last lesson full of balanced, clean data all about cuisines.\n\nYou will use this dataset with a variety of classifiers to _predict a given national cuisine based on a group of ingredients_. While doing so, you'll learn more about some of the ways that algorithms can be leveraged for classification tasks.\n\n## [Pre-lecture quiz](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/21/)\n# Preparation\n\nAssuming you completed [Lesson 1](../1-Introduction/README.md), make sure that a _cleaned_cuisines.csv_ file exists in the root `/data` folder for these four lessons.\n\n## Exercise - predict a national cuisine\n\n1. Working in this lesson's _notebook.ipynb_ folder, import that file along with the Pandas library:\n\n    ```python\n    import pandas as pd\n    cuisines_df = pd.read_csv(\"../data/cleaned_cuisines.csv\")\n    cuisines_df.head()\n    ```\n\n    The data looks like this:\n\n|     | Unnamed: 0 | cuisine | almond | angelica | anise | anise_seed | apple | apple_brandy | apricot | armagnac | ... | whiskey | white_bread | white_wine | whole_grain_wheat_flour | wine | wood | yam | yeast | yogurt | zucchini |\n| --- | ---------- | ------- | ------ | -------- | ----- | ---------- | ----- | ------------ | ------- | -------- | --- | ------- | ----------- | ---------- | ----------------------- | ---- | ---- | --- | ----- | ------ | -------- |\n| 0   | 0          | indian  | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 0      | 0        |\n| 1   | 1          | indian  | 1      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 0      | 0        |\n| 2   | 2          | indian  | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 0      | 0        |\n| 3   | 3          | indian  | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 0      | 0        |\n| 4   | 4          | indian  | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 1      | 0        |\n  \n\n1. Now, import several more libraries:\n\n    ```python\n    from sklearn.linear_model import LogisticRegression\n    from sklearn.model_selection import train_test_split, cross_val_score\n    from sklearn.metrics import accuracy_score,precision_score,confusion_matrix,classification_report, precision_recall_curve\n    from sklearn.svm import SVC\n    import numpy as np\n    ```\n\n1. Divide the X and y coordinates into two dataframes for training. `cuisine` can be the labels dataframe:\n\n    ```python\n    cuisines_label_df = cuisines_df['cuisine']\n    cuisines_label_df.head()\n    ```\n\n    It will look like this:\n\n    ```output\n    0    indian\n    1    indian\n    2    indian\n    3    indian\n    4    indian\n    Name: cuisine, dtype: object\n    ```\n\n1. Drop that `Unnamed: 0` column and the `cuisine` column, calling `drop()`. Save the rest of the data as trainable features:\n\n    ```python\n    cuisines_feature_df = cuisines_df.drop(['Unnamed: 0', 'cuisine'], axis=1)\n    cuisines_feature_df.head()\n    ```\n\n    Your features look like this:\n\n|      | almond | angelica | anise | anise_seed | apple | apple_brandy | apricot | armagnac | artemisia | artichoke |  ... | whiskey | white_bread | white_wine | whole_grain_wheat_flour | wine | wood |  yam | yeast | yogurt | zucchini |\n| ---: | -----: | -------: | ----: | ---------: | ----: | -----------: | ------: | -------: | --------: | --------: | ---: | ------: | ----------: | ---------: | ----------------------: | ---: | ---: | ---: | ----: | -----: | -------: |\n|    0 |      0 |        0 |     0 |          0 |     0 |            0 |       0 |        0 |         0 |         0 |  ... |       0 |           0 |          0 |                       0 |    0 |    0 |    0 |     0 |      0 |        0 | 0 |\n|    1 |      1 |        0 |     0 |          0 |     0 |            0 |       0 |        0 |         0 |         0 |  ... |       0 |           0 |          0 |                       0 |    0 |    0 |    0 |     0 |      0 |        0 | 0 |\n|    2 |      0 |        0 |     0 |          0 |     0 |            0 |       0 |        0 |         0 |         0 |  ... |       0 |           0 |          0 |                       0 |    0 |    0 |    0 |     0 |      0 |        0 | 0 |\n|    3 |      0 |        0 |     0 |          0 |     0 |            0 |       0 |        0 |         0 |         0 |  ... |       0 |           0 |          0 |                       0 |    0 |    0 |    0 |     0 |      0 |        0 | 0 |\n|    4 |      0 |        0 |     0 |          0 |     0 |            0 |       0 |        0 |         0 |         0 |  ... |       0 |           0 |          0 |                       0 |    0 |    0 |    0 |     0 |      1 |        0 | 0 |\n\nNow you are ready to train your model!\n\n## Choosing your classifier\n\nNow that your data is clean and ready for training, you have to decide which algorithm to use for the job. \n\nScikit-learn groups classification under Supervised Learning, and in that category you will find many ways to classify. [The variety](https://scikit-learn.org/stable/supervised_learning.html) is quite bewildering at first sight. The following methods all include classification techniques:\n\n- Linear Models\n- Support Vector Machines\n- Stochastic Gradient Descent\n- Nearest Neighbors\n- Gaussian Processes\n- Decision Trees\n- Ensemble methods (voting Classifier)\n- Multiclass and multioutput algorithms (multiclass and multilabel classification, multiclass-multioutput classification)\n\n> You can also use [neural networks to classify data](https://scikit-learn.org/stable/modules/neural_networks_supervised.html#classification), but that is outside the scope of this lesson.\n\n### What classifier to go with?\n\nSo, which classifier should you choose? Often, running through several and looking for a good result is a way to test. Scikit-learn offers a [side-by-side comparison](https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html) on a created dataset, comparing KNeighbors, SVC two ways, GaussianProcessClassifier, DecisionTreeClassifier, RandomForestClassifier, MLPClassifier, AdaBoostClassifier, GaussianNB and QuadraticDiscrinationAnalysis, showing the results visualized: \n\n![comparison of classifiers](images/comparison.png)\n> Plots generated on Scikit-learn's documentation\n\n> AutoML solves this problem neatly by running these comparisons in the cloud, allowing you to choose the best algorithm for your data. Try it [here](https://docs.microsoft.com/learn/modules/automate-model-selection-with-azure-automl/?WT.mc_id=academic-77952-leestott)\n\n### A better approach\n\nA better way than wildly guessing, however, is to follow the ideas on this downloadable [ML Cheat sheet](https://docs.microsoft.com/azure/machine-learning/algorithm-cheat-sheet?WT.mc_id=academic-77952-leestott). Here, we discover that, for our multiclass problem, we have some choices:\n\n![cheatsheet for multiclass problems](images/cheatsheet.png)\n> A section of Microsoft's Algorithm Cheat Sheet, detailing multiclass classification options\n\nâœ… Download this cheat sheet, print it out, and hang it on your wall!\n\n### Reasoning\n\nLet's see if we can reason our way through different approaches given the constraints we have:\n\n- **Neural networks are too heavy**. Given our clean, but minimal dataset, and the fact that we are running training locally via notebooks, neural networks are too heavyweight for this task.\n- **No two-class classifier**. We do not use a two-class classifier, so that rules out one-vs-all. \n- **Decision tree or logistic regression could work**. A decision tree might work, or logistic regression for multiclass data. \n- **Multiclass Boosted Decision Trees solve a different problem**. The multiclass boosted decision tree is most suitable for nonparametric tasks, e.g. tasks designed to build rankings, so it is not useful for us.\n\n### Using Scikit-learn \n\nWe will be using Scikit-learn to analyze our data. However, there are many ways to use logistic regression in Scikit-learn. Take a look at the [parameters to pass](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html?highlight=logistic%20regressio#sklearn.linear_model.LogisticRegression).  \n\nEssentially there are two important parameters - `multi_class` and `solver` - that we need to specify, when we ask Scikit-learn to perform a logistic regression. The `multi_class` value applies a certain behavior. The value of the solver is what algorithm to use. Not all solvers can be paired with all `multi_class` values.\n\nAccording to the docs, in the multiclass case, the training algorithm:\n\n- **Uses the one-vs-rest (OvR) scheme**, if the `multi_class` option is set to `ovr`\n- **Uses the cross-entropy loss**, if the `multi_class` option is set to `multinomial`. (Currently the `multinomial` option is supported only by the â€˜lbfgsâ€™, â€˜sagâ€™, â€˜sagaâ€™ and â€˜newton-cgâ€™ solvers.)\"\n\n> ðŸŽ“ The 'scheme' here can either be 'ovr' (one-vs-rest) or 'multinomial'. Since logistic regression is really designed to support binary classification, these schemes allow it to better handle multiclass classification tasks. [source](https://machinelearningmastery.com/one-vs-rest-and-one-vs-one-for-multi-class-classification/)\n\n> ðŸŽ“ The 'solver' is defined as \"the algorithm to use in the optimization problem\". [source](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html?highlight=logistic%20regressio#sklearn.linear_model.LogisticRegression).\n\nScikit-learn offers this table to explain how solvers handle different challenges presented by different kinds of data structures:\n\n![solvers](images/solvers.png)\n\n## Exercise - split the data\n\nWe can focus on logistic regression for our first training trial since you recently learned about the latter in a previous lesson.\nSplit your data into training and testing groups by calling `train_test_split()`:\n\n```python\nX_train, X_test, y_train, y_test = train_test_split(cuisines_feature_df, cuisines_label_df, test_size=0.3)\n```\n\n## Exercise - apply logistic regression\n\nSince you are using the multiclass case, you need to choose what _scheme_ to use and what _solver_ to set. Use LogisticRegression with a multiclass setting and the **liblinear** solver to train.\n\n1. Create a logistic regression with multi_class set to `ovr` and the solver set to `liblinear`:\n\n    ```python\n    lr = LogisticRegression(multi_class='ovr',solver='liblinear')\n    model = lr.fit(X_train, np.ravel(y_train))\n    \n    accuracy = model.score(X_test, y_test)\n    print (\"Accuracy is {}\".format(accuracy))\n    ```\n\n    âœ… Try a different solver like `lbfgs`, which is often set as default\n\n    > Note, use Pandas [`ravel`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.ravel.html) function to flatten your data when needed.\n\n    The accuracy is good at over **80%**!\n\n1. You can see this model in action by testing one row of data (#50):\n\n    ```python\n    print(f'ingredients: {X_test.iloc[50][X_test.iloc[50]!=0].keys()}')\n    print(f'cuisine: {y_test.iloc[50]}')\n    ```\n\n    The result is printed:\n\n   ```output\n   ingredients: Index(['cilantro', 'onion', 'pea', 'potato', 'tomato', 'vegetable_oil'], dtype='object')\n   cuisine: indian\n   ```\n\n   âœ… Try a different row number and check the results\n\n1. Digging deeper, you can check for the accuracy of this prediction:\n\n    ```python\n    test= X_test.iloc[50].values.reshape(-1, 1).T\n    proba = model.predict_proba(test)\n    classes = model.classes_\n    resultdf = pd.DataFrame(data=proba, columns=classes)\n    \n    topPrediction = resultdf.T.sort_values(by=[0], ascending = [False])\n    topPrediction.head()\n    ```\n\n    The result is printed - Indian cuisine is its best guess, with good probability:\n\n    |          |        0 |\n    | -------: | -------: |\n    |   indian | 0.715851 |\n    |  chinese | 0.229475 |\n    | japanese | 0.029763 |\n    |   korean | 0.017277 |\n    |     thai | 0.007634 |\n\n    âœ… Can you explain why the model is pretty sure this is an Indian cuisine?\n\n1. Get more detail by printing a classification report, as you did in the regression lessons:\n\n    ```python\n    y_pred = model.predict(X_test)\n    print(classification_report(y_test,y_pred))\n    ```\n\n    |              | precision | recall | f1-score | support |\n    | ------------ | --------- | ------ | -------- | ------- |\n    | chinese      | 0.73      | 0.71   | 0.72     | 229     |\n    | indian       | 0.91      | 0.93   | 0.92     | 254     |\n    | japanese     | 0.70      | 0.75   | 0.72     | 220     |\n    | korean       | 0.86      | 0.76   | 0.81     | 242     |\n    | thai         | 0.79      | 0.85   | 0.82     | 254     |\n    | accuracy     | 0.80      | 1199   |          |         |\n    | macro avg    | 0.80      | 0.80   | 0.80     | 1199    |\n    | weighted avg | 0.80      | 0.80   | 0.80     | 1199    |\n\n## ðŸš€Challenge\n\nIn this lesson, you used your cleaned data to build a machine learning model that can predict a national cuisine based on a series of ingredients. Take some time to read through the many options Scikit-learn provides to classify data. Dig deeper into the concept of 'solver' to understand what goes on behind the scenes.\n\n## [Post-lecture quiz](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/22/)\n\n## Review & Self Study\n\nDig a little more into the math behind logistic regression in [this lesson](https://people.eecs.berkeley.edu/~russell/classes/cs194/f11/lectures/CS194%20Fall%202011%20Lecture%2006.pdf)\n## Assignment \n\n[Study the solvers](assignment.md)\n",
              "pdfUrl": "",
              "videoUrl": "",
              "orderIndex": 1
            }
          ],
          "quizzes": [],
          "subModules": []
        },
        {
          "id": 42,
          "title": "Classifiers 2",
          "orderIndex": 2,
          "lessons": [
            {
              "id": 43,
              "title": "Classifiers 2",
              "content": "# Parameter Play\n\n## Instructions\n\nThere are a lot of parameters that are set by default when working with these classifiers. Intellisense in VS Code can help you dig into them. Adopt one of the ML Classification Techniques in this lesson and retrain models tweaking various parameter values. Build a notebook explaining why some changes help the model quality while others degrade it. Be detailed in your answer.\n\n## Rubric\n\n| Criteria | Exemplary                                                                                                              | Adequate                                              | Needs Improvement             |\n| -------- | ---------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------- | ----------------------------- |\n|          | A notebook is presented with a classifier fully built up and its parameters tweaked and changes explained in textboxes | A notebook is partially presented or poorly explained | A notebook is buggy or flawed |\n",
              "pdfUrl": "",
              "videoUrl": "",
              "orderIndex": 0
            },
            {
              "id": 44,
              "title": "Classifiers 2",
              "content": "# Cuisine classifiers 2\n\nIn this second classification lesson, you will explore more ways to classify numeric data. You will also learn about the ramifications for choosing one classifier over the other.\n\n## [Pre-lecture quiz](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/23/)\n\n### Prerequisite\n\nWe assume that you have completed the previous lessons and have a cleaned dataset in your `data` folder called _cleaned_cuisines.csv_ in the root of this 4-lesson folder.\n\n### Preparation\n\nWe have loaded your _notebook.ipynb_ file with the cleaned dataset and have divided it into X and y dataframes, ready for the model building process.\n\n## A classification map\n\nPreviously, you learned about the various options you have when classifying data using Microsoft's cheat sheet. Scikit-learn offers a similar, but more granular cheat sheet that can further help narrow down your estimators (another term for classifiers):\n\n![ML Map from Scikit-learn](images/map.png)\n> Tip: [visit this map online](https://scikit-learn.org/stable/tutorial/machine_learning_map/) and click along the path to read documentation.\n\n### The plan\n\nThis map is very helpful once you have a clear grasp of your data, as you can 'walk' along its paths to a decision:\n\n- We have >50 samples\n- We want to predict a category\n- We have labeled data\n- We have fewer than 100K samples\n- âœ¨ We can choose a Linear SVC\n- If that doesn't work, since we have numeric data\n    - We can try a âœ¨ KNeighbors Classifier \n      - If that doesn't work, try âœ¨ SVC and âœ¨ Ensemble Classifiers\n\nThis is a very helpful trail to follow.\n\n## Exercise - split the data\n\nFollowing this path, we should start by importing some libraries to use.\n\n1. Import the needed libraries:\n\n    ```python\n    from sklearn.neighbors import KNeighborsClassifier\n    from sklearn.linear_model import LogisticRegression\n    from sklearn.svm import SVC\n    from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n    from sklearn.model_selection import train_test_split, cross_val_score\n    from sklearn.metrics import accuracy_score,precision_score,confusion_matrix,classification_report, precision_recall_curve\n    import numpy as np\n    ```\n\n1. Split your training and test data:\n\n    ```python\n    X_train, X_test, y_train, y_test = train_test_split(cuisines_feature_df, cuisines_label_df, test_size=0.3)\n    ```\n\n## Linear SVC classifier\n\nSupport-Vector clustering (SVC) is a child of the Support-Vector machines family of ML techniques (learn more about these below). In this method, you can choose a 'kernel' to decide how to cluster the labels. The 'C' parameter refers to 'regularization' which regulates the influence of parameters. The kernel can be one of [several](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC); here we set it to 'linear' to ensure that we leverage linear SVC. Probability defaults to 'false'; here we set it to 'true' to gather probability estimates. We set the random state to '0' to shuffle the data to get probabilities.\n\n### Exercise - apply a linear SVC\n\nStart by creating an array of classifiers. You will add progressively to this array as we test. \n\n1. Start with a Linear SVC:\n\n    ```python\n    C = 10\n    # Create different classifiers.\n    classifiers = {\n        'Linear SVC': SVC(kernel='linear', C=C, probability=True,random_state=0)\n    }\n    ```\n\n2. Train your model using the Linear SVC and print out a report:\n\n    ```python\n    n_classifiers = len(classifiers)\n    \n    for index, (name, classifier) in enumerate(classifiers.items()):\n        classifier.fit(X_train, np.ravel(y_train))\n    \n        y_pred = classifier.predict(X_test)\n        accuracy = accuracy_score(y_test, y_pred)\n        print(\"Accuracy (train) for %s: %0.1f%% \" % (name, accuracy * 100))\n        print(classification_report(y_test,y_pred))\n    ```\n\n    The result is pretty good:\n\n    ```output\n    Accuracy (train) for Linear SVC: 78.6% \n                  precision    recall  f1-score   support\n    \n         chinese       0.71      0.67      0.69       242\n          indian       0.88      0.86      0.87       234\n        japanese       0.79      0.74      0.76       254\n          korean       0.85      0.81      0.83       242\n            thai       0.71      0.86      0.78       227\n    \n        accuracy                           0.79      1199\n       macro avg       0.79      0.79      0.79      1199\n    weighted avg       0.79      0.79      0.79      1199\n    ```\n\n## K-Neighbors classifier\n\nK-Neighbors is part of the \"neighbors\" family of ML methods, which can be used for both supervised and unsupervised learning. In this method, a predefined number of points is created and data are gathered around these points such that generalized labels can be predicted for the data.\n\n### Exercise - apply the K-Neighbors classifier\n\nThe previous classifier was good, and worked well with the data, but maybe we can get better accuracy. Try a K-Neighbors classifier.\n\n1. Add a line to your classifier array (add a comma after the Linear SVC item):\n\n    ```python\n    'KNN classifier': KNeighborsClassifier(C),\n    ```\n\n    The result is a little worse:\n\n    ```output\n    Accuracy (train) for KNN classifier: 73.8% \n                  precision    recall  f1-score   support\n    \n         chinese       0.64      0.67      0.66       242\n          indian       0.86      0.78      0.82       234\n        japanese       0.66      0.83      0.74       254\n          korean       0.94      0.58      0.72       242\n            thai       0.71      0.82      0.76       227\n    \n        accuracy                           0.74      1199\n       macro avg       0.76      0.74      0.74      1199\n    weighted avg       0.76      0.74      0.74      1199\n    ```\n\n    âœ… Learn about [K-Neighbors](https://scikit-learn.org/stable/modules/neighbors.html#neighbors)\n\n## Support Vector Classifier\n\nSupport-Vector classifiers are part of the [Support-Vector Machine](https://wikipedia.org/wiki/Support-vector_machine) family of ML methods that are used for classification and regression tasks. SVMs \"map training examples to points in space\" to maximize the distance between two categories. Subsequent data is mapped into this space so their category can be predicted.\n\n### Exercise - apply a Support Vector Classifier\n\nLet's try for a little better accuracy with a Support Vector Classifier.\n\n1. Add a comma after the K-Neighbors item, and then add this line:\n\n    ```python\n    'SVC': SVC(),\n    ```\n\n    The result is quite good!\n\n    ```output\n    Accuracy (train) for SVC: 83.2% \n                  precision    recall  f1-score   support\n    \n         chinese       0.79      0.74      0.76       242\n          indian       0.88      0.90      0.89       234\n        japanese       0.87      0.81      0.84       254\n          korean       0.91      0.82      0.86       242\n            thai       0.74      0.90      0.81       227\n    \n        accuracy                           0.83      1199\n       macro avg       0.84      0.83      0.83      1199\n    weighted avg       0.84      0.83      0.83      1199\n    ```\n\n    âœ… Learn about [Support-Vectors](https://scikit-learn.org/stable/modules/svm.html#svm)\n\n## Ensemble Classifiers\n\nLet's follow the path to the very end, even though the previous test was quite good. Let's try some 'Ensemble Classifiers, specifically Random Forest and AdaBoost:\n\n```python\n  'RFST': RandomForestClassifier(n_estimators=100),\n  'ADA': AdaBoostClassifier(n_estimators=100)\n```\n\nThe result is very good, especially for Random Forest:\n\n```output\nAccuracy (train) for RFST: 84.5% \n              precision    recall  f1-score   support\n\n     chinese       0.80      0.77      0.78       242\n      indian       0.89      0.92      0.90       234\n    japanese       0.86      0.84      0.85       254\n      korean       0.88      0.83      0.85       242\n        thai       0.80      0.87      0.83       227\n\n    accuracy                           0.84      1199\n   macro avg       0.85      0.85      0.84      1199\nweighted avg       0.85      0.84      0.84      1199\n\nAccuracy (train) for ADA: 72.4% \n              precision    recall  f1-score   support\n\n     chinese       0.64      0.49      0.56       242\n      indian       0.91      0.83      0.87       234\n    japanese       0.68      0.69      0.69       254\n      korean       0.73      0.79      0.76       242\n        thai       0.67      0.83      0.74       227\n\n    accuracy                           0.72      1199\n   macro avg       0.73      0.73      0.72      1199\nweighted avg       0.73      0.72      0.72      1199\n```\n\nâœ… Learn about [Ensemble Classifiers](https://scikit-learn.org/stable/modules/ensemble.html)\n\nThis method of Machine Learning \"combines the predictions of several base estimators\" to improve the model's quality. In our example, we used Random Trees and AdaBoost. \n\n- [Random Forest](https://scikit-learn.org/stable/modules/ensemble.html#forest), an averaging method, builds a 'forest' of 'decision trees' infused with randomness to avoid overfitting. The n_estimators parameter is set to the number of trees.\n\n- [AdaBoost](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html) fits a classifier to a dataset and then fits copies of that classifier to the same dataset. It focuses on the weights of incorrectly classified items and adjusts the fit for the next classifier to correct.\n\n---\n\n## ðŸš€Challenge\n\nEach of these techniques has a large number of parameters that you can tweak. Research each one's default parameters and think about what tweaking these parameters would mean for the model's quality.\n\n## [Post-lecture quiz](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/24/)\n\n## Review & Self Study\n\nThere's a lot of jargon in these lessons, so take a minute to review [this list](https://docs.microsoft.com/dotnet/machine-learning/resources/glossary?WT.mc_id=academic-77952-leestott) of useful terminology!\n\n## Assignment \n\n[Parameter play](assignment.md)\n",
              "pdfUrl": "",
              "videoUrl": "",
              "orderIndex": 1
            }
          ],
          "quizzes": [],
          "subModules": []
        },
        {
          "id": 45,
          "title": "Applied",
          "orderIndex": 3,
          "lessons": [
            {
              "id": 46,
              "title": "Applied",
              "content": "# Build a recommender\n\n## Instructions\n\nGiven your exercises in this lesson, you now know how to build JavaScript-based web app using Onnx Runtime and a converted Onnx model. Experiment with building a new recommender using data from these lessons or sourced elsewhere (give credit, please). You might create a pet recommender given various personality attributes, or a music genre recommender based on a person's mood. Be creative!\n\n## Rubric\n\n| Criteria | Exemplary                                                              | Adequate                              | Needs Improvement                 |\n| -------- | ---------------------------------------------------------------------- | ------------------------------------- | --------------------------------- |\n|          | A web app and notebook are presented, both well documented and running | One of those two is missing or flawed | Both are either missing or flawed |\n",
              "pdfUrl": "",
              "videoUrl": "",
              "orderIndex": 0
            },
            {
              "id": 47,
              "title": "Applied",
              "content": "# Build a Cuisine Recommender Web App\n\nIn this lesson, you will build a classification model using some of the techniques you have learned in previous lessons and with the delicious cuisine dataset used throughout this series. In addition, you will build a small web app to use a saved model, leveraging Onnx's web runtime.\n\nOne of the most useful practical uses of machine learning is building recommendation systems, and you can take the first step in that direction today!\n\n[![Presenting this web app](https://img.youtube.com/vi/17wdM9AHMfg/0.jpg)](https://youtu.be/17wdM9AHMfg \"Applied ML\")\n\n> ðŸŽ¥ Click the image above for a video: Jen Looper builds a web app using classified cuisine data\n\n## [Pre-lecture quiz](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/25/)\n\nIn this lesson you will learn:\n\n- How to build a model and save it as an Onnx model\n- How to use Netron to inspect the model\n- How to use your model in a web app for inference\n\n## Build your model\n\nBuilding applied ML systems is an important part of leveraging these technologies for your business systems. You can use models within your web applications (and thus use them in an offline context if needed) by using Onnx.\n\nIn a [previous lesson](../../3-Web-App/1-Web-App/README.md), you built a Regression model about UFO sightings, \"pickled\" it, and used it in a Flask app. While this architecture is very useful to know, it is a full-stack Python app, and your requirements may include the use of a JavaScript application. \n\nIn this lesson, you can build a basic JavaScript-based system for inference. First, however, you need to train a model and convert it for use with Onnx.\n\n## Exercise - train classification model\n\nFirst, train a classification model using the cleaned cuisines dataset we used. \n\n1. Start by importing useful libraries:\n\n    ```python\n    !pip install skl2onnx\n    import pandas as pd \n    ```\n\n    You need '[skl2onnx](https://onnx.ai/sklearn-onnx/)' to help convert your Scikit-learn model to Onnx format.\n\n1. Then, work with your data in the same way you did in previous lessons, by reading a CSV file using `read_csv()`:\n\n    ```python\n    data = pd.read_csv('../data/cleaned_cuisines.csv')\n    data.head()\n    ```\n\n1. Remove the first two unnecessary columns and save the remaining data as 'X':\n\n    ```python\n    X = data.iloc[:,2:]\n    X.head()\n    ```\n\n1. Save the labels as 'y':\n\n    ```python\n    y = data[['cuisine']]\n    y.head()\n    \n    ```\n\n### Commence the training routine\n\nWe will use the 'SVC' library which has good accuracy.\n\n1. Import the appropriate libraries from Scikit-learn:\n\n    ```python\n    from sklearn.model_selection import train_test_split\n    from sklearn.svm import SVC\n    from sklearn.model_selection import cross_val_score\n    from sklearn.metrics import accuracy_score,precision_score,confusion_matrix,classification_report\n    ```\n\n1. Separate training and test sets:\n\n    ```python\n    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3)\n    ```\n\n1. Build an SVC Classification model as you did in the previous lesson:\n\n    ```python\n    model = SVC(kernel='linear', C=10, probability=True,random_state=0)\n    model.fit(X_train,y_train.values.ravel())\n    ```\n\n1. Now, test your model, calling `predict()`:\n\n    ```python\n    y_pred = model.predict(X_test)\n    ```\n\n1. Print out a classification report to check the model's quality:\n\n    ```python\n    print(classification_report(y_test,y_pred))\n    ```\n\n    As we saw before, the accuracy is good:\n\n    ```output\n                    precision    recall  f1-score   support\n    \n         chinese       0.72      0.69      0.70       257\n          indian       0.91      0.87      0.89       243\n        japanese       0.79      0.77      0.78       239\n          korean       0.83      0.79      0.81       236\n            thai       0.72      0.84      0.78       224\n    \n        accuracy                           0.79      1199\n       macro avg       0.79      0.79      0.79      1199\n    weighted avg       0.79      0.79      0.79      1199\n    ```\n\n### Convert your model to Onnx\n\nMake sure to do the conversion with the proper Tensor number. This dataset has 380 ingredients listed, so you need to notate that number in `FloatTensorType`:\n\n1. Convert using a tensor number of 380.\n\n    ```python\n    from skl2onnx import convert_sklearn\n    from skl2onnx.common.data_types import FloatTensorType\n    \n    initial_type = [('float_input', FloatTensorType([None, 380]))]\n    options = {id(model): {'nocl': True, 'zipmap': False}}\n    ```\n\n1. Create the onx and store as a file **model.onnx**:\n\n    ```python\n    onx = convert_sklearn(model, initial_types=initial_type, options=options)\n    with open(\"./model.onnx\", \"wb\") as f:\n        f.write(onx.SerializeToString())\n    ```\n\n    > Note, you can pass in [options](https://onnx.ai/sklearn-onnx/parameterized.html) in your conversion script. In this case, we passed in 'nocl' to be True and 'zipmap' to be False. Since this is a classification model, you have the option to remove ZipMap which produces a list of dictionaries (not necessary). `nocl` refers to class information being included in the model. Reduce your model's size by setting `nocl` to 'True'. \n\nRunning the entire notebook will now build an Onnx model and save it to this folder.\n\n## View your model\n\nOnnx models are not very visible in Visual Studio code, but there's a very good free software that many researchers use to visualize the model to ensure that it is properly built. Download [Netron](https://github.com/lutzroeder/Netron) and  open your model.onnx file. You can see your simple model visualized, with its 380 inputs and classifier listed:\n\n![Netron visual](images/netron.png)\n\nNetron is a helpful tool to view your models.\n\nNow you are ready to use this neat model in a web app. Let's build an app that will come in handy when you look in your refrigerator and try to figure out which combination of your leftover ingredients you can use to cook a given cuisine, as determined by your model.\n\n## Build a recommender web application\n\nYou can use your model directly in a web app. This architecture also allows you to run it locally and even offline if needed. Start by creating an `index.html` file in the same folder where you stored your `model.onnx` file.\n\n1. In this file _index.html_, add the following markup:\n\n    ```html\n    <!DOCTYPE html>\n    <html>\n        <header>\n            <title>Cuisine Matcher</title>\n        </header>\n        <body>\n            ...\n        </body>\n    </html>\n    ```\n\n1. Now, working within the `body` tags, add a little markup to show a list of checkboxes reflecting some ingredients:\n\n    ```html\n    <h1>Check your refrigerator. What can you create?</h1>\n            <div id=\"wrapper\">\n                <div class=\"boxCont\">\n                    <input type=\"checkbox\" value=\"4\" class=\"checkbox\">\n                    <label>apple</label>\n                </div>\n            \n                <div class=\"boxCont\">\n                    <input type=\"checkbox\" value=\"247\" class=\"checkbox\">\n                    <label>pear</label>\n                </div>\n            \n                <div class=\"boxCont\">\n                    <input type=\"checkbox\" value=\"77\" class=\"checkbox\">\n                    <label>cherry</label>\n                </div>\n    \n                <div class=\"boxCont\">\n                    <input type=\"checkbox\" value=\"126\" class=\"checkbox\">\n                    <label>fenugreek</label>\n                </div>\n    \n                <div class=\"boxCont\">\n                    <input type=\"checkbox\" value=\"302\" class=\"checkbox\">\n                    <label>sake</label>\n                </div>\n    \n                <div class=\"boxCont\">\n                    <input type=\"checkbox\" value=\"327\" class=\"checkbox\">\n                    <label>soy sauce</label>\n                </div>\n    \n                <div class=\"boxCont\">\n                    <input type=\"checkbox\" value=\"112\" class=\"checkbox\">\n                    <label>cumin</label>\n                </div>\n            </div>\n            <div style=\"padding-top:10px\">\n                <button onClick=\"startInference()\">What kind of cuisine can you make?</button>\n            </div> \n    ```\n\n    Notice that each checkbox is given a value.  This reflects the index where the ingredient is found according to the dataset. Apple, for example, in this alphabetic list, occupies the fifth column, so its value is '4' since we start counting at 0. You can consult the [ingredients spreadsheet](../data/ingredient_indexes.csv) to discover a given ingredient's index.\n\n    Continuing your work in the index.html file, add a script block where the model is called after the final closing `</div>`. \n\n1. First, import the [Onnx Runtime](https://www.onnxruntime.ai/):\n\n    ```html\n    <script src=\"https://cdn.jsdelivr.net/npm/onnxruntime-web@1.9.0/dist/ort.min.js\"></script> \n    ```\n\n    > Onnx Runtime is used to enable running your Onnx models across a wide range of hardware platforms, including optimizations and an API to use.\n\n1. Once the Runtime is in place, you can call it:\n\n    ```html\n    <script>\n        const ingredients = Array(380).fill(0);\n        \n        const checks = [...document.querySelectorAll('.checkbox')];\n        \n        checks.forEach(check => {\n            check.addEventListener('change', function() {\n                // toggle the state of the ingredient\n                // based on the checkbox's value (1 or 0)\n                ingredients[check.value] = check.checked ? 1 : 0;\n            });\n        });\n\n        function testCheckboxes() {\n            // validate if at least one checkbox is checked\n            return checks.some(check => check.checked);\n        }\n\n        async function startInference() {\n\n            let atLeastOneChecked = testCheckboxes()\n\n            if (!atLeastOneChecked) {\n                alert('Please select at least one ingredient.');\n                return;\n            }\n            try {\n                // create a new session and load the model.\n                \n                const session = await ort.InferenceSession.create('./model.onnx');\n\n                const input = new ort.Tensor(new Float32Array(ingredients), [1, 380]);\n                const feeds = { float_input: input };\n\n                // feed inputs and run\n                const results = await session.run(feeds);\n\n                // read from results\n                alert('You can enjoy ' + results.label.data[0] + ' cuisine today!')\n\n            } catch (e) {\n                console.log(`failed to inference ONNX model`);\n                console.error(e);\n            }\n        }\n               \n    </script>\n    ```\n\nIn this code, there are several things happening:\n\n1. You created an array of 380 possible values (1 or 0) to be set and sent to the model for inference, depending on whether an ingredient checkbox is checked.\n2. You created an array of checkboxes and a way to determine whether they were checked in an `init` function that is called when the application starts. When a checkbox is checked, the `ingredients` array is altered to reflect the chosen ingredient.\n3. You created a `testCheckboxes` function that checks whether any checkbox was checked.\n4. You use `startInference` function when the button is pressed and, if any checkbox is checked, you start inference.\n5. The inference routine includes:\n   1. Setting up an asynchronous load of the model\n   2. Creating a Tensor structure to send to the model\n   3. Creating 'feeds' that reflects the `float_input` input that you created when training your model (you can use Netron to verify that name)\n   4. Sending these 'feeds' to the model and waiting for a response\n\n## Test your application\n\nOpen a terminal session in Visual Studio Code in the folder where your index.html file resides. Ensure that you have [http-server](https://www.npmjs.com/package/http-server) installed globally, and type `http-server` at the prompt. A localhost should open and you can view your web app. Check what cuisine is recommended based on various ingredients:\n\n![ingredient web app](images/web-app.png)\n\nCongratulations, you have created a 'recommendation' web app  with a few fields. Take some time to build out this system!\n## ðŸš€Challenge\n\nYour web app is very minimal, so continue to build it out using ingredients and their indexes from the [ingredient_indexes](../data/ingredient_indexes.csv) data. What flavor combinations work to create a given national dish?\n\n## [Post-lecture quiz](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/26/)\n\n## Review & Self Study\n\nWhile this lesson just touched on the utility of creating a recommendation system for food ingredients, this area of ML applications is very rich in examples. Read some more about how these systems are built:\n\n- https://www.sciencedirect.com/topics/computer-science/recommendation-engine\n- https://www.technologyreview.com/2014/08/25/171547/the-ultimate-challenge-for-recommendation-engines/\n- https://www.technologyreview.com/2015/03/23/168831/everything-is-a-recommendation/\n\n## Assignment \n\n[Build a new recommender](assignment.md)\n",
              "pdfUrl": "",
              "videoUrl": "",
              "orderIndex": 1
            }
          ],
          "quizzes": [],
          "subModules": []
        }
      ]
    },
    {
      "id": 48,
      "title": "Clustering",
      "orderIndex": 4,
      "lessons": [
        {
          "id": 49,
          "title": "Clustering",
          "content": "# Clustering models for machine learning\n\nClustering is a machine learning task where it looks to find objects that resemble one another and group these into groups called clusters.  What differs clustering from other approaches in machine learning, is that things happen automatically, in fact, it's fair to say it's the opposite of supervised learning. \n\n## Regional topic: clustering models for a Nigerian audience's musical taste ðŸŽ§\n\nNigeria's diverse audience has diverse musical tastes. Using data scraped from Spotify (inspired by [this article](https://towardsdatascience.com/country-wise-visual-analysis-of-music-taste-using-spotify-api-seaborn-in-python-77f5b749b421), let's look at some music popular in Nigeria. This dataset includes data about various songs' 'danceability' score, 'acousticness', loudness, 'speechiness', popularity and energy. It will be interesting to discover patterns in this data!\n\n![A turntable](./images/turntable.jpg)\n\n> Photo by <a href=\"https://unsplash.com/@marcelalaskoski?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">Marcela Laskoski</a> on <a href=\"https://unsplash.com/s/photos/nigerian-music?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">Unsplash</a>\n  \nIn this series of lessons, you will discover new ways to analyze data using clustering techniques. Clustering is particularly useful when your dataset lacks labels. If it does have labels, then classification techniques such as those you learned in previous lessons might be more useful. But in cases where you are looking to group unlabelled data, clustering is a great way to discover patterns.\n\n> There are useful low-code tools that can help you learn about working with clustering models. Try [Azure ML for this task](https://docs.microsoft.com/learn/modules/create-clustering-model-azure-machine-learning-designer/?WT.mc_id=academic-77952-leestott)\n\n## Lessons\n\n1. [Introduction to clustering](1-Visualize/README.md)\n2. [K-Means clustering](2-K-Means/README.md)\n\n## Credits\n\nThese lessons were written with ðŸŽ¶ by [Jen Looper](https://www.twitter.com/jenlooper) with helpful reviews by [Rishit Dagli](https://rishit_dagli) and [Muhammad Sakib Khan Inan](https://twitter.com/Sakibinan).\n\nThe [Nigerian Songs](https://www.kaggle.com/sootersaalu/nigerian-songs-spotify) dataset was sourced from Kaggle as scraped from Spotify.\n\nUseful K-Means examples that aided in creating this lesson include this [iris exploration](https://www.kaggle.com/bburns/iris-exploration-pca-k-means-and-gmm-clustering), this [introductory notebook](https://www.kaggle.com/prashant111/k-means-clustering-with-python), and this [hypothetical NGO example](https://www.kaggle.com/ankandash/pca-k-means-clustering-hierarchical-clustering).\n",
          "pdfUrl": "",
          "videoUrl": "",
          "orderIndex": 0
        }
      ],
      "quizzes": [],
      "subModules": [
        {
          "id": 50,
          "title": "Visualize",
          "orderIndex": 0,
          "lessons": [
            {
              "id": 51,
              "title": "Visualize",
              "content": "# Research other visualizations for clustering\n\n## Instructions\n\nIn this lesson, you have worked with some visualization techniques to get a grasp on plotting your data in preparation for clustering it. Scatterplots, in particular are useful for finding groups of objects. Research different ways and different libraries to create scatterplots and document your work in a notebook. You can use the data from this lesson, other lessons, or data you source yourself (please credit its source, however, in your notebook). Plot some data using scatterplots and explain what you discover.\n\n## Rubric\n\n| Criteria | Exemplary                                                      | Adequate                                                                                 | Needs Improvement                   |\n| -------- | -------------------------------------------------------------- | ---------------------------------------------------------------------------------------- | ----------------------------------- |\n|          | A notebook is presented with five well-documented scatterplots | A notebook is presented with fewer than five scatterplots and it is less well documented | An incomplete notebook is presented |\n",
              "pdfUrl": "",
              "videoUrl": "",
              "orderIndex": 0
            },
            {
              "id": 52,
              "title": "Visualize",
              "content": "# Introduction to clustering\n\nClustering is a type of [Unsupervised Learning](https://wikipedia.org/wiki/Unsupervised_learning) that presumes that a dataset is unlabelled or that its inputs are not matched with predefined outputs. It uses various algorithms to sort through unlabeled data and provide groupings according to patterns it discerns in the data. \n\n[![No One Like You by PSquare](https://img.youtube.com/vi/ty2advRiWJM/0.jpg)](https://youtu.be/ty2advRiWJM \"No One Like You by PSquare\")\n\n> ðŸŽ¥ Click the image above for a video. While you're studying machine learning with clustering, enjoy some Nigerian Dance Hall tracks - this is a highly rated song from 2014 by PSquare.\n## [Pre-lecture quiz](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/27/)\n### Introduction\n\n[Clustering](https://link.springer.com/referenceworkentry/10.1007%2F978-0-387-30164-8_124) is very useful for data exploration. Let's see if it can help discover trends and patterns in the way Nigerian audiences consume music.\n\nâœ… Take a minute to think about the uses of clustering. In real life, clustering happens whenever you have a pile of laundry and need to sort out your family members' clothes ðŸ§¦ðŸ‘•ðŸ‘–ðŸ©². In data science, clustering happens when trying to analyze a user's preferences, or determine the characteristics of any unlabeled dataset. Clustering, in a way, helps make sense of chaos, like a sock drawer.\n\n[![Introduction to ML](https://img.youtube.com/vi/esmzYhuFnds/0.jpg)](https://youtu.be/esmzYhuFnds \"Introduction to Clustering\")\n\n> ðŸŽ¥ Click the image above for a video: MIT's John Guttag introduces clustering\n\nIn a professional setting, clustering can be used to determine things like market segmentation, determining what age groups buy what items, for example. Another use would be anomaly detection, perhaps to detect fraud from a dataset of credit card transactions. Or you might use clustering to determine tumors in a batch of medical scans. \n\nâœ… Think a minute about how you might have encountered clustering 'in the wild', in a banking, e-commerce, or business setting.\n\n> ðŸŽ“ Interestingly, cluster analysis originated in the fields of Anthropology and Psychology in the 1930s. Can you imagine how it might have been used?\n\nAlternately, you could use it for grouping search results - by shopping links, images, or reviews, for example. Clustering is useful when you have a large dataset that you want to reduce and on which you want to perform more granular analysis, so the technique can be used to learn about data before other models are constructed.\n\nâœ… Once your data is organized in clusters, you assign it a cluster Id, and this technique can be useful when preserving a dataset's privacy; you can instead refer to a data point by its cluster id, rather than by more revealing identifiable data. Can you think of other reasons why you'd refer to a cluster Id rather than other elements of the cluster to identify it?\n\nDeepen your understanding of clustering techniques in this [Learn module](https://docs.microsoft.com/learn/modules/train-evaluate-cluster-models?WT.mc_id=academic-77952-leestott)\n## Getting started with clustering\n\n[Scikit-learn offers a large array](https://scikit-learn.org/stable/modules/clustering.html) of methods to perform clustering. The type you choose will depend on your use case. According to the documentation, each method has various benefits. Here is a simplified table of the methods supported by Scikit-learn and their appropriate use cases:\n\n| Method name                  | Use case                                                               |\n| :--------------------------- | :--------------------------------------------------------------------- |\n| K-Means                      | general purpose, inductive                                             |\n| Affinity propagation         | many, uneven clusters, inductive                                       |\n| Mean-shift                   | many, uneven clusters, inductive                                       |\n| Spectral clustering          | few, even clusters, transductive                                       |\n| Ward hierarchical clustering | many, constrained clusters, transductive                               |\n| Agglomerative clustering     | many, constrained, non Euclidean distances, transductive               |\n| DBSCAN                       | non-flat geometry, uneven clusters, transductive                       |\n| OPTICS                       | non-flat geometry, uneven clusters with variable density, transductive |\n| Gaussian mixtures            | flat geometry, inductive                                               |\n| BIRCH                        | large dataset with outliers, inductive                                 |\n\n> ðŸŽ“ How we create clusters has a lot to do with how we gather up the data points into groups. Let's unpack some vocabulary:\n>\n> ðŸŽ“ ['Transductive' vs. 'inductive'](https://wikipedia.org/wiki/Transduction_(machine_learning))\n> \n> Transductive inference is derived from observed training cases that map to specific test cases. Inductive inference is derived from training cases that map to general rules which are only then applied to test cases. \n> \n> An example: Imagine you have a dataset that is only partially  labelled. Some things are 'records', some 'cds', and some are blank. Your job is to provide labels for the blanks. If you choose an inductive approach, you'd train a model looking for 'records' and 'cds', and apply those labels to your unlabeled data. This approach will have trouble classifying things that are actually 'cassettes'. A transductive approach, on the other hand, handles this unknown data more effectively as it works to group similar items together and then applies a label to a group. In this case, clusters might reflect 'round musical things' and 'square musical things'. \n> \n> ðŸŽ“ ['Non-flat' vs. 'flat' geometry](https://datascience.stackexchange.com/questions/52260/terminology-flat-geometry-in-the-context-of-clustering)\n> \n> Derived from mathematical terminology, non-flat vs. flat geometry refers to the measure of distances between points by either 'flat' ([Euclidean](https://wikipedia.org/wiki/Euclidean_geometry)) or 'non-flat' (non-Euclidean) geometrical methods. \n>\n>'Flat' in this context refers to Euclidean geometry (parts of which are taught as 'plane' geometry), and non-flat refers to non-Euclidean geometry. What does geometry have to do with machine learning? Well, as two fields that are rooted in mathematics, there must be a common way to measure distances between points in clusters, and that can be done in a 'flat' or 'non-flat' way, depending on the nature of the data. [Euclidean distances](https://wikipedia.org/wiki/Euclidean_distance) are measured as the length of a line segment between two points. [Non-Euclidean distances](https://wikipedia.org/wiki/Non-Euclidean_geometry) are measured along a curve. If your data, visualized, seems to not exist on a plane, you might need to use a specialized algorithm to handle it.\n>\n![Flat vs Nonflat Geometry Infographic](./images/flat-nonflat.png)\n> Infographic by [Dasani Madipalli](https://twitter.com/dasani_decoded)\n> \n> ðŸŽ“ ['Distances'](https://web.stanford.edu/class/cs345a/slides/12-clustering.pdf)\n> \n> Clusters are defined by their distance matrix, e.g. the distances between points. This distance can be measured in a few ways. Euclidean clusters are defined by the average of the point values, and contain a 'centroid' or center point. Distances are thus measured by the distance to that centroid. Non-Euclidean distances refer to 'clustroids', the point closest to other points. Clustroids in turn can be defined in various ways.\n> \n> ðŸŽ“ ['Constrained'](https://wikipedia.org/wiki/Constrained_clustering)\n> \n> [Constrained Clustering](https://web.cs.ucdavis.edu/~davidson/Publications/ICDMTutorial.pdf) introduces 'semi-supervised' learning into this unsupervised method. The relationships between points are flagged as 'cannot link' or 'must-link' so some rules are forced on the dataset.\n>\n>An example: If an algorithm is set free on a batch of unlabelled or semi-labelled data, the clusters it produces may be of poor quality. In the example above, the clusters might group 'round music things' and 'square music things' and 'triangular things' and 'cookies'. If given some constraints, or rules to follow (\"the item must be made of plastic\", \"the item needs to be able to produce music\") this can help 'constrain' the algorithm to make better choices.\n> \n> ðŸŽ“ 'Density'\n> \n> Data that is 'noisy' is considered to be 'dense'. The distances between points in each of its clusters may prove, on examination, to be more or less dense, or 'crowded' and thus this data needs to be analyzed with the appropriate clustering method. [This article](https://www.kdnuggets.com/2020/02/understanding-density-based-clustering.html) demonstrates the difference between using K-Means clustering vs. HDBSCAN algorithms to explore a noisy dataset with uneven cluster density.\n\n## Clustering algorithms\n\nThere are over 100 clustering algorithms, and their use depends on the nature of the data at hand. Let's discuss some of the major ones:\n\n- **Hierarchical clustering**. If an object is classified by its proximity to a nearby object, rather than to one farther away, clusters are formed based on their members' distance to and from other objects. Scikit-learn's agglomerative clustering is hierarchical.\n\n   ![Hierarchical clustering Infographic](./images/hierarchical.png)\n   > Infographic by [Dasani Madipalli](https://twitter.com/dasani_decoded)\n\n- **Centroid clustering**. This popular algorithm requires the choice of 'k', or the number of clusters to form, after which the algorithm determines the center point of a cluster and gathers data around that point. [K-means clustering](https://wikipedia.org/wiki/K-means_clustering) is a popular version of centroid clustering. The center is determined by the nearest mean, thus the name. The squared distance from the cluster is minimized.\n\n   ![Centroid clustering Infographic](./images/centroid.png)\n   > Infographic by [Dasani Madipalli](https://twitter.com/dasani_decoded)\n\n- **Distribution-based clustering**. Based in statistical modeling, distribution-based clustering centers on determining the probability that a data point belongs to a cluster, and assigning it accordingly. Gaussian mixture methods belong to this type.\n\n- **Density-based clustering**. Data points are assigned to clusters based on their density, or their grouping around each other. Data points far from the group are considered outliers or noise. DBSCAN, Mean-shift and OPTICS belong to this type of clustering.\n\n- **Grid-based clustering**. For multi-dimensional datasets, a grid is created and the data is divided amongst the grid's cells, thereby creating clusters.\n\n## Exercise - cluster your data\n\nClustering as a technique is greatly aided by proper visualization, so let's get started by visualizing our music data. This exercise will help us decide which of the methods of clustering we should most effectively use for the nature of this data.\n\n1. Open the [_notebook.ipynb_](https://github.com/microsoft/ML-For-Beginners/blob/main/5-Clustering/1-Visualize/notebook.ipynb) file in this folder.\n\n1. Import the `Seaborn` package for good data visualization.\n\n    ```python\n    !pip install seaborn\n    ```\n\n1. Append the song data from [_nigerian-songs.csv_](https://github.com/microsoft/ML-For-Beginners/blob/main/5-Clustering/data/nigerian-songs.csv). Load up a dataframe with some data about the songs. Get ready to explore this data by importing the libraries and dumping out the data:\n\n    ```python\n    import matplotlib.pyplot as plt\n    import pandas as pd\n    \n    df = pd.read_csv(\"../data/nigerian-songs.csv\")\n    df.head()\n    ```\n\n    Check the first few lines of data:\n\n    |     | name                     | album                        | artist              | artist_top_genre | release_date | length | popularity | danceability | acousticness | energy | instrumentalness | liveness | loudness | speechiness | tempo   | time_signature |\n    | --- | ------------------------ | ---------------------------- | ------------------- | ---------------- | ------------ | ------ | ---------- | ------------ | ------------ | ------ | ---------------- | -------- | -------- | ----------- | ------- | -------------- |\n    | 0   | Sparky                   | Mandy & The Jungle           | Cruel Santino       | alternative r&b  | 2019         | 144000 | 48         | 0.666        | 0.851        | 0.42   | 0.534            | 0.11     | -6.699   | 0.0829      | 133.015 | 5              |\n    | 1   | shuga rush               | EVERYTHING YOU HEARD IS TRUE | Odunsi (The Engine) | afropop          | 2020         | 89488  | 30         | 0.71         | 0.0822       | 0.683  | 0.000169         | 0.101    | -5.64    | 0.36        | 129.993 | 3              |\n    | 2   | LITT!                    | LITT!                        | AYLÃ˜                | indie r&b        | 2018         | 207758 | 40         | 0.836        | 0.272        | 0.564  | 0.000537         | 0.11     | -7.127   | 0.0424      | 130.005 | 4              |\n    | 3   | Confident / Feeling Cool | Enjoy Your Life              | Lady Donli          | nigerian pop     | 2019         | 175135 | 14         | 0.894        | 0.798        | 0.611  | 0.000187         | 0.0964   | -4.961   | 0.113       | 111.087 | 4              |\n    | 4   | wanted you               | rare.                        | Odunsi (The Engine) | afropop          | 2018         | 152049 | 25         | 0.702        | 0.116        | 0.833  | 0.91             | 0.348    | -6.044   | 0.0447      | 105.115 | 4              |\n\n1. Get some information about the dataframe, calling `info()`:\n\n    ```python\n    df.info()\n    ```\n\n   The output looking like so:\n\n    ```output\n    <class 'pandas.core.frame.DataFrame'>\n    RangeIndex: 530 entries, 0 to 529\n    Data columns (total 16 columns):\n     #   Column            Non-Null Count  Dtype  \n    ---  ------            --------------  -----  \n     0   name              530 non-null    object \n     1   album             530 non-null    object \n     2   artist            530 non-null    object \n     3   artist_top_genre  530 non-null    object \n     4   release_date      530 non-null    int64  \n     5   length            530 non-null    int64  \n     6   popularity        530 non-null    int64  \n     7   danceability      530 non-null    float64\n     8   acousticness      530 non-null    float64\n     9   energy            530 non-null    float64\n     10  instrumentalness  530 non-null    float64\n     11  liveness          530 non-null    float64\n     12  loudness          530 non-null    float64\n     13  speechiness       530 non-null    float64\n     14  tempo             530 non-null    float64\n     15  time_signature    530 non-null    int64  \n    dtypes: float64(8), int64(4), object(4)\n    memory usage: 66.4+ KB\n    ```\n\n1. Double-check for null values, by calling `isnull()` and verifying the sum being 0:\n\n    ```python\n    df.isnull().sum()\n    ```\n\n    Looking good:\n\n    ```output\n    name                0\n    album               0\n    artist              0\n    artist_top_genre    0\n    release_date        0\n    length              0\n    popularity          0\n    danceability        0\n    acousticness        0\n    energy              0\n    instrumentalness    0\n    liveness            0\n    loudness            0\n    speechiness         0\n    tempo               0\n    time_signature      0\n    dtype: int64\n    ```\n\n1. Describe the data:\n\n    ```python\n    df.describe()\n    ```\n\n    |       | release_date | length      | popularity | danceability | acousticness | energy   | instrumentalness | liveness | loudness  | speechiness | tempo      | time_signature |\n    | ----- | ------------ | ----------- | ---------- | ------------ | ------------ | -------- | ---------------- | -------- | --------- | ----------- | ---------- | -------------- |\n    | count | 530          | 530         | 530        | 530          | 530          | 530      | 530              | 530      | 530       | 530         | 530        | 530            |\n    | mean  | 2015.390566  | 222298.1698 | 17.507547  | 0.741619     | 0.265412     | 0.760623 | 0.016305         | 0.147308 | -4.953011 | 0.130748    | 116.487864 | 3.986792       |\n    | std   | 3.131688     | 39696.82226 | 18.992212  | 0.117522     | 0.208342     | 0.148533 | 0.090321         | 0.123588 | 2.464186  | 0.092939    | 23.518601  | 0.333701       |\n    | min   | 1998         | 89488       | 0          | 0.255        | 0.000665     | 0.111    | 0                | 0.0283   | -19.362   | 0.0278      | 61.695     | 3              |\n    | 25%   | 2014         | 199305      | 0          | 0.681        | 0.089525     | 0.669    | 0                | 0.07565  | -6.29875  | 0.0591      | 102.96125  | 4              |\n    | 50%   | 2016         | 218509      | 13         | 0.761        | 0.2205       | 0.7845   | 0.000004         | 0.1035   | -4.5585   | 0.09795     | 112.7145   | 4              |\n    | 75%   | 2017         | 242098.5    | 31         | 0.8295       | 0.403        | 0.87575  | 0.000234         | 0.164    | -3.331    | 0.177       | 125.03925  | 4              |\n    | max   | 2020         | 511738      | 73         | 0.966        | 0.954        | 0.995    | 0.91             | 0.811    | 0.582     | 0.514       | 206.007    | 5              |\n\n> ðŸ¤” If we are working with clustering, an unsupervised method that does not require labeled data, why are we showing this data with labels? In the data exploration phase, they come in handy, but they are not necessary for the clustering algorithms to work. You could just as well remove the column headers and refer to the data by column number. \n\nLook at the general values of the data. Note that popularity can be '0', which show songs that have no ranking. Let's remove those shortly.\n\n1. Use a barplot to find out the most popular genres:\n\n    ```python\n    import seaborn as sns\n    \n    top = df['artist_top_genre'].value_counts()\n    plt.figure(figsize=(10,7))\n    sns.barplot(x=top[:5].index,y=top[:5].values)\n    plt.xticks(rotation=45)\n    plt.title('Top genres',color = 'blue')\n    ```\n\n    ![most popular](./images/popular.png)\n\nâœ… If you'd like to see more top values, change the top `[:5]` to a bigger value, or remove it to see all.\n\nNote, when the top genre is described as 'Missing', that means that Spotify did not classify it, so let's get rid of it.\n\n1. Get rid of missing data by filtering it out\n\n    ```python\n    df = df[df['artist_top_genre'] != 'Missing']\n    top = df['artist_top_genre'].value_counts()\n    plt.figure(figsize=(10,7))\n    sns.barplot(x=top.index,y=top.values)\n    plt.xticks(rotation=45)\n    plt.title('Top genres',color = 'blue')\n    ```\n\n    Now recheck the genres:\n\n    ![most popular](images/all-genres.png)\n\n1. By far, the top three genres dominate this dataset. Let's concentrate on `afro dancehall`, `afropop`, and `nigerian pop`, additionally filter the dataset to remove anything with a 0 popularity value (meaning it was not classified with a popularity in the dataset and can be considered noise for our purposes):\n\n    ```python\n    df = df[(df['artist_top_genre'] == 'afro dancehall') | (df['artist_top_genre'] == 'afropop') | (df['artist_top_genre'] == 'nigerian pop')]\n    df = df[(df['popularity'] > 0)]\n    top = df['artist_top_genre'].value_counts()\n    plt.figure(figsize=(10,7))\n    sns.barplot(x=top.index,y=top.values)\n    plt.xticks(rotation=45)\n    plt.title('Top genres',color = 'blue')\n    ```\n\n1. Do a quick test to see if the data correlates in any particularly strong way:\n\n    ```python\n    corrmat = df.corr(numeric_only=True)\n    f, ax = plt.subplots(figsize=(12, 9))\n    sns.heatmap(corrmat, vmax=.8, square=True)\n    ```\n\n    ![correlations](images/correlation.png)\n\n    The only strong correlation is between `energy` and `loudness`, which is not too surprising, given that loud music is usually pretty energetic. Otherwise, the correlations are relatively weak. It will be interesting to see what a clustering algorithm can make of this data.\n\n    > ðŸŽ“ Note that correlation does not imply causation! We have proof of correlation but no proof of causation. An [amusing web site](https://tylervigen.com/spurious-correlations) has some visuals that emphasize this point.\n\nIs there any convergence in this dataset around a song's perceived popularity and danceability? A FacetGrid shows that there are concentric circles that line up, regardless of genre. Could it be that Nigerian tastes converge at a certain level of danceability for this genre?  \n\nâœ… Try different datapoints (energy, loudness, speechiness) and more or different musical genres. What can you discover? Take a look at the `df.describe()` table to see the general spread of the data points.\n\n### Exercise - data distribution\n\nAre these three genres significantly different in the perception of their danceability, based on their popularity?\n\n1. Examine our top three genres data distribution for popularity and danceability along a given x and y axis.\n\n    ```python\n    sns.set_theme(style=\"ticks\")\n    \n    g = sns.jointplot(\n        data=df,\n        x=\"popularity\", y=\"danceability\", hue=\"artist_top_genre\",\n        kind=\"kde\",\n    )\n    ```\n\n    You can discover concentric circles around a general point of convergence, showing the distribution of points.\n\n    > ðŸŽ“ Note that this example uses a KDE (Kernel Density Estimate) graph that represents the data using a continuous probability density curve. This allows us to interpret data when working with multiple distributions.\n\n    In general, the three genres align loosely in terms of their popularity and danceability. Determining clusters in this loosely-aligned data will be a challenge:\n\n    ![distribution](images/distribution.png)\n\n1. Create a scatter plot:\n\n    ```python\n    sns.FacetGrid(df, hue=\"artist_top_genre\", height=5) \\\n       .map(plt.scatter, \"popularity\", \"danceability\") \\\n       .add_legend()\n    ```\n\n    A scatterplot of the same axes shows a similar pattern of convergence\n\n    ![Facetgrid](images/facetgrid.png)\n\nIn general, for clustering, you can use scatterplots to show clusters of data, so mastering this type of visualization is very useful. In the next lesson, we will take this filtered data and use k-means clustering to discover groups in this data that see to overlap in interesting ways.\n\n---\n\n## ðŸš€Challenge\n\nIn preparation for the next lesson, make a chart about the various clustering algorithms you might discover and use in a production environment. What kinds of problems is the clustering trying to address?\n\n## [Post-lecture quiz](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/28/)\n\n## Review & Self Study\n\nBefore you apply clustering algorithms, as we have learned, it's a good idea to understand the nature of your dataset. Read more on this topic [here](https://www.kdnuggets.com/2019/10/right-clustering-algorithm.html)\n\n[This helpful article](https://www.freecodecamp.org/news/8-clustering-algorithms-in-machine-learning-that-all-data-scientists-should-know/) walks you through the different ways that various clustering algorithms behave, given different data shapes.\n\n## Assignment\n\n[Research other visualizations for clustering](assignment.md)\n",
              "pdfUrl": "",
              "videoUrl": "",
              "orderIndex": 1
            }
          ],
          "quizzes": [],
          "subModules": []
        },
        {
          "id": 53,
          "title": "K Means",
          "orderIndex": 1,
          "lessons": [
            {
              "id": 54,
              "title": "K Means",
              "content": "# Try different clustering methods\n\n## Instructions\n\nIn this lesson you learned about K-Means clustering. Sometimes K-Means is not appropriate for your data. Create a notebook using data either from these lessons or from somewhere else (credit your source) and show a different clustering method NOT using K-Means. What did you learn? \n## Rubric\n\n| Criteria | Exemplary                                                       | Adequate                                                             | Needs Improvement            |\n| -------- | --------------------------------------------------------------- | -------------------------------------------------------------------- | ---------------------------- |\n|          | A notebook is presented with a well-documented clustering model | A notebook is presented without good documentation and/or incomplete | Incomplete work is submitted |\n",
              "pdfUrl": "",
              "videoUrl": "",
              "orderIndex": 0
            },
            {
              "id": 55,
              "title": "K Means",
              "content": "# K-Means clustering\n\n## [Pre-lecture quiz](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/29/)\n\nIn this lesson, you will learn how to create clusters using Scikit-learn and the Nigerian music dataset you imported earlier. We will cover the basics of K-Means for Clustering. Keep in mind that, as you learned in the earlier lesson, there are many ways to work with clusters and the method you use depends on your data. We will try K-Means as it's the most common clustering technique. Let's get started!\n\nTerms you will learn about:\n\n- Silhouette scoring\n- Elbow method\n- Inertia\n- Variance\n\n## Introduction\n\n[K-Means Clustering](https://wikipedia.org/wiki/K-means_clustering) is a method derived from the domain of signal processing. It is used to divide and partition groups of data into 'k' clusters using a series of observations. Each observation works to group a given datapoint closest to its nearest 'mean', or the center point of a cluster.\n\nThe clusters can be visualized as [Voronoi diagrams](https://wikipedia.org/wiki/Voronoi_diagram), which include a point (or 'seed') and its corresponding region. \n\n![voronoi diagram](images/voronoi.png)\n\n> infographic by [Jen Looper](https://twitter.com/jenlooper)\n\nThe K-Means clustering process [executes in a three-step process](https://scikit-learn.org/stable/modules/clustering.html#k-means):\n\n1. The algorithm selects k-number of center points by sampling from the dataset. After this, it loops:\n    1. It assigns each sample to the nearest centroid.\n    2. It creates new centroids by taking the mean value of all of the samples assigned to the previous centroids.\n    3. Then, it calculates the difference between the new and old centroids and repeats until the centroids are stabilized.\n\nOne drawback of using K-Means includes the fact that you will need to establish 'k', that is the number of centroids. Fortunately the  'elbow method' helps to estimate a good starting value for 'k'. You'll try it in a minute.\n\n## Prerequisite\n\nYou will work in this lesson's [_notebook.ipynb_](https://github.com/microsoft/ML-For-Beginners/blob/main/5-Clustering/2-K-Means/notebook.ipynb) file that includes the data import and preliminary cleaning you did in the last lesson.\n\n## Exercise - preparation\n\nStart by taking another look at the songs data.\n\n1. Create a boxplot, calling `boxplot()` for each column:\n\n    ```python\n    plt.figure(figsize=(20,20), dpi=200)\n    \n    plt.subplot(4,3,1)\n    sns.boxplot(x = 'popularity', data = df)\n    \n    plt.subplot(4,3,2)\n    sns.boxplot(x = 'acousticness', data = df)\n    \n    plt.subplot(4,3,3)\n    sns.boxplot(x = 'energy', data = df)\n    \n    plt.subplot(4,3,4)\n    sns.boxplot(x = 'instrumentalness', data = df)\n    \n    plt.subplot(4,3,5)\n    sns.boxplot(x = 'liveness', data = df)\n    \n    plt.subplot(4,3,6)\n    sns.boxplot(x = 'loudness', data = df)\n    \n    plt.subplot(4,3,7)\n    sns.boxplot(x = 'speechiness', data = df)\n    \n    plt.subplot(4,3,8)\n    sns.boxplot(x = 'tempo', data = df)\n    \n    plt.subplot(4,3,9)\n    sns.boxplot(x = 'time_signature', data = df)\n    \n    plt.subplot(4,3,10)\n    sns.boxplot(x = 'danceability', data = df)\n    \n    plt.subplot(4,3,11)\n    sns.boxplot(x = 'length', data = df)\n    \n    plt.subplot(4,3,12)\n    sns.boxplot(x = 'release_date', data = df)\n    ```\n\n    This data is a little noisy: by observing each column as a boxplot, you can see outliers.\n\n    ![outliers](images/boxplots.png)\n\nYou could go through the dataset and remove these outliers, but that would make the data pretty minimal.\n\n1. For now, choose which columns you will use for your clustering exercise. Pick ones with similar ranges and encode the `artist_top_genre` column as numeric data:\n\n    ```python\n    from sklearn.preprocessing import LabelEncoder\n    le = LabelEncoder()\n    \n    X = df.loc[:, ('artist_top_genre','popularity','danceability','acousticness','loudness','energy')]\n    \n    y = df['artist_top_genre']\n    \n    X['artist_top_genre'] = le.fit_transform(X['artist_top_genre'])\n    \n    y = le.transform(y)\n    ```\n\n1. Now you need to pick how many clusters to target. You know there are 3 song genres that we carved out of the dataset, so let's try 3:\n\n    ```python\n    from sklearn.cluster import KMeans\n    \n    nclusters = 3 \n    seed = 0\n    \n    km = KMeans(n_clusters=nclusters, random_state=seed)\n    km.fit(X)\n    \n    # Predict the cluster for each data point\n    \n    y_cluster_kmeans = km.predict(X)\n    y_cluster_kmeans\n    ```\n\nYou see an array printed out with predicted clusters (0, 1,or 2) for each row of the dataframe.\n\n1. Use this array to calculate a 'silhouette score':\n\n    ```python\n    from sklearn import metrics\n    score = metrics.silhouette_score(X, y_cluster_kmeans)\n    score\n    ```\n\n## Silhouette score\n\nLook for a silhouette score closer to 1. This score varies from -1 to 1, and if the score is 1, the cluster is dense and well-separated from other clusters. A value near 0 represents overlapping clusters with samples very close to the decision boundary of the neighboring clusters. [(Source)](https://dzone.com/articles/kmeans-silhouette-score-explained-with-python-exam)\n\nOur score is **.53**, so right in the middle. This indicates that our data is not particularly well-suited to this type of clustering, but let's continue.\n\n### Exercise - build a model\n\n1. Import `KMeans` and start the clustering process.\n\n    ```python\n    from sklearn.cluster import KMeans\n    wcss = []\n    \n    for i in range(1, 11):\n        kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42)\n        kmeans.fit(X)\n        wcss.append(kmeans.inertia_)\n    \n    ```\n\n    There are a few parts here that warrant explaining.\n\n    > ðŸŽ“ range: These are the iterations of the clustering process\n\n    > ðŸŽ“ random_state: \"Determines random number generation for centroid initialization.\" [Source](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans)\n\n    > ðŸŽ“ WCSS: \"within-cluster sums of squares\" measures the squared average distance of all the points within a cluster to the cluster centroid. [Source](https://medium.com/@ODSC/unsupervised-learning-evaluating-clusters-bd47eed175ce). \n\n    > ðŸŽ“ Inertia: K-Means algorithms attempt to choose centroids to minimize 'inertia', \"a measure of how internally coherent clusters are.\" [Source](https://scikit-learn.org/stable/modules/clustering.html). The value is appended to the wcss variable on each iteration.\n\n    > ðŸŽ“ k-means++: In [Scikit-learn](https://scikit-learn.org/stable/modules/clustering.html#k-means) you can use the 'k-means++' optimization, which \"initializes the centroids to be (generally) distant from each other, leading to probably better results than random initialization.\n\n### Elbow method\n\nPreviously, you surmised that, because you have targeted 3 song genres, you should choose 3 clusters. But is that the case?\n\n1. Use the 'elbow method' to make sure.\n\n    ```python\n    plt.figure(figsize=(10,5))\n    sns.lineplot(x=range(1, 11), y=wcss, marker='o', color='red')\n    plt.title('Elbow')\n    plt.xlabel('Number of clusters')\n    plt.ylabel('WCSS')\n    plt.show()\n    ```\n\n    Use the `wcss` variable that you built in the previous step to create a chart showing where the 'bend' in the elbow is, which indicates the optimum number of clusters. Maybe it **is** 3!\n\n    ![elbow method](images/elbow.png)\n\n## Exercise - display the clusters\n\n1. Try the process again, this time setting three clusters, and display the clusters as a scatterplot:\n\n    ```python\n    from sklearn.cluster import KMeans\n    kmeans = KMeans(n_clusters = 3)\n    kmeans.fit(X)\n    labels = kmeans.predict(X)\n    plt.scatter(df['popularity'],df['danceability'],c = labels)\n    plt.xlabel('popularity')\n    plt.ylabel('danceability')\n    plt.show()\n    ```\n\n1. Check the model's accuracy:\n\n    ```python\n    labels = kmeans.labels_\n    \n    correct_labels = sum(y == labels)\n    \n    print(\"Result: %d out of %d samples were correctly labeled.\" % (correct_labels, y.size))\n    \n    print('Accuracy score: {0:0.2f}'. format(correct_labels/float(y.size)))\n    ```\n\n    This model's accuracy is not very good, and the shape of the clusters gives you a hint why. \n\n    ![clusters](images/clusters.png)\n\n    This data is too imbalanced, too little correlated and there is too much variance between the column values to cluster well. In fact, the clusters that form are probably heavily influenced or skewed by the three genre categories we defined above. That was a learning process!\n\n    In Scikit-learn's documentation, you can see that a model like this one, with clusters not very well demarcated, has a 'variance' problem:\n\n    ![problem models](images/problems.png)\n    > Infographic from Scikit-learn\n\n## Variance\n\nVariance is defined as \"the average of the squared differences from the Mean\" [(Source)](https://www.mathsisfun.com/data/standard-deviation.html). In the context of this clustering problem, it refers to data that the numbers of our dataset tend to diverge a bit too much from the mean. \n\nâœ… This is a great moment to think about all the ways you could correct this issue. Tweak the data a bit more? Use different columns? Use a different algorithm? Hint: Try [scaling your data](https://www.mygreatlearning.com/blog/learning-data-science-with-k-means-clustering/) to normalize it and test other columns.\n\n> Try this '[variance calculator](https://www.calculatorsoup.com/calculators/statistics/variance-calculator.php)' to understand the concept a bit more.\n\n---\n\n## ðŸš€Challenge\n\nSpend some time with this notebook, tweaking parameters. Can you improve the accuracy of the model by cleaning  the data more (removing outliers, for example)? You can use weights to give more weight to given data samples. What else can you do to create better clusters?\n\nHint: Try to scale your data. There's commented code in the notebook that adds standard scaling to make the data columns resemble each other more closely in terms of range. You'll find that while the silhouette score goes down, the 'kink' in the elbow graph smooths out. This is because leaving the data unscaled allows data with less variance to carry more weight. Read a bit more on this problem [here](https://stats.stackexchange.com/questions/21222/are-mean-normalization-and-feature-scaling-needed-for-k-means-clustering/21226#21226).\n\n## [Post-lecture quiz](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/30/)\n\n## Review & Self Study\n\nTake a look at a K-Means Simulator [such as this one](https://user.ceng.metu.edu.tr/~akifakkus/courses/ceng574/k-means/). You can use this tool to visualize sample data points and determine its centroids. You can edit the data's randomness, numbers of clusters and numbers of centroids. Does this help you get an idea of how the data can be grouped?\n\nAlso, take a look at [this handout on K-Means](https://stanford.edu/~cpiech/cs221/handouts/kmeans.html) from Stanford.\n\n## Assignment\n\n[Try different clustering methods](assignment.md)\n",
              "pdfUrl": "",
              "videoUrl": "",
              "orderIndex": 1
            }
          ],
          "quizzes": [],
          "subModules": []
        }
      ]
    },
    {
      "id": 56,
      "title": "NLP",
      "orderIndex": 5,
      "lessons": [
        {
          "id": 57,
          "title": "NLP",
          "content": "# Getting started with natural language processing\n\nNatural language processing (NLP) is the ability of a computer program to understand human language as it is spoken and written -- referred to as natural language. It is a component of artificial intelligence (AI). NLP has existed for more than 50 years and has roots in the field of linguistics. The whole field is directed at helping machines understand and process the human language. This can then be used to perform tasks like spell check or machine translation. It has a variety of real-world applications in a number of fields, including medical research, search engines and business intelligence.\n\n## Regional topic: European languages and literature and romantic hotels of Europe â¤ï¸\n\nIn this section of the curriculum, you will be introduced to one of the most widespread uses of machine learning: natural language processing (NLP). Derived from computational linguistics, this category of artificial intelligence is the bridge between humans and machines via voice or textual communication.\n\nIn these lessons we'll learn the basics of NLP by building small conversational bots to learn how machine learning aids in making these conversations more and more 'smart'. You'll travel back in time, chatting with Elizabeth Bennett and Mr. Darcy from Jane Austen's classic novel, **Pride and Prejudice**, published in 1813. Then, you'll further your knowledge by learning about sentiment analysis via hotel reviews in Europe.\n\n![Pride and Prejudice book and tea](images/p&p.jpg)\n> Photo by <a href=\"https://unsplash.com/@elaineh?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">Elaine Howlin</a> on <a href=\"https://unsplash.com/s/photos/pride-and-prejudice?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">Unsplash</a>\n  \n## Lessons\n\n1. [Introduction to natural language processing](1-Introduction-to-NLP/README.md)\n2. [Common NLP tasks and techniques](2-Tasks/README.md)\n3. [Translation and sentiment analysis with machine learning](3-Translation-Sentiment/README.md)\n4. [Preparing your data](4-Hotel-Reviews-1/README.md)\n5. [NLTK for Sentiment Analysis](5-Hotel-Reviews-2/README.md)\n\n## Credits \n\nThese natural language processing lessons were written with â˜• by [Stephen Howell](https://twitter.com/Howell_MSFT)\n",
          "pdfUrl": "",
          "videoUrl": "",
          "orderIndex": 0
        }
      ],
      "quizzes": [],
      "subModules": [
        {
          "id": 58,
          "title": "Introduction To NLP",
          "orderIndex": 0,
          "lessons": [
            {
              "id": 59,
              "title": "Introduction To NLP",
              "content": "# Search for a bot\n\n## Instructions\n\nBots are everywhere. Your assignment: find one and adopt it! You can find them on web sites, in banking applications, and on the phone, for example when you call financial services companies for advice or account information. Analyze the bot and see if you can confuse it. If you can confuse the bot, why do you think that happened? Write a short paper about your experience.\n\n## Rubric\n\n| Criteria | Exemplary                                                                                                     | Adequate                                     | Needs Improvement     |\n| -------- | ------------------------------------------------------------------------------------------------------------- | -------------------------------------------- | --------------------- |\n|          | A full  page paper is written, explaining the presumed bot architecture and outlining your experience with it | A paper is incomplete or not well researched | No paper is submitted |\n",
              "pdfUrl": "",
              "videoUrl": "",
              "orderIndex": 0
            },
            {
              "id": 60,
              "title": "Introduction To NLP",
              "content": "# Introduction to natural language processing\n\nThis lesson covers a brief history and important concepts of *natural language processing*, a subfield of *computational linguistics*.\n\n## [Pre-lecture quiz](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/31/)\n\n## Introduction\n\nNLP, as it is commonly known, is one of the best-known areas where machine learning has been applied and used in production software.\n\nâœ… Can you think of software that you use every day that probably has some NLP embedded? What about your word processing programs or mobile apps that you use regularly?\n\nYou will learn about:\n\n- **The idea of languages**. How languages developed and what the major areas of study have been.\n- **Definition and concepts**. You will also learn definitions and concepts about how computers process text, including parsing, grammar, and identifying nouns and verbs. There are some coding tasks in this lesson, and several important concepts are introduced that you will learn to code later on in the next lessons.\n\n## Computational linguistics\n\nComputational linguistics is an area of research and development over many decades that studies how computers can work with, and even understand, translate, and communicate with languages. Natural language processing (NLP) is a related field focused on how computers can process 'natural', or human, languages.\n\n### Example - phone dictation\n\nIf you have ever dictated to your phone instead of typing or asked a virtual assistant a question, your speech was converted into a text form and then processed or *parsed* from the language you spoke. The detected keywords were then processed into a format that the phone or assistant could understand and act on.\n\n![comprehension](images/comprehension.png)\n> Real linguistic comprehension is hard! Image by [Jen Looper](https://twitter.com/jenlooper)\n\n### How is this technology made possible?\n\nThis is possible because someone wrote a computer program to do this. A few decades ago, some science fiction writers predicted that people would mostly speak to their computers, and the computers would always understand exactly what they meant. Sadly, it turned out to be a harder problem that many imagined, and while it is a much better understood problem today, there are significant challenges in achieving 'perfect' natural language processing when it comes to understanding the meaning of a sentence. This is a particularly hard problem when it comes to understanding humour or detecting emotions such as sarcasm in a sentence.\n\nAt this point, you may be remembering school classes where the teacher covered the parts of grammar in a sentence. In some countries, students are taught grammar and linguistics as a dedicated subject, but in many, these topics are included as part of learning a language: either your first language in primary school (learning to read and write) and perhaps a second language in post-primary, or high school. Don't  worry if you are not an expert at differentiating nouns from verbs or adverbs from adjectives!\n\nIf you struggle with the difference between the *simple present* and *present progressive*, you are not alone. This is a challenging thing for many people, even native speakers of a language. The good news is that computers are really good at applying formal rules, and you will learn to write code that can *parse* a sentence as well as a human. The greater challenge you will examine later is understanding the *meaning*, and *sentiment*, of a sentence.\n\n## Prerequisites\n\nFor this lesson, the main prerequisite is being able to read and understand the language of this lesson. There are no math problems or equations to solve. While the original author wrote this lesson in English, it is also translated into other languages, so you could be reading a translation. There are examples where a number of different languages are used (to compare the different grammar rules of different languages). These are *not* translated, but the explanatory text is, so the meaning should be clear.\n\nFor the coding tasks, you will use Python and the examples are using Python 3.8.\n\nIn this section, you will need, and use:\n\n- **Python 3 comprehension**.  Programming language comprehension in Python 3, this lesson uses input, loops, file reading, arrays.\n- **Visual Studio Code + extension**. We will use Visual Studio Code and its Python extension. You can also use a Python IDE of your choice.\n- **TextBlob**. [TextBlob](https://github.com/sloria/TextBlob) is a simplified text processing library for Python. Follow the instructions on the TextBlob site to install it on your system (install the corpora as well, as shown below):\n\n   ```bash\n   pip install -U textblob\n   python -m textblob.download_corpora\n   ```\n\n> ðŸ’¡ Tip: You can run Python directly in VS Code environments. Check the [docs](https://code.visualstudio.com/docs/languages/python?WT.mc_id=academic-77952-leestott) for more information.\n\n## Talking to machines\n\nThe history of trying to make computers understand human language goes back decades, and one of the earliest scientists to consider natural language processing was *Alan Turing*.\n\n### The 'Turing test'\n\nWhen Turing was researching *artificial intelligence* in the 1950's, he considered if a conversational test could be given to a human and computer (via typed correspondence) where the human in the conversation was not sure if they were conversing with another human or a computer.\n\nIf, after a certain length of conversation, the human could not determine that the answers were from a computer or not, then could the computer be said to be *thinking*?\n\n### The inspiration - 'the imitation game'\n\nThe idea for this came from a party game called *The Imitation Game* where an interrogator is alone in a room and tasked with determining which of two people (in another room) are male and female respectively. The interrogator can send notes, and must try to think of questions where the written answers reveal the gender of the mystery person. Of course, the players in the other room are trying to trick the interrogator by answering questions in such as way as to mislead or confuse the interrogator, whilst also giving the appearance of answering honestly.\n\n### Developing Eliza\n\nIn the 1960's an MIT scientist called *Joseph Weizenbaum* developed [*Eliza*](https://wikipedia.org/wiki/ELIZA), a computer 'therapist' that would ask the human questions and give the appearance of understanding their answers. However, while Eliza could parse a sentence and identify certain grammatical constructs and keywords so as to give a reasonable answer, it could not be said to *understand* the sentence. If Eliza was presented with a sentence following the format \"**I am** <u>sad</u>\" it might rearrange and substitute words in the sentence to form the response \"How long have **you been** <u>sad</u>\". \n\nThis gave the impression that Eliza understood the statement and was asking a follow-on question, whereas in reality, it was changing the tense and adding some words. If Eliza could not identify a keyword that it had a response for, it would instead give a random response that could be applicable to many different statements. Eliza could be easily tricked, for instance if a user wrote \"**You are** a <u>bicycle</u>\" it might respond with \"How long have **I been** a <u>bicycle</u>?\", instead of a more reasoned response.\n\n[![Chatting with Eliza](https://img.youtube.com/vi/RMK9AphfLco/0.jpg)](https://youtu.be/RMK9AphfLco \"Chatting with Eliza\")\n\n> ðŸŽ¥ Click the image above for a video about original ELIZA program\n\n> Note: You can read the original description of [Eliza](https://cacm.acm.org/magazines/1966/1/13317-elizaa-computer-program-for-the-study-of-natural-language-communication-between-man-and-machine/abstract) published in 1966 if you have an ACM account. Alternately, read about Eliza on [wikipedia](https://wikipedia.org/wiki/ELIZA)\n\n## Exercise - coding a basic conversational bot\n\nA conversational bot, like Eliza, is a program that elicits user input and seems to understand and respond intelligently. Unlike Eliza, our bot will not have several rules giving it the appearance of having an intelligent conversation. Instead, our bot will have one ability only, to keep the conversation going with random responses that might work in almost any trivial conversation.\n\n### The plan\n\nYour steps when building a conversational bot:\n\n1. Print instructions advising the user how to interact with the bot\n2. Start a loop\n   1. Accept user input\n   2. If user has asked to exit, then exit\n   3. Process user input and determine response (in this case, the response is a random choice from a list of possible generic responses)\n   4. Print response\n3. loop back to step 2\n\n### Building the bot\n\nLet's create the bot next. We'll start by defining some phrases.\n\n1. Create this bot yourself in Python with the following random responses:\n\n    ```python\n    random_responses = [\"That is quite interesting, please tell me more.\",\n                        \"I see. Do go on.\",\n                        \"Why do you say that?\",\n                        \"Funny weather we've been having, isn't it?\",\n                        \"Let's change the subject.\",\n                        \"Did you catch the game last night?\"]\n    ```\n\n    Here is some sample output to guide you (user input is on the lines starting with `>`):\n\n    ```output\n    Hello, I am Marvin, the simple robot.\n    You can end this conversation at any time by typing 'bye'\n    After typing each answer, press 'enter'\n    How are you today?\n    > I am good thanks\n    That is quite interesting, please tell me more.\n    > today I went for a walk     \n    Did you catch the game last night?\n    > I did, but my team lost\n    Funny weather we've been having, isn't it?\n    > yes but I hope next week is better\n    Let's change the subject.\n    > ok, lets talk about music\n    Why do you say that?\n    > because I like music!\n    Why do you say that?\n    > bye\n    It was nice talking to you, goodbye!\n    ```\n\n    One possible solution to the task is [here](https://github.com/microsoft/ML-For-Beginners/blob/main/6-NLP/1-Introduction-to-NLP/solution/bot.py)\n\n    âœ… Stop and consider\n\n    1. Do you think the random responses would 'trick' someone into thinking that the bot actually understood them?\n    2. What features would the bot need to be more effective?\n    3. If a bot could really 'understand' the meaning of a sentence, would it need to 'remember' the meaning of previous sentences in a conversation too?\n\n---\n\n## ðŸš€Challenge\n\nChoose one of the \"stop and consider\" elements above and either try to implement them in code or write a solution on paper using pseudocode.\n\nIn the next lesson, you'll learn about a number of other approaches to parsing natural language and machine learning.\n\n## [Post-lecture quiz](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/32/)\n\n## Review & Self Study\n\nTake a look at the references below as further reading opportunities.\n\n### References\n\n1. Schubert, Lenhart, \"Computational Linguistics\", *The Stanford Encyclopedia of Philosophy* (Spring 2020 Edition), Edward N. Zalta (ed.), URL = <https://plato.stanford.edu/archives/spr2020/entries/computational-linguistics/>.\n2. Princeton University \"About WordNet.\" [WordNet](https://wordnet.princeton.edu/). Princeton University. 2010. \n\n## Assignment \n\n[Search for a bot](assignment.md)\n",
              "pdfUrl": "",
              "videoUrl": "",
              "orderIndex": 1
            }
          ],
          "quizzes": [],
          "subModules": []
        },
        {
          "id": 61,
          "title": "Tasks",
          "orderIndex": 1,
          "lessons": [
            {
              "id": 62,
              "title": "Tasks",
              "content": "# Make a Bot talk back\n\n## Instructions\n\nIn the past few lessons, you programmed a basic bot with whom to chat. This bot gives random answers until you say 'bye'. Can you make the answers a little less random, and trigger answers if you say specific things, like 'why' or 'how'? Think a bit how machine learning might make this type of work less manual as you extend your bot. You can use NLTK or TextBlob libraries to make your tasks easier.\n\n## Rubric\n\n| Criteria | Exemplary                                     | Adequate                                         | Needs Improvement       |\n| -------- | --------------------------------------------- | ------------------------------------------------ | ----------------------- |\n|          | A new bot.py file is presented and documented | A new bot file is presented but it contains bugs | A file is not presented |\n",
              "pdfUrl": "",
              "videoUrl": "",
              "orderIndex": 0
            },
            {
              "id": 63,
              "title": "Tasks",
              "content": "# Common natural language processing tasks and techniques\n\nFor most *natural language processing* tasks, the text to be processed, must be broken down, examined, and the results stored or cross referenced with rules and data sets. These tasks, allows the programmer to derive the _meaning_ or _intent_ or only the _frequency_ of terms and words in a text.\n\n## [Pre-lecture quiz](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/33/)\n\nLet's discover common techniques used in processing text. Combined with machine learning, these techniques help you to analyse large amounts of text efficiently. Before applying ML to these tasks, however, let's understand the problems encountered by an NLP specialist.\n\n## Tasks common to NLP\n\nThere are different ways to analyse a text you are working on. There are tasks you can perform and through these tasks you are able to gauge an understanding of the text and draw conclusions. You usually carry out these tasks in a sequence.\n\n### Tokenization\n\nProbably the first thing most NLP algorithms have to do is to split the text into tokens, or words. While this sounds simple, having to account for punctuation and different languages' word and sentence delimiters can make it tricky. You might have to use various methods to determine demarcations.\n\n![tokenization](images/tokenization.png)\n> Tokenizing a sentence from **Pride and Prejudice**. Infographic by [Jen Looper](https://twitter.com/jenlooper)\n\n### Embeddings\n\n[Word embeddings](https://wikipedia.org/wiki/Word_embedding) are a way to convert your text data numerically. Embeddings are done in a way so that words with a similar meaning or words used together cluster together.\n\n![word embeddings](images/embedding.png)\n> \"I have the highest respect for your nerves, they are my old friends.\" - Word embeddings for a sentence in **Pride and Prejudice**. Infographic by [Jen Looper](https://twitter.com/jenlooper)\n\nâœ… Try [this interesting tool](https://projector.tensorflow.org/) to experiment with word embeddings. Clicking on one word shows clusters of similar words: 'toy' clusters with 'disney', 'lego', 'playstation', and 'console'.\n\n### Parsing & Part-of-speech Tagging\n\nEvery word that has been tokenized can be tagged as a part of speech - a noun, verb, or adjective. The sentence `the quick red fox jumped over the lazy brown dog` might be POS tagged as fox = noun, jumped = verb.\n\n![parsing](images/parse.png)\n\n> Parsing a sentence from **Pride and Prejudice**. Infographic by [Jen Looper](https://twitter.com/jenlooper)\n\nParsing is recognizing what words are related to each other in a sentence - for instance `the quick red fox jumped` is an adjective-noun-verb sequence that is separate from the `lazy brown dog` sequence.  \n\n### Word and Phrase Frequencies\n\nA useful procedure when analyzing a large body of text is to build a dictionary of every word or phrase of interest and how often it appears. The phrase `the quick red fox jumped over the lazy brown dog` has a word frequency of 2 for the.\n\nLet's look at an example text where we count the frequency of words. Rudyard Kipling's poem The Winners contains the following verse:\n\n```output\nWhat the moral? Who rides may read.\nWhen the night is thick and the tracks are blind\nA friend at a pinch is a friend, indeed,\nBut a fool to wait for the laggard behind.\nDown to Gehenna or up to the Throne,\nHe travels the fastest who travels alone.\n```\n\nAs phrase frequencies can be case insensitive or case sensitive as required, the phrase `a friend` has a frequency of 2 and `the` has a frequency of 6, and `travels` is 2.\n\n### N-grams\n\nA text can be split into sequences of words of a set length, a single word (unigram), two words (bigrams), three words (trigrams) or any number of words (n-grams).\n\nFor instance `the quick red fox jumped over the lazy brown dog` with a n-gram score of 2 produces the following n-grams:\n\n1. the quick \n2. quick red \n3. red fox\n4. fox jumped \n5. jumped over \n6. over the \n7. the lazy \n8. lazy brown \n9. brown dog\n\nIt might be easier to visualize it as a sliding box over the sentence. Here it is for n-grams of 3 words, the n-gram is in bold in each sentence:\n\n1.   <u>**the quick red**</u> fox jumped over the lazy brown dog\n2.   the **<u>quick red fox</u>** jumped over the lazy brown dog\n3.   the quick **<u>red fox jumped</u>** over the lazy brown dog\n4.   the quick red **<u>fox jumped over</u>** the lazy brown dog\n5.   the quick red fox **<u>jumped over the</u>** lazy brown dog\n6.   the quick red fox jumped **<u>over the lazy</u>** brown dog\n7.   the quick red fox jumped over <u>**the lazy brown**</u> dog\n8.   the quick red fox jumped over the **<u>lazy brown dog</u>**\n\n![n-grams sliding window](images/n-grams.gif)\n\n> N-gram value of 3: Infographic by [Jen Looper](https://twitter.com/jenlooper)\n\n### Noun phrase Extraction\n\nIn most sentences, there is a noun that is the subject, or object of the sentence. In English, it is often identifiable as having 'a' or 'an' or 'the' preceding it. Identifying the subject or object of a sentence by 'extracting the noun phrase' is a common task in NLP when attempting to understand the meaning of a sentence.\n\nâœ… In the sentence \"I cannot fix on the hour, or the spot, or the look or the words, which laid the foundation. It is too long ago. I was in the middle before I knew that I had begun.\", can you identify the noun phrases?\n\nIn the sentence `the quick red fox jumped over the lazy brown dog` there are 2 noun phrases: **quick red fox** and **lazy brown dog**.\n\n### Sentiment analysis\n\nA sentence or text can be analysed for sentiment, or how *positive* or *negative* it is. Sentiment is measured in *polarity* and *objectivity/subjectivity*. Polarity is measured from -1.0 to 1.0 (negative to positive) and 0.0 to 1.0 (most objective to most subjective).\n\nâœ… Later you'll learn that there are different ways to determine sentiment using machine learning, but one way is to have a list of words and phrases that are categorized as positive or negative by a human expert and apply that model to text to calculate a polarity score. Can you see how this would work in some circumstances and less well in others?\n\n### Inflection\n\nInflection enables you to take a word and get the singular or plural of the word.\n\n### Lemmatization\n\nA *lemma* is the root or headword for a set of words, for instance *flew*, *flies*, *flying* have a lemma of the verb *fly*.\n\nThere are also useful databases available for the NLP researcher, notably:\n\n### WordNet\n\n[WordNet](https://wordnet.princeton.edu/) is a database of words, synonyms, antonyms and many other details for every word in many different languages. It is incredibly useful when attempting to build translations, spell checkers, or language tools of any type.\n\n## NLP Libraries\n\nLuckily, you don't have to build all of these techniques yourself, as there are excellent Python libraries available that make it much more accessible to developers who aren't specialized in natural language processing or machine learning. The next lessons include more examples of these, but here you will learn some useful examples to help you with the next task.\n\n### Exercise - using `TextBlob` library\n\nLet's use a library called TextBlob as it contains helpful APIs for tackling these types of tasks. TextBlob \"stands on the giant shoulders of [NLTK](https://nltk.org) and [pattern](https://github.com/clips/pattern), and plays nicely with both.\" It has a considerable amount of ML embedded in its API.\n\n> Note: A useful [Quick Start](https://textblob.readthedocs.io/en/dev/quickstart.html#quickstart) guide is available for TextBlob that is recommended for experienced Python developers \n\nWhen attempting to identify *noun phrases*, TextBlob offers several options of extractors to find noun phrases. \n\n1. Take a look at `ConllExtractor`.\n\n    ```python\n    from textblob import TextBlob\n    from textblob.np_extractors import ConllExtractor\n    # import and create a Conll extractor to use later \n    extractor = ConllExtractor()\n    \n    # later when you need a noun phrase extractor:\n    user_input = input(\"> \")\n    user_input_blob = TextBlob(user_input, np_extractor=extractor)  # note non-default extractor specified\n    np = user_input_blob.noun_phrases                                    \n    ```\n\n    > What's going on here? [ConllExtractor](https://textblob.readthedocs.io/en/dev/api_reference.html?highlight=Conll#textblob.en.np_extractors.ConllExtractor) is \"A noun phrase extractor that uses chunk parsing trained with the ConLL-2000 training corpus.\" ConLL-2000 refers to the 2000 Conference on Computational Natural Language Learning. Each year the conference hosted a workshop to tackle a thorny NLP problem, and in 2000 it was noun chunking. A model was trained on the Wall Street Journal, with \"sections 15-18 as training data (211727 tokens) and section 20 as test data (47377 tokens)\". You can look at the procedures used [here](https://www.clips.uantwerpen.be/conll2000/chunking/) and the [results](https://ifarm.nl/erikt/research/np-chunking.html).\n\n### Challenge - improving your bot with NLP\n\nIn the previous lesson you built a very simple Q&A bot. Now, you'll make Marvin a bit more sympathetic by analyzing your input for sentiment and printing out a response to match the sentiment. You'll also need to identify a `noun_phrase` and ask about it.\n\nYour steps when building a better conversational bot:\n\n1. Print instructions advising the user how to interact with the bot\n2. Start loop \n   1. Accept user input\n   2. If user has asked to exit, then exit\n   3. Process user input and determine appropriate sentiment response\n   4. If a noun phrase is detected in the sentiment, pluralize it and ask for more input on that topic\n   5. Print response\n3. loop back to step 2\n\nHere is the code snippet to determine sentiment using TextBlob. Note there are only four *gradients* of sentiment response (you could have more if you like):\n\n```python\nif user_input_blob.polarity <= -0.5:\n  response = \"Oh dear, that sounds bad. \"\nelif user_input_blob.polarity <= 0:\n  response = \"Hmm, that's not great. \"\nelif user_input_blob.polarity <= 0.5:\n  response = \"Well, that sounds positive. \"\nelif user_input_blob.polarity <= 1:\n  response = \"Wow, that sounds great. \"\n```\n\nHere is some sample output to guide you (user input is on the lines with starting with >):\n\n```output\nHello, I am Marvin, the friendly robot.\nYou can end this conversation at any time by typing 'bye'\nAfter typing each answer, press 'enter'\nHow are you today?\n> I am ok\nWell, that sounds positive. Can you tell me more?\n> I went for a walk and saw a lovely cat\nWell, that sounds positive. Can you tell me more about lovely cats?\n> cats are the best. But I also have a cool dog\nWow, that sounds great. Can you tell me more about cool dogs?\n> I have an old hounddog but he is sick\nHmm, that's not great. Can you tell me more about old hounddogs?\n> bye\nIt was nice talking to you, goodbye!\n```\n\nOne possible solution to the task is [here](https://github.com/microsoft/ML-For-Beginners/blob/main/6-NLP/2-Tasks/solution/bot.py)\n\nâœ… Knowledge Check\n\n1. Do you think the sympathetic responses would 'trick' someone into thinking that the bot actually understood them?\n2. Does identifying the noun phrase make the bot more 'believable'?\n3. Why would extracting a 'noun phrase' from a sentence a useful thing to do?\n\n---\n\nImplement the bot in the prior knowledge check and test it on a friend. Can it trick them? Can you make your bot more 'believable?'\n\n## ðŸš€Challenge\n\nTake a task in the prior knowledge check and try to implement it. Test the bot on a friend. Can it trick them? Can you make your bot more 'believable?'\n\n## [Post-lecture quiz](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/34/)\n\n## Review & Self Study\n\nIn the next few lessons you will learn more about sentiment analysis. Research this interesting technique in articles such as these on [KDNuggets](https://www.kdnuggets.com/tag/nlp)\n\n## Assignment \n\n[Make a bot talk back](assignment.md)\n",
              "pdfUrl": "",
              "videoUrl": "",
              "orderIndex": 1
            }
          ],
          "quizzes": [],
          "subModules": []
        },
        {
          "id": 64,
          "title": "Translation Sentiment",
          "orderIndex": 2,
          "lessons": [
            {
              "id": 65,
              "title": "Translation Sentiment",
              "content": "# Poetic license\n\n## Instructions\n\nIn [this notebook](https://www.kaggle.com/jenlooper/emily-dickinson-word-frequency) you can find over 500 Emily Dickinson poems previously analyzed for sentiment using Azure text analytics. Using this dataset, analyze it using the techniques described in the lesson. Does the suggested sentiment of a poem match the more sophistic Azure service's decision? Why or why not, in your opinion? Does anything surprise you?\n## Rubric\n\n| Criteria | Exemplary                                                                  | Adequate                                                | Needs Improvement        |\n| -------- | -------------------------------------------------------------------------- | ------------------------------------------------------- | ------------------------ |\n|          | A notebook is presented with a solid analysis of an author's sample output | The notebook is incomplete or does not perform analysis | No notebook is presented |\n",
              "pdfUrl": "",
              "videoUrl": "",
              "orderIndex": 0
            },
            {
              "id": 66,
              "title": "Translation Sentiment",
              "content": "# Translation and sentiment analysis with ML\n\nIn the previous lessons you learned how to build a basic bot using `TextBlob`, a library that embeds ML behind-the-scenes to perform basic NLP tasks such as noun phrase extraction. Another important challenge in computational linguistics is accurate _translation_ of a sentence from one spoken or written language to another.\n\n## [Pre-lecture quiz](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/35/)\n\nTranslation is a very hard problem compounded by the fact that there are thousands of languages and each can have very different grammar rules. One approach is to convert the formal grammar rules for one language, such as English, into a non-language dependent structure, and then translate it by converting back to another language. This approach means that you would take the following steps:\n\n1. **Identification**. Identify or tag the words in input language into nouns, verbs etc.\n2. **Create translation**. Produce a direct translation of each word in the target language format.\n\n### Example sentence, English to Irish\n\nIn 'English', the sentence _I feel happy_ is three words in the order:\n\n- **subject** (I)\n- **verb** (feel)\n- **adjective** (happy)\n\nHowever, in the 'Irish' language, the same sentence has a very different grammatical structure - emotions like \"*happy*\" or \"*sad*\" are expressed as being *upon* you.\n\nThe English phrase `I feel happy` in Irish would be `TÃ¡ athas orm`. A *literal* translation would be `Happy is upon me`.\n\nAn Irish speaker translating to English would say `I feel happy`, not `Happy is upon me`, because they understand the meaning of the sentence, even if the words and sentence structure are different.\n\nThe formal order for the sentence in Irish are:\n\n- **verb** (TÃ¡ or is)\n- **adjective** (athas, or happy)\n- **subject** (orm, or upon me)\n\n## Translation\n\nA naive translation program might translate words only, ignoring the sentence structure.\n\nâœ… If you've learned a second (or third or more) language as an adult, you might have started by thinking in your native language, translating a concept word by word in your head to the second language, and then speaking out your translation. This is similar to what naive translation computer programs are doing. It's important to get past this phase to attain fluency!\n\nNaive translation leads to bad (and sometimes hilarious) mistranslations: `I feel happy` translates literally to `Mise bhraitheann athas` in Irish. That means (literally) `me feel happy` and is not a valid Irish sentence. Even though English and Irish are languages spoken on two closely neighboring islands, they are very different languages with different grammar structures.\n\n> You can watch some videos about Irish linguistic traditions such as [this one](https://www.youtube.com/watch?v=mRIaLSdRMMs)\n\n### Machine learning approaches\n\nSo far, you've learned about the formal rules approach to natural language processing. Another approach is to ignore the meaning of the words, and _instead use machine learning to detect patterns_. This can work in translation if you have lots of text (a *corpus*) or texts (*corpora*) in both the origin and target languages.\n\nFor instance, consider the case of *Pride and Prejudice*, a well-known English novel written by Jane Austen in 1813. If you consult the book in English and a human translation of the book in *French*, you could detect phrases in one that are _idiomatically_ translated into the other. You'll do that in a minute.\n\nFor instance, when an English phrase such as `I have no money` is translated literally to French, it might become `Je n'ai pas de monnaie`. \"Monnaie\" is a tricky french 'false cognate', as 'money' and 'monnaie' are not synonymous. A better translation that a human might make would be `Je n'ai pas d'argent`, because it better conveys the meaning that you have no money (rather than 'loose change' which is the meaning of 'monnaie').\n\n![monnaie](images/monnaie.png)\n\n> Image by [Jen Looper](https://twitter.com/jenlooper)\n\nIf an ML model has enough human translations to build a model on, it can improve the accuracy of translations by identifying common patterns in texts that have been previously translated by expert human speakers of both languages.\n\n### Exercise - translation\n\nYou can use `TextBlob` to translate sentences. Try the famous first line of **Pride and Prejudice**:\n\n```python\nfrom textblob import TextBlob\n\nblob = TextBlob(\n    \"It is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a wife!\"\n)\nprint(blob.translate(to=\"fr\"))\n\n```\n\n`TextBlob` does a pretty good job at the translation: \"C'est une vÃ©ritÃ© universellement reconnue, qu'un homme cÃ©libataire en possession d'une bonne fortune doit avoir besoin d'une femme!\". \n\nIt can be argued that TextBlob's translation is far more exact, in fact, than the 1932 French translation of the book by V. Leconte and Ch. Pressoir:\n\n\"C'est une vÃ©ritÃ© universelle qu'un cÃ©libataire pourvu d'une belle fortune doit avoir envie de se marier, et, si peu que l'on sache de son sentiment Ã  cet egard, lorsqu'il arrive dans une nouvelle rÃ©sidence, cette idÃ©e est si bien fixÃ©e dans l'esprit de ses voisins qu'ils le considÃ¨rent sur-le-champ comme la propriÃ©tÃ© lÃ©gitime de l'une ou l'autre de leurs filles.\"\n\nIn this case, the translation informed by ML does a better job than the human translator who is unnecessarily putting words in the original author's mouth for 'clarity'.\n\n> What's going on here? and why is TextBlob so good at translation? Well, behind the scenes, it's using Google translate, a sophisticated AI able to parse millions of phrases to predict the best strings for the task at hand. There's nothing manual going on here and you need an internet connection to use `blob.translate`.\n\nâœ… Try some more sentences. Which is better, ML or human translation? In which cases?\n\n## Sentiment analysis\n\nAnother area where machine learning can work very well is sentiment analysis. A non-ML approach to sentiment is to identify words and phrases which are 'positive' and 'negative'. Then, given a new piece of text, calculate the total value of the positive, negative and neutral words to identify the overall sentiment. \n\nThis approach is easily tricked as you may have seen in the Marvin task - the sentence `Great, that was a wonderful waste of time, I'm glad we are lost on this dark road` is a sarcastic, negative sentiment sentence, but the simple algorithm detects 'great', 'wonderful', 'glad' as positive and 'waste', 'lost' and 'dark' as negative. The overall sentiment is swayed by these conflicting words.\n\nâœ… Stop a second and think about how we convey sarcasm as human speakers. Tone inflection plays a large role. Try to say the phrase \"Well, that film was awesome\" in different ways to discover how your voice conveys meaning.\n\n### ML approaches\n\nThe ML approach would be to manually gather negative and positive bodies of text - tweets, or movie reviews, or anything where the human has given a score *and* a written opinion. Then NLP techniques can be applied to opinions and scores, so that patterns emerge (e.g., positive movie reviews tend to have the phrase 'Oscar worthy' more than negative movie reviews, or positive restaurant reviews say 'gourmet' much more than 'disgusting').\n\n> âš–ï¸ **Example**: If you worked in a politician's office and there was some new law being debated, constituents might write to the office with emails supporting or emails against the particular new law. Let's say you are tasked with reading the emails and sorting them in 2 piles, *for* and *against*. If there were a lot of emails, you might be overwhelmed attempting to read them all. Wouldn't it be nice if a bot could read them all for you, understand them and tell you in which pile each email belonged? \n> \n> One way to achieve that is to use Machine Learning. You would train the model with a portion of the *against* emails and a portion of the *for* emails. The model would tend to associate phrases and words with the against side and the for side, *but it would not understand any of the content*, only that certain words and patterns were more likely to appear in an *against* or a *for* email. You could test it with some emails that you had not used to train the model, and see if it came to the same conclusion as you did. Then, once you were happy with the accuracy of the model, you could process future emails without having to read each one.\n\nâœ… Does this process sound like processes you have used in previous lessons?\n\n## Exercise - sentimental sentences\n\nSentiment is measured in with a *polarity* of -1 to 1, meaning -1 is the most negative sentiment, and 1 is the most positive. Sentiment is also measured with an 0 - 1 score for objectivity (0) and subjectivity (1).\n\nTake another look at Jane Austen's *Pride and Prejudice*. The text is available here at [Project Gutenberg](https://www.gutenberg.org/files/1342/1342-h/1342-h.htm). The sample below shows a short program which analyses the sentiment of first and last sentences from the book and display its sentiment polarity and subjectivity/objectivity score.\n\nYou should use the `TextBlob` library (described above) to determine `sentiment` (you do not have to write your own sentiment calculator) in the following task.\n\n```python\nfrom textblob import TextBlob\n\nquote1 = \"\"\"It is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a wife.\"\"\"\n\nquote2 = \"\"\"Darcy, as well as Elizabeth, really loved them; and they were both ever sensible of the warmest gratitude towards the persons who, by bringing her into Derbyshire, had been the means of uniting them.\"\"\"\n\nsentiment1 = TextBlob(quote1).sentiment\nsentiment2 = TextBlob(quote2).sentiment\n\nprint(quote1 + \" has a sentiment of \" + str(sentiment1))\nprint(quote2 + \" has a sentiment of \" + str(sentiment2))\n```\n\nYou see the following output:\n\n```output\nIt is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want # of a wife. has a sentiment of Sentiment(polarity=0.20952380952380953, subjectivity=0.27142857142857146)\n\nDarcy, as well as Elizabeth, really loved them; and they were\n     both ever sensible of the warmest gratitude towards the persons\n      who, by bringing her into Derbyshire, had been the means of\n      uniting them. has a sentiment of Sentiment(polarity=0.7, subjectivity=0.8)\n```\n\n## Challenge - check sentiment polarity\n\nYour task is to determine, using sentiment polarity, if *Pride and Prejudice* has more absolutely positive sentences than absolutely negative ones. For this task, you may assume that a polarity score of 1 or -1 is absolutely positive or negative respectively.\n\n**Steps:**\n\n1. Download a [copy of Pride and Prejudice](https://www.gutenberg.org/files/1342/1342-h/1342-h.htm) from Project Gutenberg as a .txt file. Remove the metadata at the start and end of the file, leaving only the original text\n2. Open the file in Python and extract the contents as a string\n3. Create a TextBlob using the book string\n4. Analyse each sentence in the book in a loop\n   1. If the polarity is 1 or -1 store the sentence in an array or list of positive or negative messages\n5. At the end, print out all the positive sentences and negative sentences (separately) and the number of each.\n\nHere is a sample [solution](https://github.com/microsoft/ML-For-Beginners/blob/main/6-NLP/3-Translation-Sentiment/solution/notebook.ipynb).\n\nâœ… Knowledge Check\n\n1. The sentiment is based on words used in the sentence, but does the code *understand* the words?\n2. Do you think the sentiment polarity is accurate, or in other words, do you *agree* with the scores?\n   1. In particular, do you agree or disagree with the absolute **positive** polarity of the following sentences?\n      * â€œWhat an excellent father you have, girls!â€ said she, when the door was shut.\n      * â€œYour examination of Mr. Darcy is over, I presume,â€ said Miss Bingley; â€œand pray what is the result?â€ â€œI am perfectly convinced by it that Mr. Darcy has no defect.\n      * How wonderfully these sort of things occur!\n      * I have the greatest dislike in the world to that sort of thing.\n      * Charlotte is an excellent manager, I dare say.\n      * â€œThis is delightful indeed!\n      * I am so happy!\n      * Your idea of the ponies is delightful.\n   2. The next 3 sentences were scored with an absolute positive sentiment, but on close reading, they are not positive sentences. Why did the sentiment analysis think they were positive sentences?\n      * Happy shall I be, when his stay at Netherfield is over!â€ â€œI wish I could say anything to comfort you,â€ replied Elizabeth; â€œbut it is wholly out of my power.\n      * If I could but see you as happy!\n      * Our distress, my dear Lizzy, is very great.\n   3. Do you agree or disagree with the absolute **negative** polarity of the following sentences?\n      - Everybody is disgusted with his pride.\n      - â€œI should like to know how he behaves among strangers.â€ â€œYou shall hear thenâ€”but prepare yourself for something very dreadful.\n      - The pause was to Elizabethâ€™s feelings dreadful.\n      - It would be dreadful!\n\nâœ… Any aficionado of Jane Austen will understand that she often uses her books to critique the more ridiculous aspects of English Regency society. Elizabeth Bennett, the main character in *Pride and Prejudice*, is a keen social observer (like the author) and her language is often heavily nuanced. Even Mr. Darcy (the love interest in the story) notes Elizabeth's playful and teasing use of language: \"I have had the pleasure of your acquaintance long enough to know that you find great enjoyment in occasionally professing opinions which in fact are not your own.\"\n\n---\n\n## ðŸš€Challenge\n\nCan you make Marvin even better by extracting other features from the user input?\n\n## [Post-lecture quiz](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/36/)\n\n## Review & Self Study\n\nThere are many ways to extract sentiment from text. Think of the business applications that might make use of this technique. Think about how it can go awry. Read more about sophisticated enterprise-ready systems that analyze sentiment such as [Azure Text Analysis](https://docs.microsoft.com/azure/cognitive-services/Text-Analytics/how-tos/text-analytics-how-to-sentiment-analysis?tabs=version-3-1?WT.mc_id=academic-77952-leestott). Test some of the Pride and Prejudice sentences above and see if it can detect nuance.\n\n## Assignment \n\n[Poetic license](assignment.md)\n",
              "pdfUrl": "",
              "videoUrl": "",
              "orderIndex": 1
            }
          ],
          "quizzes": [],
          "subModules": []
        },
        {
          "id": 67,
          "title": "Hotel Reviews 1",
          "orderIndex": 3,
          "lessons": [
            {
              "id": 68,
              "title": "Hotel Reviews 1",
              "content": "# NLTK\n\n## Instructions\n\nNLTK is a well-known library for use in computational linguistics and NLP. Take this opportunity to read through the '[NLTK book](https://www.nltk.org/book/)' and try out its exercises. In this ungraded assignment, you will get to know this library more deeply.\n\n",
              "pdfUrl": "",
              "videoUrl": "",
              "orderIndex": 0
            },
            {
              "id": 69,
              "title": "Hotel Reviews 1",
              "content": "# Sentiment analysis with hotel reviews - processing the data\n\nIn this section you will use the techniques in the previous lessons to do some exploratory data analysis of a large dataset. Once you have a good understanding of the usefulness of the various columns, you will learn: \n\n- how to remove the unnecessary columns\n- how to calculate some new data based on the existing columns\n- how to save the resulting dataset for use in the final challenge\n\n## [Pre-lecture quiz](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/37/)\n\n### Introduction\n\nSo far you've learned about how text data is quite unlike numerical types of data. If it's text that was written or spoken by a human, if can be analysed to find patterns and frequencies, sentiment and meaning. This lesson takes you into a real data set with a real challenge: **[515K Hotel Reviews Data in Europe](https://www.kaggle.com/jiashenliu/515k-hotel-reviews-data-in-europe)** and includes a [CC0: Public Domain license](https://creativecommons.org/publicdomain/zero/1.0/). It was scraped from Booking.com from public sources. The creator of the dataset was Jiashen Liu.\n\n### Preparation\n\nYou will need:\n\n* The ability to run .ipynb notebooks using Python 3\n* pandas\n* NLTK, [which you should install locally](https://www.nltk.org/install.html)\n* The data set which is available on Kaggle [515K Hotel Reviews Data in Europe](https://www.kaggle.com/jiashenliu/515k-hotel-reviews-data-in-europe). It is around 230 MB unzipped. Download it to the root `/data` folder associated with these NLP lessons.\n\n## Exploratory data analysis\n\nThis challenge assumes that you are building a hotel recommendation bot using sentiment analysis and guest reviews scores. The dataset you will be using includes reviews of 1493 different hotels in 6 cities. \n\nUsing Python, a dataset of hotel reviews, and NLTK's sentiment analysis you could find out:\n\n* What are the most frequently used words and phrases in reviews?\n* Do the official *tags* describing a hotel correlate with review scores (e.g. are the more negative reviews for a particular hotel for  *Family with young children* than by *Solo traveller*, perhaps indicating it is better for *Solo travellers*?)\n* Do the NLTK sentiment scores 'agree' with the hotel reviewer's numerical score?\n\n#### Dataset\n\nLet's explore the dataset that you've downloaded and saved locally. Open the file in an editor like VS Code or even Excel.\n\nThe headers in the dataset are as follows:\n\n*Hotel_Address, Additional_Number_of_Scoring, Review_Date, Average_Score, Hotel_Name, Reviewer_Nationality, Negative_Review, Review_Total_Negative_Word_Counts, Total_Number_of_Reviews, Positive_Review, Review_Total_Positive_Word_Counts, Total_Number_of_Reviews_Reviewer_Has_Given, Reviewer_Score, Tags, days_since_review, lat, lng*\n\nHere they are grouped in a way that might be easier to examine: \n##### Hotel columns\n\n* `Hotel_Name`, `Hotel_Address`, `lat` (latitude), `lng` (longitude)\n  * Using *lat* and *lng* you could plot a map with Python showing the hotel locations (perhaps color coded for negative and positive reviews)\n  * Hotel_Address is not obviously useful to us, and we'll probably replace that with a country for easier sorting & searching\n\n**Hotel Meta-review columns**\n\n* `Average_Score`\n  * According to the dataset creator, this column is the *Average Score of the hotel, calculated based on the latest comment in the last year*. This seems like an unusual way to calculate the score, but it is the data scraped so we may take it as face value for now. \n  \n  âœ… Based on the other columns in this data, can you think of another way to calculate the average score?\n\n* `Total_Number_of_Reviews`\n  * The total number of reviews this hotel has received - it is not clear (without writing some code) if this refers to the reviews in the dataset.\n* `Additional_Number_of_Scoring`\n  * This means a review score was given but no positive or negative review was written by the reviewer\n\n**Review columns**\n\n- `Reviewer_Score`\n  - This is a numerical value with at most 1 decimal place between the min and max values 2.5 and 10\n  - It is not explained why 2.5 is the lowest score possible\n- `Negative_Review`\n  - If a reviewer wrote nothing, this field will have \"**No Negative**\"\n  - Note that a reviewer may write a positive review in the Negative review column (e.g. \"there is nothing bad about this hotel\")\n- `Review_Total_Negative_Word_Counts`\n  - Higher negative word counts indicate a lower score (without checking the sentimentality)\n- `Positive_Review`\n  - If a reviewer wrote nothing, this field will have \"**No Positive**\"\n  - Note that a reviewer may write a negative review in the Positive review column (e.g. \"there is nothing good about this hotel at all\")\n- `Review_Total_Positive_Word_Counts`\n  - Higher positive word counts indicate a higher score (without checking the sentimentality)\n- `Review_Date` and `days_since_review`\n  - A freshness or staleness measure might be applied to a review (older reviews might not be as accurate as newer ones because hotel management changed, or renovations have been done, or a pool was added etc.)\n- `Tags`\n  - These are short descriptors that a reviewer may select to describe the type of guest they were (e.g. solo or family), the type of room they had, the length of stay and how the review was submitted. \n  - Unfortunately, using these tags is problematic, check the section below which discusses their usefulness\n\n**Reviewer columns**\n\n- `Total_Number_of_Reviews_Reviewer_Has_Given`\n  - This might be a factor in a recommendation model, for instance, if you could determine that more prolific reviewers with hundreds of reviews were more likely to be negative rather than positive. However, the reviewer of any particular review is not identified with a unique code, and therefore cannot be linked to a set of reviews. There are 30 reviewers with 100 or more reviews, but it's hard to see how this can aid the recommendation model.\n- `Reviewer_Nationality`\n  - Some people might think that certain nationalities are more likely to give a positive or negative review because of a national inclination. Be careful building such anecdotal views into your models. These are national (and sometimes racial) stereotypes, and each reviewer was an individual who wrote a review based on their experience. It may have been filtered through many lenses such as their previous hotel stays, the distance travelled, and their personal temperament. Thinking that their nationality was the reason for a review score is hard to justify.\n\n##### Examples\n\n| Average  Score | Total Number   Reviews | Reviewer   Score | Negative <br />Review                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | Positive   Review                 | Tags                                                                                      |\n| -------------- | ---------------------- | ---------------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------- | ----------------------------------------------------------------------------------------- |\n| 7.8            | 1945                   | 2.5              | This is  currently not a hotel but a construction site I was terrorized from early  morning and all day with unacceptable building noise while resting after a  long trip and working in the room People were working all day i e with  jackhammers in the adjacent rooms I asked for a room change but no silent  room was available To make things worse I was overcharged I checked out in  the evening since I had to leave very early flight and received an appropriate  bill A day later the hotel made another charge without my consent in excess  of booked price It's a terrible place Don't punish yourself by booking  here | Nothing  Terrible place Stay away | Business trip                                Couple Standard Double  Room Stayed 2 nights |\n\nAs you can see, this guest did not have a happy stay at this hotel. The hotel has a good average score of 7.8 and 1945 reviews, but this reviewer gave it 2.5 and wrote 115 words about how negative their stay was. If they wrote nothing at all in the Positive_Review column, you might surmise there was nothing positive, but alas they wrote 7 words of warning. If we just counted words instead of the meaning, or sentiment of the words, we might have a skewed view of the reviewer's intent. Strangely, their score of 2.5 is confusing, because if that hotel stay was so bad, why give it any points at all? Investigating the dataset closely, you'll see that the lowest possible score is 2.5, not 0. The highest possible score is 10.\n\n##### Tags\n\nAs mentioned above, at first glance, the idea to use `Tags` to categorize the data makes sense. Unfortunately these tags are not standardized, which means that in a given hotel, the options might be *Single room*, *Twin room*, and *Double room*, but in the next hotel, they are *Deluxe Single Room*, *Classic Queen Room*, and *Executive King Room*. These might be the same things, but there are so many variations that the choice becomes:\n\n1. Attempt to change all terms to a single standard, which is very difficult, because it is not clear what the conversion path would be in each case (e.g. *Classic single room* maps to *Single room* but *Superior Queen Room with Courtyard Garden or City View* is much harder to map)\n\n1. We can take an NLP approach and measure the frequency of certain terms like *Solo*, *Business Traveller*, or *Family with young kids* as they apply to each hotel, and factor that into the recommendation  \n\nTags are usually (but not always) a single field containing a list of 5 to 6 comma separated values aligning to *Type of trip*, *Type of guests*, *Type of room*, *Number of nights*, and *Type of device review was submitted on*. However, because some reviewers don't fill in each field (they might leave one blank), the values are not always in the same order.\n\nAs an example, take *Type of group*. There are 1025 unique possibilities in this field in the `Tags` column, and unfortunately only some of them refer to a group (some are the type of room etc.). If you filter only the ones that mention family, the results contain many *Family room* type results. If you include the term *with*, i.e. count the *Family with* values, the results are better, with over 80,000 of the 515,000 results containing the phrase \"Family with young children\" or \"Family with older children\".\n\nThis means the tags column is not completely useless to us, but it will take some work to make it useful.\n\n##### Average hotel score\n\nThere are a number of oddities or discrepancies with the data set that I can't figure out, but are illustrated here so you are aware of them when building your models. If you figure it out, please let us know in the discussion section!\n\nThe dataset has the following columns relating to the average score and number of reviews: \n\n1. Hotel_Name\n2. Additional_Number_of_Scoring\n3. Average_Score\n4. Total_Number_of_Reviews\n5. Reviewer_Score  \n\nThe single hotel with the most reviews in this dataset is *Britannia International Hotel Canary Wharf* with 4789 reviews out of 515,000. But if we look at the `Total_Number_of_Reviews` value for this hotel, it is 9086. You might surmise that there are many more scores without reviews, so perhaps we should add in the `Additional_Number_of_Scoring` column value. That value is 2682, and adding it to 4789 gets us 7,471 which is still 1615 short of the `Total_Number_of_Reviews`. \n\nIf you take the `Average_Score` columns, you might surmise it is the average of the reviews in the dataset, but the description from Kaggle is \"*Average Score of the hotel, calculated based on the latest comment in the last year*\". That doesn't seem that useful, but we can calculate our own average based on the reviews scores in the data set. Using the same hotel as an example, the average hotel score is given as 7.1 but the calculated score (average reviewer score *in* the dataset) is 6.8. This is close, but not the same value, and we can only guess that the scores given in the `Additional_Number_of_Scoring` reviews increased the average to 7.1. Unfortunately with no way to test or prove that assertion, it is difficult to use or trust `Average_Score`, `Additional_Number_of_Scoring` and `Total_Number_of_Reviews` when they are based on, or refer to, data we do not have.\n\nTo complicate things further, the hotel with the second highest number of reviews has a calculated average score of 8.12 and the dataset `Average_Score` is 8.1. Is this correct score a coincidence or is the first hotel a discrepancy? \n\nOn the possibility that these hotel might be an outlier, and that maybe most of the values tally up (but some do not for some reason) we will write a short program next to explore the values in the dataset and determine the correct usage (or non-usage) of the values.\n\n> ðŸš¨ A note of caution\n>\n> When working with this dataset you will write code that calculates something from the text without having to read or analyse the text yourself. This is the essence of NLP, interpreting meaning or sentiment without having to have a human do it. However, it is possible that you will read some of the negative reviews. I would urge you not to, because you don't have to. Some of them are silly, or irrelevant negative hotel reviews, such as  \"The weather wasn't great\", something beyond the control of the hotel, or indeed, anyone. But there is a dark side to some reviews too. Sometimes the negative reviews are racist, sexist, or ageist. This is unfortunate but to be expected in a dataset scraped off a public website. Some reviewers leave reviews that you would find distasteful, uncomfortable, or upsetting. Better to let the code measure the sentiment than read them yourself and be upset. That said, it is a minority that write such things, but they exist all the same. \n\n## Exercise -  Data exploration\n### Load the data\n\nThat's enough examining the data visually, now you'll write some code and get some answers! This section uses the pandas library. Your very first task is to ensure you can load and read the CSV data. The pandas library has a fast CSV loader, and the result is placed in a dataframe, as in previous lessons. The CSV we are loading has over half a million rows, but only 17 columns. Pandas gives you lots of powerful ways to interact with a dataframe, including the ability to perform operations on every row. \n\nFrom here on in this lesson, there will be code snippets and some explanations of the code and some discussion about what the results mean. Use the included _notebook.ipynb_ for your code.\n\nLet's start with loading the data file you be using:\n\n```python\n# Load the hotel reviews from CSV\nimport pandas as pd\nimport time\n# importing time so the start and end time can be used to calculate file loading time\nprint(\"Loading data file now, this could take a while depending on file size\")\nstart = time.time()\n# df is 'DataFrame' - make sure you downloaded the file to the data folder\ndf = pd.read_csv('../../data/Hotel_Reviews.csv')\nend = time.time()\nprint(\"Loading took \" + str(round(end - start, 2)) + \" seconds\")\n```\n\nNow that the data is loaded, we can perform some operations on it. Keep this code at the top of your program for the next part.\n\n## Explore the data\n\nIn this case, the data is already *clean*, that means that it is ready to work with, and does not have characters in other languages that might trip up algorithms expecting only English characters. \n\nâœ… You might have to work with data that required some initial processing to format it before applying NLP techniques, but not this time. If you had to, how would you handle non-English characters?\n\nTake a moment to ensure that once the data is loaded, you can explore it with code. It's very easy to want to focus on the `Negative_Review` and `Positive_Review` columns. They are filled with natural text for your NLP algorithms to process. But wait! Before you jump into the NLP and sentiment, you should follow the code below to ascertain if the values given in the dataset match the values you calculate with pandas.\n\n## Dataframe operations\n\nThe first task in this lesson is to check if the following assertions are correct by writing some code that examines the data frame (without changing it).\n\n> Like many programming tasks, there are several ways to complete this, but good advice is to do it in the simplest, easiest way you can, especially if it will be easier to understand when you come back to this code in the future. With dataframes, there is a comprehensive API that will often have a way to do what you want efficiently.\n\nTreat the following questions as coding tasks and attempt to answer them without looking at the solution. \n\n1. Print out the *shape* of the data frame you have just loaded (the shape is the number of rows and columns)\n2. Calculate the frequency count for reviewer nationalities:\n   1. How many distinct values are there for the column `Reviewer_Nationality` and what are they?\n   2. What reviewer nationality is the most common in the dataset (print country and number of reviews)?\n   3. What are the next top 10 most frequently found nationalities, and their frequency count?\n3. What was the most frequently reviewed hotel for each of the top 10 most reviewer nationalities?\n4. How many reviews are there per hotel (frequency count of hotel) in the dataset?\n5. While there is an `Average_Score` column for each hotel in the dataset, you can also calculate an average score (getting the average of all reviewer scores in the dataset for each hotel). Add a new column to your dataframe with the column header `Calc_Average_Score` that contains that calculated average. \n6. Do any hotels have the same (rounded to 1 decimal place) `Average_Score` and `Calc_Average_Score`?\n   1. Try writing a Python function that takes a Series (row) as an argument and compares the values, printing out a message when the values are not equal. Then use the `.apply()` method to process every row with the function.\n7. Calculate and print out how many rows have column `Negative_Review` values of \"No Negative\" \n8. Calculate and print out how many rows have column `Positive_Review` values of \"No Positive\"\n9. Calculate and print out how many rows have column `Positive_Review` values of \"No Positive\" **and** `Negative_Review` values of \"No Negative\"\n### Code answers\n\n1. Print out the *shape* of the data frame you have just loaded (the shape is the number of rows and columns)\n\n   ```python\n   print(\"The shape of the data (rows, cols) is \" + str(df.shape))\n   > The shape of the data (rows, cols) is (515738, 17)\n   ```\n\n2. Calculate the frequency count for reviewer nationalities:\n\n   1. How many distinct values are there for the column `Reviewer_Nationality` and what are they?\n   2. What reviewer nationality is the most common in the dataset (print country and number of reviews)?\n\n   ```python\n   # value_counts() creates a Series object that has index and values in this case, the country and the frequency they occur in reviewer nationality\n   nationality_freq = df[\"Reviewer_Nationality\"].value_counts()\n   print(\"There are \" + str(nationality_freq.size) + \" different nationalities\")\n   # print first and last rows of the Series. Change to nationality_freq.to_string() to print all of the data\n   print(nationality_freq) \n   \n   There are 227 different nationalities\n    United Kingdom               245246\n    United States of America      35437\n    Australia                     21686\n    Ireland                       14827\n    United Arab Emirates          10235\n                                  ...  \n    Comoros                           1\n    Palau                             1\n    Northern Mariana Islands          1\n    Cape Verde                        1\n    Guinea                            1\n   Name: Reviewer_Nationality, Length: 227, dtype: int64\n   ```\n\n   3. What are the next top 10 most frequently found nationalities, and their frequency count?\n\n      ```python\n      print(\"The highest frequency reviewer nationality is \" + str(nationality_freq.index[0]).strip() + \" with \" + str(nationality_freq[0]) + \" reviews.\")\n      # Notice there is a leading space on the values, strip() removes that for printing\n      # What is the top 10 most common nationalities and their frequencies?\n      print(\"The next 10 highest frequency reviewer nationalities are:\")\n      print(nationality_freq[1:11].to_string())\n      \n      The highest frequency reviewer nationality is United Kingdom with 245246 reviews.\n      The next 10 highest frequency reviewer nationalities are:\n       United States of America     35437\n       Australia                    21686\n       Ireland                      14827\n       United Arab Emirates         10235\n       Saudi Arabia                  8951\n       Netherlands                   8772\n       Switzerland                   8678\n       Germany                       7941\n       Canada                        7894\n       France                        7296\n      ```\n\n3. What was the most frequently reviewed hotel for each of the top 10 most reviewer nationalities?\n\n   ```python\n   # What was the most frequently reviewed hotel for the top 10 nationalities\n   # Normally with pandas you will avoid an explicit loop, but wanted to show creating a new dataframe using criteria (don't do this with large amounts of data because it could be very slow)\n   for nat in nationality_freq[:10].index:\n      # First, extract all the rows that match the criteria into a new dataframe\n      nat_df = df[df[\"Reviewer_Nationality\"] == nat]   \n      # Now get the hotel freq\n      freq = nat_df[\"Hotel_Name\"].value_counts()\n      print(\"The most reviewed hotel for \" + str(nat).strip() + \" was \" + str(freq.index[0]) + \" with \" + str(freq[0]) + \" reviews.\") \n      \n   The most reviewed hotel for United Kingdom was Britannia International Hotel Canary Wharf with 3833 reviews.\n   The most reviewed hotel for United States of America was Hotel Esther a with 423 reviews.\n   The most reviewed hotel for Australia was Park Plaza Westminster Bridge London with 167 reviews.\n   The most reviewed hotel for Ireland was Copthorne Tara Hotel London Kensington with 239 reviews.\n   The most reviewed hotel for United Arab Emirates was Millennium Hotel London Knightsbridge with 129 reviews.\n   The most reviewed hotel for Saudi Arabia was The Cumberland A Guoman Hotel with 142 reviews.\n   The most reviewed hotel for Netherlands was Jaz Amsterdam with 97 reviews.\n   The most reviewed hotel for Switzerland was Hotel Da Vinci with 97 reviews.\n   The most reviewed hotel for Germany was Hotel Da Vinci with 86 reviews.\n   The most reviewed hotel for Canada was St James Court A Taj Hotel London with 61 reviews.\n   ```\n\n4. How many reviews are there per hotel (frequency count of hotel) in the dataset?\n\n   ```python\n   # First create a new dataframe based on the old one, removing the uneeded columns\n   hotel_freq_df = df.drop([\"Hotel_Address\", \"Additional_Number_of_Scoring\", \"Review_Date\", \"Average_Score\", \"Reviewer_Nationality\", \"Negative_Review\", \"Review_Total_Negative_Word_Counts\", \"Positive_Review\", \"Review_Total_Positive_Word_Counts\", \"Total_Number_of_Reviews_Reviewer_Has_Given\", \"Reviewer_Score\", \"Tags\", \"days_since_review\", \"lat\", \"lng\"], axis = 1)\n   \n   # Group the rows by Hotel_Name, count them and put the result in a new column Total_Reviews_Found\n   hotel_freq_df['Total_Reviews_Found'] = hotel_freq_df.groupby('Hotel_Name').transform('count')\n   \n   # Get rid of all the duplicated rows\n   hotel_freq_df = hotel_freq_df.drop_duplicates(subset = [\"Hotel_Name\"])\n   display(hotel_freq_df) \n   ```\n   |                 Hotel_Name                 | Total_Number_of_Reviews | Total_Reviews_Found |\n   | :----------------------------------------: | :---------------------: | :-----------------: |\n   | Britannia International Hotel Canary Wharf |          9086           |        4789         |\n   |    Park Plaza Westminster Bridge London    |          12158          |        4169         |\n   |   Copthorne Tara Hotel London Kensington   |          7105           |        3578         |\n   |                    ...                     |           ...           |         ...         |\n   |       Mercure Paris Porte d Orleans        |           110           |         10          |\n   |                Hotel Wagner                |           135           |         10          |\n   |            Hotel Gallitzinberg             |           173           |          8          |\n   \n   You may notice that the *counted in the dataset* results do not match the value in `Total_Number_of_Reviews`. It is unclear if this value in the dataset represented the total number of reviews the hotel had, but not all were scraped, or some other calculation. `Total_Number_of_Reviews` is not used in the model because of this unclarity.\n\n5. While there is an `Average_Score` column for each hotel in the dataset, you can also calculate an average score (getting the average of all reviewer scores in the dataset for each hotel). Add a new column to your dataframe with the column header `Calc_Average_Score` that contains that calculated average. Print out the columns `Hotel_Name`, `Average_Score`, and `Calc_Average_Score`.\n\n   ```python\n   # define a function that takes a row and performs some calculation with it\n   def get_difference_review_avg(row):\n     return row[\"Average_Score\"] - row[\"Calc_Average_Score\"]\n   \n   # 'mean' is mathematical word for 'average'\n   df['Calc_Average_Score'] = round(df.groupby('Hotel_Name').Reviewer_Score.transform('mean'), 1)\n   \n   # Add a new column with the difference between the two average scores\n   df[\"Average_Score_Difference\"] = df.apply(get_difference_review_avg, axis = 1)\n   \n   # Create a df without all the duplicates of Hotel_Name (so only 1 row per hotel)\n   review_scores_df = df.drop_duplicates(subset = [\"Hotel_Name\"])\n   \n   # Sort the dataframe to find the lowest and highest average score difference\n   review_scores_df = review_scores_df.sort_values(by=[\"Average_Score_Difference\"])\n   \n   display(review_scores_df[[\"Average_Score_Difference\", \"Average_Score\", \"Calc_Average_Score\", \"Hotel_Name\"]])\n   ```\n\n   You may also wonder about the `Average_Score` value and why it is sometimes different from the calculated average score. As we can't know why some of the values match, but others have a difference, it's safest in this case to use the review scores that we have to calculate the average ourselves. That said, the differences are usually very small, here are the hotels with the greatest deviation from the dataset average and the calculated average:\n\n   | Average_Score_Difference | Average_Score | Calc_Average_Score |                                  Hotel_Name |\n   | :----------------------: | :-----------: | :----------------: | ------------------------------------------: |\n   |           -0.8           |      7.7      |        8.5         |                  Best Western Hotel Astoria |\n   |           -0.7           |      8.8      |        9.5         | Hotel Stendhal Place Vend me Paris MGallery |\n   |           -0.7           |      7.5      |        8.2         |               Mercure Paris Porte d Orleans |\n   |           -0.7           |      7.9      |        8.6         |             Renaissance Paris Vendome Hotel |\n   |           -0.5           |      7.0      |        7.5         |                         Hotel Royal Elys es |\n   |           ...            |      ...      |        ...         |                                         ... |\n   |           0.7            |      7.5      |        6.8         |     Mercure Paris Op ra Faubourg Montmartre |\n   |           0.8            |      7.1      |        6.3         |      Holiday Inn Paris Montparnasse Pasteur |\n   |           0.9            |      6.8      |        5.9         |                               Villa Eugenie |\n   |           0.9            |      8.6      |        7.7         |   MARQUIS Faubourg St Honor Relais Ch teaux |\n   |           1.3            |      7.2      |        5.9         |                          Kube Hotel Ice Bar |\n\n   With only 1 hotel having a difference of score greater than 1, it means we can probably ignore the difference and use the calculated average score.\n\n6. Calculate and print out how many rows have column `Negative_Review` values of \"No Negative\" \n\n7. Calculate and print out how many rows have column `Positive_Review` values of \"No Positive\"\n\n8. Calculate and print out how many rows have column `Positive_Review` values of \"No Positive\" **and** `Negative_Review` values of \"No Negative\"\n\n   ```python\n   # with lambdas:\n   start = time.time()\n   no_negative_reviews = df.apply(lambda x: True if x['Negative_Review'] == \"No Negative\" else False , axis=1)\n   print(\"Number of No Negative reviews: \" + str(len(no_negative_reviews[no_negative_reviews == True].index)))\n   \n   no_positive_reviews = df.apply(lambda x: True if x['Positive_Review'] == \"No Positive\" else False , axis=1)\n   print(\"Number of No Positive reviews: \" + str(len(no_positive_reviews[no_positive_reviews == True].index)))\n   \n   both_no_reviews = df.apply(lambda x: True if x['Negative_Review'] == \"No Negative\" and x['Positive_Review'] == \"No Positive\" else False , axis=1)\n   print(\"Number of both No Negative and No Positive reviews: \" + str(len(both_no_reviews[both_no_reviews == True].index)))\n   end = time.time()\n   print(\"Lambdas took \" + str(round(end - start, 2)) + \" seconds\")\n   \n   Number of No Negative reviews: 127890\n   Number of No Positive reviews: 35946\n   Number of both No Negative and No Positive reviews: 127\n   Lambdas took 9.64 seconds\n   ```\n\n## Another way\n\nAnother way count items without Lambdas, and use sum to count the rows:\n\n   ```python\n   # without lambdas (using a mixture of notations to show you can use both)\n   start = time.time()\n   no_negative_reviews = sum(df.Negative_Review == \"No Negative\")\n   print(\"Number of No Negative reviews: \" + str(no_negative_reviews))\n   \n   no_positive_reviews = sum(df[\"Positive_Review\"] == \"No Positive\")\n   print(\"Number of No Positive reviews: \" + str(no_positive_reviews))\n   \n   both_no_reviews = sum((df.Negative_Review == \"No Negative\") & (df.Positive_Review == \"No Positive\"))\n   print(\"Number of both No Negative and No Positive reviews: \" + str(both_no_reviews))\n   \n   end = time.time()\n   print(\"Sum took \" + str(round(end - start, 2)) + \" seconds\")\n   \n   Number of No Negative reviews: 127890\n   Number of No Positive reviews: 35946\n   Number of both No Negative and No Positive reviews: 127\n   Sum took 0.19 seconds\n   ```\n\n   You may have noticed that there are 127 rows that have both \"No Negative\" and \"No Positive\" values for the columns `Negative_Review` and `Positive_Review` respectively. That means that the reviewer gave the hotel a numerical score, but declined to write either a positive or negative review. Luckily this is a small amount of rows (127 out of 515738, or 0.02%), so it probably won't skew our model or results in any particular direction, but you might not have expected a data set of reviews to have rows with no reviews, so it's worth exploring the data to discover rows like this.\n\nNow that you have explored the dataset, in the next lesson you will filter the data and add some sentiment analysis.\n\n---\n## ðŸš€Challenge\n\nThis lesson demonstrates, as we saw in previous lessons, how critically important it is to understand your data and its foibles before performing operations on it. Text-based data, in particular, bears careful scrutiny. Dig through various text-heavy datasets and see if you can discover areas that could introduce bias or skewed sentiment into a model. \n\n## [Post-lecture quiz](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/38/)\n\n## Review & Self Study\n\nTake [this Learning Path on NLP](https://docs.microsoft.com/learn/paths/explore-natural-language-processing/?WT.mc_id=academic-77952-leestott) to discover tools to try when building speech and text-heavy models.\n\n## Assignment \n\n[NLTK](assignment.md)\n",
              "pdfUrl": "",
              "videoUrl": "",
              "orderIndex": 1
            }
          ],
          "quizzes": [],
          "subModules": []
        },
        {
          "id": 70,
          "title": "Hotel Reviews 2",
          "orderIndex": 4,
          "lessons": [
            {
              "id": 71,
              "title": "Hotel Reviews 2",
              "content": "# Try a different dataset\n\n## Instructions\n\nNow that you have learned about using NLTK to assign sentiment to text, try a different dataset. You will probably need to do some data processing around it, so create a notebook and document your thought process. What do you discover?\n\n## Rubric\n\n| Criteria | Exemplary                                                                                                         | Adequate                                  | Needs Improvement      |\n| -------- | ----------------------------------------------------------------------------------------------------------------- | ----------------------------------------- | ---------------------- |\n|          | A complete notebook and dataset are presented with well-documented cells explaining how the sentiment is assigned | The notebook is missing good explanations | The notebook is flawed |\n",
              "pdfUrl": "",
              "videoUrl": "",
              "orderIndex": 0
            },
            {
              "id": 72,
              "title": "Hotel Reviews 2",
              "content": "# Sentiment analysis with hotel reviews\n\nNow that you have explored the dataset in detail, it's time to filter the columns and then use NLP techniques on the dataset to gain new insights about the hotels.\n## [Pre-lecture quiz](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/39/)\n\n### Filtering & Sentiment Analysis Operations\n\nAs you've probably noticed, the dataset has a few issues. Some columns are filled with useless information, others seem incorrect. If they are correct, it's unclear how they were calculated, and answers cannot be independently verified by your own calculations.\n\n## Exercise: a bit more data processing\n\nClean the data just a bit more. Add columns that will be useful later, change the values in other columns, and drop certain columns completely.\n\n1. Initial column processing\n\n   1. Drop `lat` and `lng`\n\n   2. Replace `Hotel_Address` values with the following values (if the address contains the same of the city and the country, change it to just the city and the country).\n\n      These are the only cities and countries in the dataset:\n\n      Amsterdam, Netherlands\n\n      Barcelona, Spain\n\n      London, United Kingdom\n\n      Milan, Italy\n\n      Paris, France\n\n      Vienna, Austria \n\n      ```python\n      def replace_address(row):\n          if \"Netherlands\" in row[\"Hotel_Address\"]:\n              return \"Amsterdam, Netherlands\"\n          elif \"Barcelona\" in row[\"Hotel_Address\"]:\n              return \"Barcelona, Spain\"\n          elif \"United Kingdom\" in row[\"Hotel_Address\"]:\n              return \"London, United Kingdom\"\n          elif \"Milan\" in row[\"Hotel_Address\"]:        \n              return \"Milan, Italy\"\n          elif \"France\" in row[\"Hotel_Address\"]:\n              return \"Paris, France\"\n          elif \"Vienna\" in row[\"Hotel_Address\"]:\n              return \"Vienna, Austria\" \n      \n      # Replace all the addresses with a shortened, more useful form\n      df[\"Hotel_Address\"] = df.apply(replace_address, axis = 1)\n      # The sum of the value_counts() should add up to the total number of reviews\n      print(df[\"Hotel_Address\"].value_counts())\n      ```\n\n      Now you can query country level data:\n\n      ```python\n      display(df.groupby(\"Hotel_Address\").agg({\"Hotel_Name\": \"nunique\"}))\n      ```\n\n      | Hotel_Address          | Hotel_Name |\n      | :--------------------- | :--------: |\n      | Amsterdam, Netherlands |    105     |\n      | Barcelona, Spain       |    211     |\n      | London, United Kingdom |    400     |\n      | Milan, Italy           |    162     |\n      | Paris, France          |    458     |\n      | Vienna, Austria        |    158     |\n\n2. Process Hotel Meta-review columns\n\n  1. Drop `Additional_Number_of_Scoring`\n\n  1. Replace `Total_Number_of_Reviews` with the total number of reviews for that hotel that are actually in the dataset \n\n  1. Replace `Average_Score` with our own calculated score\n\n  ```python\n  # Drop `Additional_Number_of_Scoring`\n  df.drop([\"Additional_Number_of_Scoring\"], axis = 1, inplace=True)\n  # Replace `Total_Number_of_Reviews` and `Average_Score` with our own calculated values\n  df.Total_Number_of_Reviews = df.groupby('Hotel_Name').transform('count')\n  df.Average_Score = round(df.groupby('Hotel_Name').Reviewer_Score.transform('mean'), 1)\n  ```\n\n3. Process review columns\n\n   1. Drop `Review_Total_Negative_Word_Counts`, `Review_Total_Positive_Word_Counts`, `Review_Date` and `days_since_review`\n\n   2. Keep `Reviewer_Score`, `Negative_Review`, and `Positive_Review` as they are,\n     \n   3. Keep `Tags` for now\n\n     - We'll be doing some additional filtering operations on the tags in the next section and then tags will be dropped\n\n4. Process reviewer columns\n\n  1. Drop `Total_Number_of_Reviews_Reviewer_Has_Given`\n  \n  2. Keep `Reviewer_Nationality`\n\n### Tag columns\n\nThe `Tag` column is problematic as it is a list (in text form) stored in the column. Unfortunately the order and number of sub sections in this column are not always the same. It's hard for a human to identify the correct phrases to be interested in, because there are 515,000 rows, and 1427 hotels, and each has slightly different options a reviewer could choose. This is where NLP shines. You can scan the text and find the most common phrases, and count them.\n\nUnfortunately, we are not interested in single words, but multi-word phrases (e.g. *Business trip*). Running a multi-word frequency distribution algorithm on that much data (6762646 words) could take an extraordinary amount of time, but without looking at the data, it would seem that is a necessary expense. This is where exploratory data analysis comes in useful, because you've seen a sample of the tags such as `[' Business trip  ', ' Solo traveler ', ' Single Room ', ' Stayed 5 nights ', ' Submitted from  a mobile device ']` , you can begin to ask if it's possible to greatly reduce the processing you have to do. Luckily, it is - but first you need to follow a few steps to ascertain the tags of interest.\n\n### Filtering tags\n\nRemember that the goal of the dataset is to add sentiment and columns that will help you choose the best hotel (for yourself or maybe a client tasking you to make a hotel recommendation bot). You need to ask yourself if the tags are useful or not in the final dataset. Here is one interpretation (if you needed the dataset for other reasons different tags might stay in/out of the selection):\n\n1. The type of trip is relevant, and that should stay\n2. The type of guest group is important, and that should stay\n3. The type of room, suite, or studio that the guest stayed in is irrelevant (all hotels have basically the same rooms)\n4. The device the review was submitted on is irrelevant\n5. The number of nights reviewer stayed for *could* be relevant if you attributed longer stays with them liking the hotel more, but it's a stretch, and probably irrelevant\n\nIn summary, **keep 2 kinds of tags and remove the others**.\n\nFirst, you don't want to count the tags until they are in a better format, so that means removing the square brackets and quotes. You can do this several ways, but you want the fastest as it could take a long time to process a lot of data. Luckily, pandas has an easy way to do each of these steps.\n\n```Python\n# Remove opening and closing brackets\ndf.Tags = df.Tags.str.strip(\"[']\")\n# remove all quotes too\ndf.Tags = df.Tags.str.replace(\" ', '\", \",\", regex = False)\n```\n\nEach tag becomes something like: `Business trip, Solo traveler, Single Room, Stayed 5 nights, Submitted from a mobile device`. \n\nNext we find a problem. Some reviews, or rows, have 5 columns, some 3, some 6. This is a result of how the dataset was created, and hard to fix. You want to get a frequency count of each phrase, but they are in different order in each review, so the count might be off, and a hotel might not get a tag assigned to it that it deserved.\n\nInstead you will use the different order to our advantage, because each tag is multi-word but also separated by a comma! The simplest way to do this is to create 6 temporary columns with each tag inserted in to the column corresponding to its order in the tag. You can then merge the 6 columns into one big column and run the `value_counts()` method on the resulting column. Printing that out, you'll see there was 2428 unique tags. Here is a small sample:\n\n| Tag                            | Count  |\n| ------------------------------ | ------ |\n| Leisure trip                   | 417778 |\n| Submitted from a mobile device | 307640 |\n| Couple                         | 252294 |\n| Stayed 1 night                 | 193645 |\n| Stayed 2 nights                | 133937 |\n| Solo traveler                  | 108545 |\n| Stayed 3 nights                | 95821  |\n| Business trip                  | 82939  |\n| Group                          | 65392  |\n| Family with young children     | 61015  |\n| Stayed 4 nights                | 47817  |\n| Double Room                    | 35207  |\n| Standard Double Room           | 32248  |\n| Superior Double Room           | 31393  |\n| Family with older children     | 26349  |\n| Deluxe Double Room             | 24823  |\n| Double or Twin Room            | 22393  |\n| Stayed 5 nights                | 20845  |\n| Standard Double or Twin Room   | 17483  |\n| Classic Double Room            | 16989  |\n| Superior Double or Twin Room   | 13570  |\n| 2 rooms                        | 12393  |\n\nSome of the common tags like `Submitted from a mobile device` are of no use to us, so it might be a smart thing to remove them before counting phrase occurrence, but it is such a fast operation you can leave them in and ignore them.\n\n### Removing the length of stay tags\n\nRemoving these tags is step 1, it reduces the total number of tags to be considered slightly. Note you do not remove them from the dataset, just choose to remove them from consideration as values to  count/keep in the reviews dataset.\n\n| Length of stay   | Count  |\n| ---------------- | ------ |\n| Stayed 1 night   | 193645 |\n| Stayed  2 nights | 133937 |\n| Stayed 3 nights  | 95821  |\n| Stayed  4 nights | 47817  |\n| Stayed 5 nights  | 20845  |\n| Stayed  6 nights | 9776   |\n| Stayed 7 nights  | 7399   |\n| Stayed  8 nights | 2502   |\n| Stayed 9 nights  | 1293   |\n| ...              | ...    |\n\nThere are a huge variety of rooms, suites, studios, apartments and so on. They all mean roughly the same thing and not relevant to you, so remove them from consideration.\n\n| Type of room                  | Count |\n| ----------------------------- | ----- |\n| Double Room                   | 35207 |\n| Standard  Double Room         | 32248 |\n| Superior Double Room          | 31393 |\n| Deluxe  Double Room           | 24823 |\n| Double or Twin Room           | 22393 |\n| Standard  Double or Twin Room | 17483 |\n| Classic Double Room           | 16989 |\n| Superior  Double or Twin Room | 13570 |\n\nFinally, and this is delightful (because it didn't take much processing at all), you will be left with the following *useful* tags:\n\n| Tag                                           | Count  |\n| --------------------------------------------- | ------ |\n| Leisure trip                                  | 417778 |\n| Couple                                        | 252294 |\n| Solo  traveler                                | 108545 |\n| Business trip                                 | 82939  |\n| Group (combined with Travellers with friends) | 67535  |\n| Family with young children                    | 61015  |\n| Family  with older children                   | 26349  |\n| With a  pet                                   | 1405   |\n\nYou could argue that `Travellers with friends` is the same as `Group` more or less, and that would be fair to combine the two as above. The code for identifying the correct tags is [the Tags notebook](https://github.com/microsoft/ML-For-Beginners/blob/main/6-NLP/5-Hotel-Reviews-2/solution/1-notebook.ipynb).\n\nThe final step is to create new columns for each of these tags. Then, for every review row, if the `Tag` column matches one of the new columns, add a 1, if not, add a 0. The end result will be a count of how many reviewers chose this hotel (in aggregate) for, say, business vs leisure, or to bring a pet to, and this is useful information when recommending a hotel.\n\n```python\n# Process the Tags into new columns\n# The file Hotel_Reviews_Tags.py, identifies the most important tags\n# Leisure trip, Couple, Solo traveler, Business trip, Group combined with Travelers with friends, \n# Family with young children, Family with older children, With a pet\ndf[\"Leisure_trip\"] = df.Tags.apply(lambda tag: 1 if \"Leisure trip\" in tag else 0)\ndf[\"Couple\"] = df.Tags.apply(lambda tag: 1 if \"Couple\" in tag else 0)\ndf[\"Solo_traveler\"] = df.Tags.apply(lambda tag: 1 if \"Solo traveler\" in tag else 0)\ndf[\"Business_trip\"] = df.Tags.apply(lambda tag: 1 if \"Business trip\" in tag else 0)\ndf[\"Group\"] = df.Tags.apply(lambda tag: 1 if \"Group\" in tag or \"Travelers with friends\" in tag else 0)\ndf[\"Family_with_young_children\"] = df.Tags.apply(lambda tag: 1 if \"Family with young children\" in tag else 0)\ndf[\"Family_with_older_children\"] = df.Tags.apply(lambda tag: 1 if \"Family with older children\" in tag else 0)\ndf[\"With_a_pet\"] = df.Tags.apply(lambda tag: 1 if \"With a pet\" in tag else 0)\n\n```\n\n### Save your file\n\nFinally, save the dataset as it is now with a new name.\n\n```python\ndf.drop([\"Review_Total_Negative_Word_Counts\", \"Review_Total_Positive_Word_Counts\", \"days_since_review\", \"Total_Number_of_Reviews_Reviewer_Has_Given\"], axis = 1, inplace=True)\n\n# Saving new data file with calculated columns\nprint(\"Saving results to Hotel_Reviews_Filtered.csv\")\ndf.to_csv(r'../data/Hotel_Reviews_Filtered.csv', index = False)\n```\n\n## Sentiment Analysis Operations\n\nIn this final section, you will apply sentiment analysis to the review columns and save the results in a dataset.\n\n## Exercise: load and save the filtered data\n\nNote that now you are loading the filtered dataset that was saved in the previous section, **not** the original dataset.\n\n```python\nimport time\nimport pandas as pd\nimport nltk as nltk\nfrom nltk.corpus import stopwords\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nnltk.download('vader_lexicon')\n\n# Load the filtered hotel reviews from CSV\ndf = pd.read_csv('../../data/Hotel_Reviews_Filtered.csv')\n\n# You code will be added here\n\n\n# Finally remember to save the hotel reviews with new NLP data added\nprint(\"Saving results to Hotel_Reviews_NLP.csv\")\ndf.to_csv(r'../data/Hotel_Reviews_NLP.csv', index = False)\n```\n\n### Removing stop words\n\nIf you were to run Sentiment Analysis on the Negative and Positive review columns, it could take a long time. Tested on a powerful test laptop with fast CPU,it took 12 - 14 minutes depending on which sentiment library was used. That's a (relatively) long time, so worth investigating if that can be speeded up. \n\nRemoving stop words, or common English words that do not change the sentiment of a sentence, is the first step. By removing them, the sentiment analysis should run faster, but not be less accurate (as the stop words do not affect sentiment, but they do slow down the analysis). \n\nThe longest negative review was 395 words, but after removing the stop words, it is 195 words.\n\nRemoving the stop words is also a fast operation, removing the stop words from 2 review columns over 515,000 rows took 3.3 seconds on the test device. It could take slightly more or less time for you depending on your device CPU speed, RAM, whether you have an SSD or not, and some other factors. The relative shortness of the operation means that if it improves the sentiment analysis time, then it is worth doing.\n\n```python\nfrom nltk.corpus import stopwords\n\n# Load the hotel reviews from CSV\ndf = pd.read_csv(\"../../data/Hotel_Reviews_Filtered.csv\")\n\n# Remove stop words - can be slow for a lot of text!\n# Ryan Han (ryanxjhan on Kaggle) has a great post measuring performance of different stop words removal approaches\n# https://www.kaggle.com/ryanxjhan/fast-stop-words-removal # using the approach that Ryan recommends\nstart = time.time()\ncache = set(stopwords.words(\"english\"))\ndef remove_stopwords(review):\n    text = \" \".join([word for word in review.split() if word not in cache])\n    return text\n\n# Remove the stop words from both columns\ndf.Negative_Review = df.Negative_Review.apply(remove_stopwords)   \ndf.Positive_Review = df.Positive_Review.apply(remove_stopwords)\n```\n\n### Performing sentiment analysis\n\nNow you should calculate the sentiment analysis for both negative and positive review columns, and store the result in 2 new columns. The test of the sentiment will be to compare it to the reviewer's score for the same review. For instance, if the sentiment thinks the negative review had a sentiment of 1 (extremely positive sentiment) and a positive review sentiment of 1, but the reviewer gave the hotel the lowest score possible, then either the review text doesn't match the score, or the sentiment analyser could not recognize the sentiment correctly. You should expect some sentiment scores to be completely wrong, and often that will be explainable, e.g. the review could be extremely sarcastic \"Of course I LOVED sleeping in a room with no heating\" and the sentiment analyser thinks that's positive sentiment, even though a human reading it would know it was sarcasm. \n\nNLTK supplies different sentiment analyzers to learn with, and you can substitute them and see if the sentiment is more or less accurate. The VADER sentiment analysis is used here.\n\n> Hutto, C.J. & Gilbert, E.E. (2014). VADER: A Parsimonious Rule-based Model for Sentiment Analysis of Social Media Text. Eighth International Conference on Weblogs and Social Media (ICWSM-14). Ann Arbor, MI, June 2014.\n\n```python\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n\n# Create the vader sentiment analyser (there are others in NLTK you can try too)\nvader_sentiment = SentimentIntensityAnalyzer()\n# Hutto, C.J. & Gilbert, E.E. (2014). VADER: A Parsimonious Rule-based Model for Sentiment Analysis of Social Media Text. Eighth International Conference on Weblogs and Social Media (ICWSM-14). Ann Arbor, MI, June 2014.\n\n# There are 3 possibilities of input for a review:\n# It could be \"No Negative\", in which case, return 0\n# It could be \"No Positive\", in which case, return 0\n# It could be a review, in which case calculate the sentiment\ndef calc_sentiment(review):    \n    if review == \"No Negative\" or review == \"No Positive\":\n        return 0\n    return vader_sentiment.polarity_scores(review)[\"compound\"]    \n```\n\nLater in your program when you are ready to calculate sentiment, you can apply it to each review as follows:\n\n```python\n# Add a negative sentiment and positive sentiment column\nprint(\"Calculating sentiment columns for both positive and negative reviews\")\nstart = time.time()\ndf[\"Negative_Sentiment\"] = df.Negative_Review.apply(calc_sentiment)\ndf[\"Positive_Sentiment\"] = df.Positive_Review.apply(calc_sentiment)\nend = time.time()\nprint(\"Calculating sentiment took \" + str(round(end - start, 2)) + \" seconds\")\n```\n\nThis takes approximately 120 seconds on my computer, but it will vary on each computer. If you want to print of the results and see if the sentiment matches the review:\n\n```python\ndf = df.sort_values(by=[\"Negative_Sentiment\"], ascending=True)\nprint(df[[\"Negative_Review\", \"Negative_Sentiment\"]])\ndf = df.sort_values(by=[\"Positive_Sentiment\"], ascending=True)\nprint(df[[\"Positive_Review\", \"Positive_Sentiment\"]])\n```\n\nThe very last thing to do with the file before using it in the challenge, is to save it! You should also consider reordering all your new columns so they are easy to work with (for a human, it's a cosmetic change).\n\n```python\n# Reorder the columns (This is cosmetic, but to make it easier to explore the data later)\ndf = df.reindex([\"Hotel_Name\", \"Hotel_Address\", \"Total_Number_of_Reviews\", \"Average_Score\", \"Reviewer_Score\", \"Negative_Sentiment\", \"Positive_Sentiment\", \"Reviewer_Nationality\", \"Leisure_trip\", \"Couple\", \"Solo_traveler\", \"Business_trip\", \"Group\", \"Family_with_young_children\", \"Family_with_older_children\", \"With_a_pet\", \"Negative_Review\", \"Positive_Review\"], axis=1)\n\nprint(\"Saving results to Hotel_Reviews_NLP.csv\")\ndf.to_csv(r\"../data/Hotel_Reviews_NLP.csv\", index = False)\n```\n\nYou should run the entire code for [the analysis notebook](https://github.com/microsoft/ML-For-Beginners/blob/main/6-NLP/5-Hotel-Reviews-2/solution/3-notebook.ipynb) (after you've run [your filtering notebook](https://github.com/microsoft/ML-For-Beginners/blob/main/6-NLP/5-Hotel-Reviews-2/solution/1-notebook.ipynb) to generate the Hotel_Reviews_Filtered.csv file).\n\nTo review, the steps are:\n\n1. Original dataset file **Hotel_Reviews.csv** is explored in the previous lesson with [the explorer notebook](https://github.com/microsoft/ML-For-Beginners/blob/main/6-NLP/4-Hotel-Reviews-1/solution/notebook.ipynb)\n2. Hotel_Reviews.csv is filtered by [the filtering notebook](https://github.com/microsoft/ML-For-Beginners/blob/main/6-NLP/5-Hotel-Reviews-2/solution/1-notebook.ipynb) resulting in **Hotel_Reviews_Filtered.csv**\n3. Hotel_Reviews_Filtered.csv is processed by [the sentiment analysis notebook](https://github.com/microsoft/ML-For-Beginners/blob/main/6-NLP/5-Hotel-Reviews-2/solution/3-notebook.ipynb) resulting in **Hotel_Reviews_NLP.csv**\n4. Use Hotel_Reviews_NLP.csv in the NLP Challenge below\n\n### Conclusion\n\nWhen you started, you had a dataset with columns and data but not all of it could be verified or used. You've explored the data, filtered out what you don't need, converted tags into something useful, calculated your own averages, added some sentiment columns and hopefully, learned some interesting things about processing natural text.\n\n## [Post-lecture quiz](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/40/)\n\n## Challenge\n\nNow that you have your dataset analyzed for sentiment, see if you can use strategies you've learned in this curriculum (clustering, perhaps?) to determine patterns around sentiment. \n\n## Review & Self Study\n\nTake [this Learn module](https://docs.microsoft.com/en-us/learn/modules/classify-user-feedback-with-the-text-analytics-api/?WT.mc_id=academic-77952-leestott) to learn more and use different tools to explore sentiment in text.\n## Assignment \n\n[Try a different dataset](assignment.md)\n",
              "pdfUrl": "",
              "videoUrl": "",
              "orderIndex": 1
            }
          ],
          "quizzes": [],
          "subModules": []
        }
      ]
    },
    {
      "id": 73,
      "title": "TimeSeries",
      "orderIndex": 6,
      "lessons": [
        {
          "id": 74,
          "title": "TimeSeries",
          "content": "# Introduction to time series forecasting\n\nWhat is time series forecasting? It's about predicting future events by analyzing trends of the past.\n\n## Regional topic: worldwide electricity usage âœ¨\n\nIn these two lessons, you will be introduced to time series forecasting, a somewhat lesser known area of machine learning that is nevertheless extremely valuable for industry and business applications, among other fields. While neural networks can be used to enhance the utility of these models, we will study them in the context of classical machine learning as models help predict future performance based on the past.\n\nOur regional focus is electrical usage in the world, an interesting dataset to learn about forecasting future power usage based on patterns of past load. You can see how this kind of forecasting can be extremely helpful in a business environment.\n\n![electric grid](images/electric-grid.jpg)\n\nPhoto by [Peddi Sai hrithik](https://unsplash.com/@shutter_log?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) of electrical towers on a road in Rajasthan on [Unsplash](https://unsplash.com/s/photos/electric-india?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)\n\n## Lessons\n\n1. [Introduction to time series forecasting](1-Introduction/README.md)\n2. [Building ARIMA time series models](2-ARIMA/README.md)\n3. [Building Support Vector Regressor for time series forcasting](3-SVR/README.md)\n\n## Credits\n\n\"Introduction to time series forecasting\" was written with âš¡ï¸ by [Francesca Lazzeri](https://twitter.com/frlazzeri) and [Jen Looper](https://twitter.com/jenlooper). The notebooks first appeared online in the [Azure \"Deep Learning For Time Series\" repo](https://github.com/Azure/DeepLearningForTimeSeriesForecasting) originally written by Francesca Lazzeri. The SVR lesson was written by [Anirban Mukherjee](https://github.com/AnirbanMukherjeeXD)\n",
          "pdfUrl": "",
          "videoUrl": "",
          "orderIndex": 0
        }
      ],
      "quizzes": [],
      "subModules": [
        {
          "id": 75,
          "title": "Introduction",
          "orderIndex": 0,
          "lessons": [
            {
              "id": 76,
              "title": "Introduction",
              "content": "# Visualize some more Time Series\n\n## Instructions\n\nYou've begun to learn about Time Series Forecasting by looking at the type of data that requires this special modeling. You've visualized some data around energy. Now, look around for some other data that would benefit from Time Series Forecasting. Find three examples (try [Kaggle](https://kaggle.com) and [Azure Open Datasets](https://azure.microsoft.com/en-us/services/open-datasets/catalog/?WT.mc_id=academic-77952-leestott)) and create a notebook to visualize them. Notate any special characteristics they have (seasonality, abrupt changes, or other trends) in the notebook.\n\n## Rubric\n\n| Criteria | Exemplary                                              | Adequate                                             | Needs Improvement                                                                         |\n| -------- | ------------------------------------------------------ | ---------------------------------------------------- | ----------------------------------------------------------------------------------------- |\n|          | Three datasets are plotted and explained in a notebook | Two datasets are plotted and explained in a notebook | Few datasets are plotted or explained in a notebook or the data presented is insufficient |\n",
              "pdfUrl": "",
              "videoUrl": "",
              "orderIndex": 0
            },
            {
              "id": 77,
              "title": "Introduction",
              "content": "# Introduction to time series forecasting\n\n![Summary of time series in a sketchnote](../../sketchnotes/ml-timeseries.png)\n\n> Sketchnote by [Tomomi Imura](https://www.twitter.com/girlie_mac)\n\nIn this lesson and the following one, you will learn a bit about time series forecasting, an interesting and valuable part of a ML scientist's repertoire that is a bit less known than other topics. Time series forecasting is a sort of 'crystal ball': based on past performance of a variable such as price, you can predict its future potential value.\n\n[![Introduction to time series forecasting](https://img.youtube.com/vi/cBojo1hsHiI/0.jpg)](https://youtu.be/cBojo1hsHiI \"Introduction to time series forecasting\")\n\n> ðŸŽ¥ Click the image above for a video about time series forecasting\n\n## [Pre-lecture quiz](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/41/)\n\nIt's a useful and interesting field with real value to business, given its direct application to problems of pricing, inventory, and supply chain issues. While deep learning techniques have started to be used to gain more insights to better predict future performance, time series forecasting remains a field greatly informed by classic ML techniques.\n\n> Penn State's useful time series curriculum can be found [here](https://online.stat.psu.edu/stat510/lesson/1)\n\n## Introduction\n\nSuppose you maintain an array of smart parking meters that provide data about how often they are used and for how long over time.\n\n> What if you could predict, based on the meter's past performance, its future value according to the laws of supply and demand?\n\nAccurately predicting when to act so as to achieve your goal is a challenge that could be tackled by time series forecasting. It wouldn't make folks happy to be charged more in busy times when they're looking for a parking spot, but it would be a sure way to generate revenue to clean the streets!\n\nLet's explore some of the types of time series algorithms and start a notebook to clean and prepare some data. The data you will analyze  is taken from the GEFCom2014 forecasting competition. It consists of 3 years of hourly electricity load and temperature values between 2012 and 2014. Given the historical patterns of electricity load and temperature, you can predict future values of electricity load.\n\nIn this example, you'll learn how to forecast one time step ahead, using historical load data only. Before starting, however, it's useful to understand what's going on behind the scenes.\n\n## Some definitions\n\nWhen encountering the term 'time series' you need to understand its use in several different contexts.\n\nðŸŽ“ **Time series**\n\nIn mathematics, \"a time series is a series of data points indexed (or listed or graphed) in time order. Most commonly, a time series is a sequence taken at successive equally spaced points in time.\" An example of a time series is the daily closing value of the [Dow Jones Industrial Average](https://wikipedia.org/wiki/Time_series). The use of time series plots and statistical modeling is frequently encountered in signal processing, weather forecasting, earthquake prediction, and other fields where events occur and data points can be plotted over time.\n\nðŸŽ“ **Time series analysis**\n\nTime series analysis, is the analysis of the above mentioned time series data. Time series data can take distinct forms, including 'interrupted time series' which detects patterns in a time series' evolution before and after an interrupting event. The type of analysis needed for the time series, depends on the nature of the data. Time series data itself can take the form of series of numbers or characters.\n\nThe analysis to be performed, uses a variety of methods, including frequency-domain and time-domain, linear and nonlinear, and more. [Learn more](https://www.itl.nist.gov/div898/handbook/pmc/section4/pmc4.htm) about the many ways to analyze this type of data.\n\nðŸŽ“ **Time series forecasting**\n\nTime series forecasting is the use of a model to predict future values based on patterns displayed by previously gathered data as it occurred in the past. While it is possible to use regression models to explore time series data, with time indices as x variables on a plot, such data is best analyzed using special types of models.\n\nTime series data is a list of ordered observations, unlike data that can be analyzed by linear regression.   The most common one is ARIMA, an acronym that stands for \"Autoregressive Integrated Moving Average\".\n\n[ARIMA models](https://online.stat.psu.edu/stat510/lesson/1/1.1) \"relate the present value of a series to past values and past prediction errors.\" They are most appropriate for analyzing time-domain data, where data is ordered over time.\n\n> There are several types of ARIMA models, which you can learn about [here](https://people.duke.edu/~rnau/411arim.htm) and which you will touch on in the next lesson.\n\nIn the next lesson, you will build an ARIMA model using [Univariate Time Series](https://itl.nist.gov/div898/handbook/pmc/section4/pmc44.htm), which focuses on one variable that changes its value over time. An example of this type of data is [this dataset](https://itl.nist.gov/div898/handbook/pmc/section4/pmc4411.htm) that records the monthly C02 concentration at the Mauna Loa Observatory:\n\n|  CO2   | YearMonth | Year  | Month |\n| :----: | :-------: | :---: | :---: |\n| 330.62 |  1975.04  | 1975  |   1   |\n| 331.40 |  1975.13  | 1975  |   2   |\n| 331.87 |  1975.21  | 1975  |   3   |\n| 333.18 |  1975.29  | 1975  |   4   |\n| 333.92 |  1975.38  | 1975  |   5   |\n| 333.43 |  1975.46  | 1975  |   6   |\n| 331.85 |  1975.54  | 1975  |   7   |\n| 330.01 |  1975.63  | 1975  |   8   |\n| 328.51 |  1975.71  | 1975  |   9   |\n| 328.41 |  1975.79  | 1975  |  10   |\n| 329.25 |  1975.88  | 1975  |  11   |\n| 330.97 |  1975.96  | 1975  |  12   |\n\nâœ… Identify the variable that changes over time in this dataset\n\n## Time Series data characteristics to consider\n\nWhen looking at time series data, you might notice that it has [certain characteristics](https://online.stat.psu.edu/stat510/lesson/1/1.1) that you need to take into account and mitigate to better understand its patterns. If you consider time series data as potentially providing a 'signal' that you want to analyze, these characteristics can be thought of as 'noise'. You often will need to reduce this 'noise' by offsetting some of these characteristics using some statistical techniques.\n\nHere are some concepts you should know to be able to work with time series:\n\nðŸŽ“ **Trends**\n\nTrends are defined as measurable increases and decreases over time. [Read more](https://machinelearningmastery.com/time-series-trends-in-python). In the context of time series, it's about how to use and, if necessary, remove trends from your time series.\n\nðŸŽ“ **[Seasonality](https://machinelearningmastery.com/time-series-seasonality-with-python/)**\n\nSeasonality is defined as periodic fluctuations, such as holiday rushes that might affect sales, for example. [Take a look](https://itl.nist.gov/div898/handbook/pmc/section4/pmc443.htm) at how different types of plots display seasonality in data.\n\nðŸŽ“ **Outliers**\n\nOutliers are far away from the standard data variance.\n\nðŸŽ“ **Long-run cycle**\n\nIndependent of seasonality, data might display a long-run cycle such as an economic down-turn that lasts longer than a year.\n\nðŸŽ“ **Constant variance**\n\nOver time, some data display constant fluctuations, such as energy usage per day and night.\n\nðŸŽ“ **Abrupt changes**\n\nThe data might display an abrupt change that might need further analysis. The abrupt shuttering of businesses due to COVID, for example, caused changes in data.\n\nâœ… Here is a [sample time series plot](https://www.kaggle.com/kashnitsky/topic-9-part-1-time-series-analysis-in-python) showing daily in-game currency spent over a few years. Can you identify any of the characteristics listed above in this data?\n\n![In-game currency spend](./images/currency.png)\n\n## Exercise - getting started with power usage data\n\nLet's get started creating a time series model to predict future power usage given past usage.\n\n> The data in this example is taken from the GEFCom2014 forecasting competition. It consists of 3 years of hourly electricity load and temperature values between 2012 and 2014.\n>\n> Tao Hong, Pierre Pinson, Shu Fan, Hamidreza Zareipour, Alberto Troccoli and Rob J. Hyndman, \"Probabilistic energy forecasting: Global Energy Forecasting Competition 2014 and beyond\", International Journal of Forecasting, vol.32, no.3, pp 896-913, July-September, 2016.\n\n1. In the `working` folder of this lesson, open the _notebook.ipynb_ file. Start by adding libraries that will help you load and visualize data\n\n    ```python\n    import os\n    import matplotlib.pyplot as plt\n    from common.utils import load_data\n    %matplotlib inline\n    ```\n\n    Note, you are using the files from the included `common` folder which set up your environment and handle downloading the data.\n\n2. Next, examine the data as a dataframe calling `load_data()` and `head()`:\n\n    ```python\n    data_dir = './data'\n    energy = load_data(data_dir)[['load']]\n    energy.head()\n    ```\n\n    You can see that there are two columns representing date and load:\n\n    |                     |  load  |\n    | :-----------------: | :----: |\n    | 2012-01-01 00:00:00 | 2698.0 |\n    | 2012-01-01 01:00:00 | 2558.0 |\n    | 2012-01-01 02:00:00 | 2444.0 |\n    | 2012-01-01 03:00:00 | 2402.0 |\n    | 2012-01-01 04:00:00 | 2403.0 |\n\n3. Now, plot the data calling `plot()`:\n\n    ```python\n    energy.plot(y='load', subplots=True, figsize=(15, 8), fontsize=12)\n    plt.xlabel('timestamp', fontsize=12)\n    plt.ylabel('load', fontsize=12)\n    plt.show()\n    ```\n\n    ![energy plot](images/energy-plot.png)\n\n4. Now, plot the first week of July 2014, by providing it as input to the `energy` in `[from date]: [to date]` pattern:\n\n    ```python\n    energy['2014-07-01':'2014-07-07'].plot(y='load', subplots=True, figsize=(15, 8), fontsize=12)\n    plt.xlabel('timestamp', fontsize=12)\n    plt.ylabel('load', fontsize=12)\n    plt.show()\n    ```\n\n    ![july](images/july-2014.png)\n\n    A beautiful plot! Take a look at these plots and see if you can determine any of the characteristics listed above. What can we surmise by visualizing the data?\n\nIn the next lesson, you will create an ARIMA model to create some forecasts.\n\n---\n\n## ðŸš€Challenge\n\nMake a list of all the industries and areas of inquiry you can think of that would benefit from time series forecasting. Can you think of an application of these techniques in the arts? In Econometrics? Ecology? Retail? Industry? Finance? Where else?\n\n## [Post-lecture quiz](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/42/)\n\n## Review & Self Study\n\nAlthough we won't cover them here, neural networks are sometimes used to enhance classic methods of time series forecasting. Read more about them [in this article](https://medium.com/microsoftazure/neural-networks-for-forecasting-financial-and-economic-time-series-6aca370ff412)\n\n## Assignment\n\n[Visualize some more time series](assignment.md)\n",
              "pdfUrl": "",
              "videoUrl": "",
              "orderIndex": 1
            }
          ],
          "quizzes": [],
          "subModules": []
        },
        {
          "id": 78,
          "title": "ARIMA",
          "orderIndex": 1,
          "lessons": [
            {
              "id": 79,
              "title": "ARIMA",
              "content": "# A new ARIMA model\n\n## Instructions\n\nNow that you have built an ARIMA model, build a new one with fresh data (try one of [these datasets from Duke](http://www2.stat.duke.edu/~mw/ts_data_sets.html). Annotate your work in a notebook, visualize the data and your model, and test its accuracy using MAPE.\n## Rubric\n\n| Criteria | Exemplary                                                                                                           | Adequate                                                 | Needs Improvement                   |\n| -------- | ------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------- | ----------------------------------- |\n|          | A notebook is presented with a new ARIMA model built, tested and explained with visualizations and accuracy stated. | The notebook presented is not annotated or contains bugs | An incomplete notebook is presented |\n",
              "pdfUrl": "",
              "videoUrl": "",
              "orderIndex": 0
            },
            {
              "id": 80,
              "title": "ARIMA",
              "content": "# Time series forecasting with ARIMA\n\nIn the previous lesson, you learned a bit about time series forecasting and loaded a dataset showing the fluctuations of electrical load over a time period.\n\n[![Introduction to ARIMA](https://img.youtube.com/vi/IUSk-YDau10/0.jpg)](https://youtu.be/IUSk-YDau10 \"Introduction to ARIMA\")\n\n> ðŸŽ¥ Click the image above for a video: A brief introduction to ARIMA models. The example is done in R, but the concepts are universal.\n\n## [Pre-lecture quiz](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/43/)\n\n## Introduction\n\nIn this lesson, you will discover a specific way to build models with [ARIMA: *A*uto*R*egressive *I*ntegrated *M*oving *A*verage](https://wikipedia.org/wiki/Autoregressive_integrated_moving_average). ARIMA models are particularly suited to fit data that shows [non-stationarity](https://wikipedia.org/wiki/Stationary_process).\n\n## General concepts\n\nTo be able to work with ARIMA, there are some concepts you need to know about:\n\n- ðŸŽ“ **Stationarity**. From a statistical context, stationarity refers to data whose distribution does not change when shifted in time. Non-stationary data, then, shows fluctuations due to trends that must be transformed to be analyzed. Seasonality, for example, can introduce fluctuations in data and can be eliminated by a process of 'seasonal-differencing'.\n\n- ðŸŽ“ **[Differencing](https://wikipedia.org/wiki/Autoregressive_integrated_moving_average#Differencing)**. Differencing data, again from a statistical context, refers to the process of transforming non-stationary data to make it stationary by removing its non-constant trend. \"Differencing removes the changes in the level of a time series, eliminating trend and seasonality and consequently stabilizing the mean of the time series.\" [Paper by Shixiong et al](https://arxiv.org/abs/1904.07632)\n\n## ARIMA in the context of time series\n\nLet's unpack the parts of ARIMA to better understand how it helps us model time series and help us make predictions against it.\n\n- **AR - for AutoRegressive**. Autoregressive models, as the name implies, look 'back' in time to analyze previous values in your data and make assumptions about them. These previous values are called 'lags'. An example would be data that shows monthly sales of pencils. Each month's sales total would be considered an 'evolving variable' in the dataset. This model is built as the \"evolving variable of interest is regressed on its own lagged (i.e., prior) values.\" [wikipedia](https://wikipedia.org/wiki/Autoregressive_integrated_moving_average)\n\n- **I - for Integrated**. As opposed to the similar 'ARMA' models, the 'I' in ARIMA refers to its *[integrated](https://wikipedia.org/wiki/Order_of_integration)* aspect. The data is 'integrated' when differencing steps are applied so as to eliminate non-stationarity.\n\n- **MA -  for Moving Average**. The [moving-average](https://wikipedia.org/wiki/Moving-average_model) aspect of this model refers to the output variable that is determined by observing the current and past values of lags.\n\nBottom line: ARIMA is used to make a model fit the special form of time series data as closely as possible.\n\n## Exercise - build an ARIMA model\n\nOpen the [_/working_](https://github.com/microsoft/ML-For-Beginners/tree/main/7-TimeSeries/2-ARIMA/working) folder in this lesson and find the [_notebook.ipynb_](https://github.com/microsoft/ML-For-Beginners/blob/main/7-TimeSeries/2-ARIMA/working/notebook.ipynb) file.\n\n1. Run the notebook to load the `statsmodels` Python library; you will need this for ARIMA models.\n\n1. Load necessary libraries\n\n1. Now, load up several more libraries useful for plotting data:\n\n    ```python\n    import os\n    import warnings\n    import matplotlib.pyplot as plt\n    import numpy as np\n    import pandas as pd\n    import datetime as dt\n    import math\n\n    from pandas.plotting import autocorrelation_plot\n    from statsmodels.tsa.statespace.sarimax import SARIMAX\n    from sklearn.preprocessing import MinMaxScaler\n    from common.utils import load_data, mape\n    from IPython.display import Image\n\n    %matplotlib inline\n    pd.options.display.float_format = '{:,.2f}'.format\n    np.set_printoptions(precision=2)\n    warnings.filterwarnings(\"ignore\") # specify to ignore warning messages\n    ```\n\n1. Load the data from the `/data/energy.csv` file into a Pandas dataframe and take a look:\n\n    ```python\n    energy = load_data('./data')[['load']]\n    energy.head(10)\n    ```\n\n1. Plot all the available energy data from January 2012 to December 2014. There should be no surprises as we saw this data in the last lesson:\n\n    ```python\n    energy.plot(y='load', subplots=True, figsize=(15, 8), fontsize=12)\n    plt.xlabel('timestamp', fontsize=12)\n    plt.ylabel('load', fontsize=12)\n    plt.show()\n    ```\n\n    Now, let's build a model!\n\n### Create training and testing datasets\n\nNow your data is loaded, so you can separate it into train and test sets. You'll train your model on the train set. As usual, after the model has finished training, you'll evaluate its accuracy using the test set. You need to ensure that the test set covers a later period in time from the training set to ensure that the model does not gain information from future time periods.\n\n1. Allocate a two-month period from September 1 to October 31, 2014 to the training set. The test set will include the two-month period of November 1 to December 31, 2014:\n\n    ```python\n    train_start_dt = '2014-11-01 00:00:00'\n    test_start_dt = '2014-12-30 00:00:00'\n    ```\n\n    Since this data reflects the daily consumption of energy, there is a strong seasonal pattern, but the consumption is most similar to the consumption in more recent days.\n\n1. Visualize the differences:\n\n    ```python\n    energy[(energy.index < test_start_dt) & (energy.index >= train_start_dt)][['load']].rename(columns={'load':'train'}) \\\n        .join(energy[test_start_dt:][['load']].rename(columns={'load':'test'}), how='outer') \\\n        .plot(y=['train', 'test'], figsize=(15, 8), fontsize=12)\n    plt.xlabel('timestamp', fontsize=12)\n    plt.ylabel('load', fontsize=12)\n    plt.show()\n    ```\n\n    ![training and testing data](images/train-test.png)\n\n    Therefore, using a relatively small window of time for training the data should be sufficient.\n\n    > Note: Since the function we use to fit the ARIMA model uses in-sample validation during fitting, we will omit validation data.\n\n### Prepare the data for training\n\nNow, you need to prepare the data for training by performing filtering and scaling of your data. Filter your dataset to only include the time periods and columns you need, and scaling to ensure the data is projected in the interval 0,1.\n\n1. Filter the original dataset to include only the aforementioned time periods per set and only including the needed column 'load' plus the date:\n\n    ```python\n    train = energy.copy()[(energy.index >= train_start_dt) & (energy.index < test_start_dt)][['load']]\n    test = energy.copy()[energy.index >= test_start_dt][['load']]\n\n    print('Training data shape: ', train.shape)\n    print('Test data shape: ', test.shape)\n    ```\n\n    You can see the shape of the data:\n\n    ```output\n    Training data shape:  (1416, 1)\n    Test data shape:  (48, 1)\n    ```\n\n1. Scale the data to be in the range (0, 1).\n\n    ```python\n    scaler = MinMaxScaler()\n    train['load'] = scaler.fit_transform(train)\n    train.head(10)\n    ```\n\n1. Visualize the original vs. scaled data:\n\n    ```python\n    energy[(energy.index >= train_start_dt) & (energy.index < test_start_dt)][['load']].rename(columns={'load':'original load'}).plot.hist(bins=100, fontsize=12)\n    train.rename(columns={'load':'scaled load'}).plot.hist(bins=100, fontsize=12)\n    plt.show()\n    ```\n\n    ![original](images/original.png)\n\n    > The original data\n\n    ![scaled](images/scaled.png)\n\n    > The scaled data\n\n1. Now that you have calibrated the scaled data, you can scale the test data:\n\n    ```python\n    test['load'] = scaler.transform(test)\n    test.head()\n    ```\n\n### Implement ARIMA\n\nIt's time to implement ARIMA! You'll now use the `statsmodels` library that you installed earlier.\n\nNow you need to follow several steps\n\n   1. Define the model by calling `SARIMAX()` and passing in the model parameters: p, d, and q parameters, and P, D, and Q parameters.\n   2. Prepare the model for the training data by calling the fit() function.\n   3. Make predictions calling the `forecast()` function and specifying the number of steps (the `horizon`) to forecast.\n\n> ðŸŽ“ What are all these parameters for? In an ARIMA model there are 3 parameters that are used to help model the major aspects of a time series: seasonality, trend, and noise. These parameters are:\n\n`p`: the parameter associated with the auto-regressive aspect of the model, which incorporates *past* values.\n`d`: the parameter associated with the integrated part of the model, which affects the amount of *differencing* (ðŸŽ“ remember differencing ðŸ‘†?) to apply to a time series.\n`q`: the parameter associated with the moving-average part of the model.\n\n> Note: If your data has a seasonal aspect - which this one does - , we use a seasonal ARIMA model (SARIMA). In that case you need to use another set of parameters: `P`, `D`, and `Q` which describe the same associations as `p`, `d`, and `q`, but correspond to the seasonal components of the model.\n\n1. Start by setting your preferred horizon value. Let's try 3 hours:\n\n    ```python\n    # Specify the number of steps to forecast ahead\n    HORIZON = 3\n    print('Forecasting horizon:', HORIZON, 'hours')\n    ```\n\n    Selecting the best values for an ARIMA model's parameters can be challenging as it's somewhat subjective and time intensive. You might consider using an `auto_arima()` function from the [`pyramid` library](https://alkaline-ml.com/pmdarima/0.9.0/modules/generated/pyramid.arima.auto_arima.html),\n\n1. For now try some manual selections to find a good model.\n\n    ```python\n    order = (4, 1, 0)\n    seasonal_order = (1, 1, 0, 24)\n\n    model = SARIMAX(endog=train, order=order, seasonal_order=seasonal_order)\n    results = model.fit()\n\n    print(results.summary())\n    ```\n\n    A table of results is printed.\n\nYou've built your first model! Now we need to find a way to evaluate it.\n\n### Evaluate your model\n\nTo evaluate your model, you can perform the so-called `walk forward` validation. In practice, time series models are re-trained each time a new data becomes available. This allows the model to make the best forecast at each time step.\n\nStarting at the beginning of the time series using this technique, train the model on the train data set. Then make a prediction on the next time step. The prediction is evaluated against the known value. The training set is then expanded to include the known value and the process is repeated.\n\n> Note: You should keep the training set window fixed for more efficient training so that every time you add a new observation to the training set, you remove the observation from the beginning of the set.\n\nThis process provides a more robust estimation of how the model will perform in practice. However, it comes at the computation cost of creating so many models. This is acceptable if the data is small or if the model is simple, but could be an issue at scale.\n\nWalk-forward validation is the gold standard of time series model evaluation and is recommended for your own projects.\n\n1. First, create a test data point for each HORIZON step.\n\n    ```python\n    test_shifted = test.copy()\n\n    for t in range(1, HORIZON+1):\n        test_shifted['load+'+str(t)] = test_shifted['load'].shift(-t, freq='H')\n\n    test_shifted = test_shifted.dropna(how='any')\n    test_shifted.head(5)\n    ```\n\n    |            |          | load | load+1 | load+2 |\n    | ---------- | -------- | ---- | ------ | ------ |\n    | 2014-12-30 | 00:00:00 | 0.33 | 0.29   | 0.27   |\n    | 2014-12-30 | 01:00:00 | 0.29 | 0.27   | 0.27   |\n    | 2014-12-30 | 02:00:00 | 0.27 | 0.27   | 0.30   |\n    | 2014-12-30 | 03:00:00 | 0.27 | 0.30   | 0.41   |\n    | 2014-12-30 | 04:00:00 | 0.30 | 0.41   | 0.57   |\n\n    The data is shifted horizontally according to its horizon point.\n\n1. Make predictions on your test data using this sliding window approach in a loop the size of the test data length:\n\n    ```python\n    %%time\n    training_window = 720 # dedicate 30 days (720 hours) for training\n\n    train_ts = train['load']\n    test_ts = test_shifted\n\n    history = [x for x in train_ts]\n    history = history[(-training_window):]\n\n    predictions = list()\n\n    order = (2, 1, 0)\n    seasonal_order = (1, 1, 0, 24)\n\n    for t in range(test_ts.shape[0]):\n        model = SARIMAX(endog=history, order=order, seasonal_order=seasonal_order)\n        model_fit = model.fit()\n        yhat = model_fit.forecast(steps = HORIZON)\n        predictions.append(yhat)\n        obs = list(test_ts.iloc[t])\n        # move the training window\n        history.append(obs[0])\n        history.pop(0)\n        print(test_ts.index[t])\n        print(t+1, ': predicted =', yhat, 'expected =', obs)\n    ```\n\n    You can watch the training occurring:\n\n    ```output\n    2014-12-30 00:00:00\n    1 : predicted = [0.32 0.29 0.28] expected = [0.32945389435989236, 0.2900626678603402, 0.2739480752014323]\n\n    2014-12-30 01:00:00\n    2 : predicted = [0.3  0.29 0.3 ] expected = [0.2900626678603402, 0.2739480752014323, 0.26812891674127126]\n\n    2014-12-30 02:00:00\n    3 : predicted = [0.27 0.28 0.32] expected = [0.2739480752014323, 0.26812891674127126, 0.3025962399283795]\n    ```\n\n1. Compare the predictions to the actual load:\n\n    ```python\n    eval_df = pd.DataFrame(predictions, columns=['t+'+str(t) for t in range(1, HORIZON+1)])\n    eval_df['timestamp'] = test.index[0:len(test.index)-HORIZON+1]\n    eval_df = pd.melt(eval_df, id_vars='timestamp', value_name='prediction', var_name='h')\n    eval_df['actual'] = np.array(np.transpose(test_ts)).ravel()\n    eval_df[['prediction', 'actual']] = scaler.inverse_transform(eval_df[['prediction', 'actual']])\n    eval_df.head()\n    ```\n\n    Output\n    |     |            | timestamp | h   | prediction | actual   |\n    | --- | ---------- | --------- | --- | ---------- | -------- |\n    | 0   | 2014-12-30 | 00:00:00  | t+1 | 3,008.74   | 3,023.00 |\n    | 1   | 2014-12-30 | 01:00:00  | t+1 | 2,955.53   | 2,935.00 |\n    | 2   | 2014-12-30 | 02:00:00  | t+1 | 2,900.17   | 2,899.00 |\n    | 3   | 2014-12-30 | 03:00:00  | t+1 | 2,917.69   | 2,886.00 |\n    | 4   | 2014-12-30 | 04:00:00  | t+1 | 2,946.99   | 2,963.00 |\n\n\n    Observe the hourly data's prediction, compared to the actual load. How accurate is this?\n\n### Check model accuracy\n\nCheck the accuracy of your model by testing its mean absolute percentage error (MAPE) over all the predictions.\n\n> **ðŸ§® Show me the math**\n>\n> ![MAPE](images/mape.png)\n>\n>  [MAPE](https://www.linkedin.com/pulse/what-mape-mad-msd-time-series-allameh-statistics/) is used to show prediction accuracy as a ratio defined by the above formula. The difference between actual<sub>t</sub> and predicted<sub>t</sub> is divided by the actual<sub>t</sub>. \"The absolute value in this calculation is summed for every forecasted point in time and divided by the number of fitted points n.\" [wikipedia](https://wikipedia.org/wiki/Mean_absolute_percentage_error)\n\n1. Express equation in code:\n\n    ```python\n    if(HORIZON > 1):\n        eval_df['APE'] = (eval_df['prediction'] - eval_df['actual']).abs() / eval_df['actual']\n        print(eval_df.groupby('h')['APE'].mean())\n    ```\n\n1. Calculate one step's MAPE:\n\n    ```python\n    print('One step forecast MAPE: ', (mape(eval_df[eval_df['h'] == 't+1']['prediction'], eval_df[eval_df['h'] == 't+1']['actual']))*100, '%')\n    ```\n\n    One step forecast MAPE:  0.5570581332313952 %\n\n1. Print the multi-step forecast MAPE:\n\n    ```python\n    print('Multi-step forecast MAPE: ', mape(eval_df['prediction'], eval_df['actual'])*100, '%')\n    ```\n\n    ```output\n    Multi-step forecast MAPE:  1.1460048657704118 %\n    ```\n\n    A nice low number is best: consider that a forecast that has a MAPE of 10 is off by 10%.\n\n1. But as always, it's easier to see this kind of accuracy measurement visually, so let's plot it:\n\n    ```python\n     if(HORIZON == 1):\n        ## Plotting single step forecast\n        eval_df.plot(x='timestamp', y=['actual', 'prediction'], style=['r', 'b'], figsize=(15, 8))\n\n    else:\n        ## Plotting multi step forecast\n        plot_df = eval_df[(eval_df.h=='t+1')][['timestamp', 'actual']]\n        for t in range(1, HORIZON+1):\n            plot_df['t+'+str(t)] = eval_df[(eval_df.h=='t+'+str(t))]['prediction'].values\n\n        fig = plt.figure(figsize=(15, 8))\n        ax = plt.plot(plot_df['timestamp'], plot_df['actual'], color='red', linewidth=4.0)\n        ax = fig.add_subplot(111)\n        for t in range(1, HORIZON+1):\n            x = plot_df['timestamp'][(t-1):]\n            y = plot_df['t+'+str(t)][0:len(x)]\n            ax.plot(x, y, color='blue', linewidth=4*math.pow(.9,t), alpha=math.pow(0.8,t))\n\n        ax.legend(loc='best')\n\n    plt.xlabel('timestamp', fontsize=12)\n    plt.ylabel('load', fontsize=12)\n    plt.show()\n    ```\n\n    ![a time series model](images/accuracy.png)\n\nðŸ† A very nice plot, showing a model with good accuracy. Well done!\n\n---\n\n## ðŸš€Challenge\n\nDig into the ways to test the accuracy of a Time Series Model. We touch on MAPE in this lesson, but are there other methods you could use? Research them and annotate them. A helpful document can be found [here](https://otexts.com/fpp2/accuracy.html)\n\n## [Post-lecture quiz](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/44/)\n\n## Review & Self Study\n\nThis lesson touches on only the basics of Time Series Forecasting with ARIMA. Take some time to deepen your knowledge by digging into [this repository](https://microsoft.github.io/forecasting/) and its various model types to learn other ways to build Time Series models.\n\n## Assignment\n\n[A new ARIMA model](assignment.md)\n",
              "pdfUrl": "",
              "videoUrl": "",
              "orderIndex": 1
            }
          ],
          "quizzes": [],
          "subModules": []
        },
        {
          "id": 81,
          "title": "SVR",
          "orderIndex": 2,
          "lessons": [
            {
              "id": 82,
              "title": "SVR",
              "content": "# A new SVR model\n\n## Instructions [^1]\n\nNow that you have built an SVR model, build a new one with fresh data (try one of [these datasets from Duke](http://www2.stat.duke.edu/~mw/ts_data_sets.html)). Annotate your work in a notebook, visualize the data and your model, and test its accuracy using appropriate plots and MAPE. Also try tweaking the different hyperparameters and also using different values for the timesteps.\n## Rubric [^1]\n\n| Criteria | Exemplary                                                    | Adequate                                                  | Needs Improvement                   |\n| -------- | ------------------------------------------------------------ | --------------------------------------------------------- | ----------------------------------- |\n|          | A notebook is presented with an SVR model built, tested and explained with visualizations and accuracy stated. | The notebook presented is not annotated or contains bugs. | An incomplete notebook is presented |\n\n\n\n[^1]:The text in this section was based on the [assignment from ARIMA](https://github.com/microsoft/ML-For-Beginners/tree/main/7-TimeSeries/2-ARIMA/assignment.md)\n",
              "pdfUrl": "",
              "videoUrl": "",
              "orderIndex": 0
            },
            {
              "id": 83,
              "title": "SVR",
              "content": "# Time Series Forecasting with Support Vector Regressor\n\nIn the previous lesson, you learned how to use ARIMA model to make time series predictions. Now you'll be looking at Support Vector Regressor model which is a regressor model used to predict continuous data.\n\n## [Pre-lecture quiz](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/51/) \n\n## Introduction\n\nIn this lesson, you will discover a specific way to build models with [**SVM**: **S**upport **V**ector **M**achine](https://en.wikipedia.org/wiki/Support-vector_machine) for regression, or **SVR: Support Vector Regressor**. \n\n### SVR in the context of time series [^1]\n\nBefore understanding the importance of SVR in time series prediction, here are some of the important concepts that you need to know:\n\n- **Regression:** Supervised learning technique to predict continuous values from a given set of inputs. The idea is to fit a curve (or line) in the feature space that has the maximum number of data points. [Click here](https://en.wikipedia.org/wiki/Regression_analysis) for more information.\n- **Support Vector Machine (SVM):** A type of supervised machine learning model used for classification, regression and outliers detection. The model is a hyperplane in the feature space, which in case of classification acts as a boundary, and in case of regression acts as the best-fit line. In SVM, a Kernel function is generally used to transform the dataset to a space of higher number of dimensions, so that they can be easily separable. [Click here](https://en.wikipedia.org/wiki/Support-vector_machine) for more information on SVMs.\n- **Support Vector Regressor (SVR):** A type of SVM, to find the best fit line (which in the case of SVM is a hyperplane) that has the maximum number of data points.\n\n### Why SVR? [^1]\n\nIn the last lesson you learned about ARIMA, which is a very successful statistical linear method to forecast time series data. However, in many cases, time series data have *non-linearity*, which cannot be mapped by linear models. In such cases, the ability of SVM to consider non-linearity in the data for regression tasks makes SVR successful in time series forecasting.\n\n## Exercise - build an SVR model\n\nThe first few steps for data preparation are the same as that of the previous lesson on [ARIMA](https://github.com/microsoft/ML-For-Beginners/tree/main/7-TimeSeries/2-ARIMA). \n\nOpen the [_/working_](https://github.com/microsoft/ML-For-Beginners/tree/main/7-TimeSeries/3-SVR/working) folder in this lesson and find the [_notebook.ipynb_](https://github.com/microsoft/ML-For-Beginners/blob/main/7-TimeSeries/3-SVR/working/notebook.ipynb) file.[^2]\n\n1. Run the notebook and import the necessary libraries:  [^2]\n\n   ```python\n   import sys\n   sys.path.append('../../')\n   ```\n\n   ```python\n   import os\n   import warnings\n   import matplotlib.pyplot as plt\n   import numpy as np\n   import pandas as pd\n   import datetime as dt\n   import math\n   \n   from sklearn.svm import SVR\n   from sklearn.preprocessing import MinMaxScaler\n   from common.utils import load_data, mape\n   ```\n\n2. Load the data from the `/data/energy.csv` file into a Pandas dataframe and take a look:  [^2]\n\n   ```python\n   energy = load_data('../../data')[['load']]\n   ```\n\n3. Plot all the available energy data from January 2012 to December 2014: [^2]\n\n   ```python\n   energy.plot(y='load', subplots=True, figsize=(15, 8), fontsize=12)\n   plt.xlabel('timestamp', fontsize=12)\n   plt.ylabel('load', fontsize=12)\n   plt.show()\n   ```\n\n   ![full data](images/full-data.png)\n\n   Now, let's build our SVR model.\n\n### Create training and testing datasets\n\nNow your data is loaded, so you can separate it into train and test sets. Then you'll reshape the data to create a time-step based dataset which will be needed for the SVR. You'll train your model on the train set. After the model has finished training, you'll evaluate its accuracy on the training set, testing set and then the full dataset to see the overall performance. You need to ensure that the test set covers a later period in time from the training set to ensure that the model does not gain information from future time periods [^2] (a situation known as *Overfitting*).\n\n1. Allocate a two-month period from September 1 to October 31, 2014 to the training set. The test set will include the two-month period of November 1 to December 31, 2014: [^2]\n\n   ```python\n   train_start_dt = '2014-11-01 00:00:00'\n   test_start_dt = '2014-12-30 00:00:00'\n   ```\n\n2. Visualize the differences: [^2]\n\n   ```python\n   energy[(energy.index < test_start_dt) & (energy.index >= train_start_dt)][['load']].rename(columns={'load':'train'}) \\\n       .join(energy[test_start_dt:][['load']].rename(columns={'load':'test'}), how='outer') \\\n       .plot(y=['train', 'test'], figsize=(15, 8), fontsize=12)\n   plt.xlabel('timestamp', fontsize=12)\n   plt.ylabel('load', fontsize=12)\n   plt.show()\n   ```\n\n   ![training and testing data](images/train-test.png)\n\n\n\n### Prepare the data for training\n\nNow, you need to prepare the data for training by performing filtering and scaling of your data. Filter your dataset to only include the time periods and columns you need, and scaling to ensure the data is projected in the interval 0,1.\n\n1. Filter the original dataset to include only the aforementioned time periods per set and only including the needed column 'load' plus the date: [^2]\n\n   ```python\n   train = energy.copy()[(energy.index >= train_start_dt) & (energy.index < test_start_dt)][['load']]\n   test = energy.copy()[energy.index >= test_start_dt][['load']]\n   \n   print('Training data shape: ', train.shape)\n   print('Test data shape: ', test.shape)\n   ```\n\n   ```output\n   Training data shape:  (1416, 1)\n   Test data shape:  (48, 1)\n   ```\n   \n2. Scale the training data to be in the range (0, 1): [^2]\n\n   ```python\n   scaler = MinMaxScaler()\n   train['load'] = scaler.fit_transform(train)\n   ```\n   \n4. Now, you scale the testing data: [^2]\n\n   ```python\n   test['load'] = scaler.transform(test)\n   ```\n\n### Create data with time-steps [^1]\n\nFor the SVR, you transform the input data to be of the form `[batch, timesteps]`. So, you reshape the existing `train_data` and `test_data` such that there is a new dimension which refers to the timesteps. \n\n```python\n# Converting to numpy arrays\ntrain_data = train.values\ntest_data = test.values\n```\n\nFor this example, we take `timesteps = 5`. So, the inputs to the model are the data for the first 4 timesteps, and the output will be the data for the 5th timestep.\n\n```python\ntimesteps=5\n```\n\nConverting training data to 2D tensor using nested list comprehension:\n\n```python\ntrain_data_timesteps=np.array([[j for j in train_data[i:i+timesteps]] for i in range(0,len(train_data)-timesteps+1)])[:,:,0]\ntrain_data_timesteps.shape\n```\n\n```output\n(1412, 5)\n```\n\nConverting testing data to 2D tensor:\n\n```python\ntest_data_timesteps=np.array([[j for j in test_data[i:i+timesteps]] for i in range(0,len(test_data)-timesteps+1)])[:,:,0]\ntest_data_timesteps.shape\n```\n\n```output\n(44, 5)\n```\n\n Selecting inputs and outputs from training and testing data:\n\n```python\nx_train, y_train = train_data_timesteps[:,:timesteps-1],train_data_timesteps[:,[timesteps-1]]\nx_test, y_test = test_data_timesteps[:,:timesteps-1],test_data_timesteps[:,[timesteps-1]]\n\nprint(x_train.shape, y_train.shape)\nprint(x_test.shape, y_test.shape)\n```\n\n```output\n(1412, 4) (1412, 1)\n(44, 4) (44, 1)\n```\n\n### Implement SVR [^1]\n\nNow, it's time to implement SVR. To read more about this implementation, you can refer to [this documentation](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html). For our implementation, we follow these steps:\n\n  1. Define the model by calling `SVR()` and passing in the model hyperparameters: kernel, gamma, c and epsilon\n  2. Prepare the model for the training data by calling the `fit()` function\n  3. Make predictions calling the `predict()` function\n\nNow we create an SVR model. Here we use the [RBF kernel](https://scikit-learn.org/stable/modules/svm.html#parameters-of-the-rbf-kernel), and set the hyperparameters gamma, C and epsilon as 0.5, 10 and 0.05 respectively.\n\n```python\nmodel = SVR(kernel='rbf',gamma=0.5, C=10, epsilon = 0.05)\n```\n\n#### Fit the model on training data [^1]\n\n```python\nmodel.fit(x_train, y_train[:,0])\n```\n\n```output\nSVR(C=10, cache_size=200, coef0=0.0, degree=3, epsilon=0.05, gamma=0.5,\n    kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)\n```\n\n#### Make model predictions [^1]\n\n```python\ny_train_pred = model.predict(x_train).reshape(-1,1)\ny_test_pred = model.predict(x_test).reshape(-1,1)\n\nprint(y_train_pred.shape, y_test_pred.shape)\n```\n\n```output\n(1412, 1) (44, 1)\n```\n\nYou've built your SVR! Now we need to evaluate it.\n\n### Evaluate your model [^1]\n\nFor evaluation, first we will scale back the data to our original scale. Then, to check the performance, we will plot the original and predicted time series plot, and also print the MAPE result.\n\nScale the predicted and original output:\n\n```python\n# Scaling the predictions\ny_train_pred = scaler.inverse_transform(y_train_pred)\ny_test_pred = scaler.inverse_transform(y_test_pred)\n\nprint(len(y_train_pred), len(y_test_pred))\n```\n\n```python\n# Scaling the original values\ny_train = scaler.inverse_transform(y_train)\ny_test = scaler.inverse_transform(y_test)\n\nprint(len(y_train), len(y_test))\n```\n\n#### Check model performance on training and testing data [^1]\n\nWe extract the timestamps from the dataset to show in the x-axis of our plot. Note that we are using the first ```timesteps-1``` values as out input for the first output, so the timestamps for the output will start after that.\n\n```python\ntrain_timestamps = energy[(energy.index < test_start_dt) & (energy.index >= train_start_dt)].index[timesteps-1:]\ntest_timestamps = energy[test_start_dt:].index[timesteps-1:]\n\nprint(len(train_timestamps), len(test_timestamps))\n```\n\n```output\n1412 44\n```\n\nPlot the predictions for training data:\n\n```python\nplt.figure(figsize=(25,6))\nplt.plot(train_timestamps, y_train, color = 'red', linewidth=2.0, alpha = 0.6)\nplt.plot(train_timestamps, y_train_pred, color = 'blue', linewidth=0.8)\nplt.legend(['Actual','Predicted'])\nplt.xlabel('Timestamp')\nplt.title(\"Training data prediction\")\nplt.show()\n```\n\n![training data prediction](images/train-data-predict.png)\n\nPrint MAPE for training data\n\n```python\nprint('MAPE for training data: ', mape(y_train_pred, y_train)*100, '%')\n```\n\n```output\nMAPE for training data: 1.7195710200875551 %\n```\n\nPlot the predictions for testing data\n\n```python\nplt.figure(figsize=(10,3))\nplt.plot(test_timestamps, y_test, color = 'red', linewidth=2.0, alpha = 0.6)\nplt.plot(test_timestamps, y_test_pred, color = 'blue', linewidth=0.8)\nplt.legend(['Actual','Predicted'])\nplt.xlabel('Timestamp')\nplt.show()\n```\n\n![testing data prediction](images/test-data-predict.png)\n\nPrint MAPE for testing data\n\n```python\nprint('MAPE for testing data: ', mape(y_test_pred, y_test)*100, '%')\n```\n\n```output\nMAPE for testing data:  1.2623790187854018 %\n```\n\nðŸ† You have a very good result on the testing dataset!\n\n### Check model performance on full dataset [^1]\n\n```python\n# Extracting load values as numpy array\ndata = energy.copy().values\n\n# Scaling\ndata = scaler.transform(data)\n\n# Transforming to 2D tensor as per model input requirement\ndata_timesteps=np.array([[j for j in data[i:i+timesteps]] for i in range(0,len(data)-timesteps+1)])[:,:,0]\nprint(\"Tensor shape: \", data_timesteps.shape)\n\n# Selecting inputs and outputs from data\nX, Y = data_timesteps[:,:timesteps-1],data_timesteps[:,[timesteps-1]]\nprint(\"X shape: \", X.shape,\"\\nY shape: \", Y.shape)\n```\n\n```output\nTensor shape:  (26300, 5)\nX shape:  (26300, 4) \nY shape:  (26300, 1)\n```\n\n```python\n# Make model predictions\nY_pred = model.predict(X).reshape(-1,1)\n\n# Inverse scale and reshape\nY_pred = scaler.inverse_transform(Y_pred)\nY = scaler.inverse_transform(Y)\n```\n\n```python\nplt.figure(figsize=(30,8))\nplt.plot(Y, color = 'red', linewidth=2.0, alpha = 0.6)\nplt.plot(Y_pred, color = 'blue', linewidth=0.8)\nplt.legend(['Actual','Predicted'])\nplt.xlabel('Timestamp')\nplt.show()\n```\n\n![full data prediction](images/full-data-predict.png)\n\n```python\nprint('MAPE: ', mape(Y_pred, Y)*100, '%')\n```\n\n```output\nMAPE:  2.0572089029888656 %\n```\n\n\n\nðŸ† Very nice plots, showing a model with good accuracy. Well done!\n\n---\n\n## ðŸš€Challenge\n\n- Try to tweak the hyperparameters (gamma, C, epsilon) while creating the model and evaluate on the data to see which set of hyperparameters give the best results on the testing data. To know more about these hyperparameters, you can refer to the  document [here](https://scikit-learn.org/stable/modules/svm.html#parameters-of-the-rbf-kernel). \n- Try to use different kernel functions for the model and analyze their performances on the dataset. A helpful document can be found [here](https://scikit-learn.org/stable/modules/svm.html#kernel-functions).\n- Try using different values for `timesteps` for the model to look back to make prediction.\n\n## [Post-lecture quiz](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/52/)\n\n## Review & Self Study\n\nThis lesson was to introduce the application of SVR for Time Series Forecasting. To read more about SVR, you can refer to [this blog](https://www.analyticsvidhya.com/blog/2020/03/support-vector-regression-tutorial-for-machine-learning/). This [documentation on scikit-learn](https://scikit-learn.org/stable/modules/svm.html) provides a more comprehensive explanation about SVMs in general, [SVRs](https://scikit-learn.org/stable/modules/svm.html#regression) and also other implementation details such as the different [kernel functions](https://scikit-learn.org/stable/modules/svm.html#kernel-functions) that can be used, and their parameters.\n\n## Assignment\n\n[A new SVR model](assignment.md)\n\n\n\n## Credits\n\n\n[^1]: The text, code and output in this section was contributed by [@AnirbanMukherjeeXD](https://github.com/AnirbanMukherjeeXD)\n[^2]: The text, code and output in this section was taken from [ARIMA](https://github.com/microsoft/ML-For-Beginners/tree/main/7-TimeSeries/2-ARIMA)\n",
              "pdfUrl": "",
              "videoUrl": "",
              "orderIndex": 1
            }
          ],
          "quizzes": [],
          "subModules": []
        }
      ]
    },
    {
      "id": 84,
      "title": "Reinforcement",
      "orderIndex": 7,
      "lessons": [
        {
          "id": 85,
          "title": "Reinforcement",
          "content": "# Introduction to reinforcement learning\n\nReinforcement learning, RL, is seen as one of the basic machine learning paradigms, next to supervised learning and unsupervised learning. RL is all about decisions: delivering the right decisions or at least learning from them.\n\nImagine you have a simulated environment such as the stock market. What happens if you impose a given regulation? Does it have a positive or negative effect? If something negative happens, you need to take this _negative reinforcement_, learn from it, and change course. If it's a positive outcome, you need to build on that _positive reinforcement_.\n\n![peter and the wolf](images/peter.png)\n\n> Peter and his friends need to escape the hungry wolf! Image by [Jen Looper](https://twitter.com/jenlooper)\n\n## Regional topic: Peter and the Wolf (Russia)\n\n[Peter and the Wolf](https://en.wikipedia.org/wiki/Peter_and_the_Wolf) is a musical fairy tale written by a Russian composer [Sergei Prokofiev](https://en.wikipedia.org/wiki/Sergei_Prokofiev). It is a story about young pioneer Peter, who bravely goes out of his house to the forest clearing to chase the wolf. In this section, we will train machine learning algorithms that will help Peter:\n\n- **Explore** the surrounding area and build an optimal navigation map\n- **Learn** how to use a skateboard and balance on it, in order to move around faster.\n\n[![Peter and the Wolf](https://img.youtube.com/vi/Fmi5zHg4QSM/0.jpg)](https://www.youtube.com/watch?v=Fmi5zHg4QSM)\n\n> ðŸŽ¥ Click the image above to listen to Peter and the Wolf by Prokofiev\n\n## Reinforcement learning\n\nIn previous sections, you have seen two examples of machine learning problems:\n\n- **Supervised**, where we have datasets that suggest sample solutions to the problem we want to solve. [Classification](../4-Classification/README.md) and [regression](../2-Regression/README.md) are supervised learning tasks.\n- **Unsupervised**, in which we do not have labeled training data. The main example of unsupervised learning is [Clustering](../5-Clustering/README.md).\n\nIn this section, we will introduce you to a new type of learning problem that does not require labeled training data. There are several types of such problems:\n\n- **[Semi-supervised learning](https://wikipedia.org/wiki/Semi-supervised_learning)**, where we have a lot of unlabeled data that can be used to pre-train the model.\n- **[Reinforcement learning](https://wikipedia.org/wiki/Reinforcement_learning)**, in which an agent learns how to behave by performing experiments in some simulated environment.\n\n### Example - computer game\n\nSuppose you want to teach a computer to play a game, such as chess, or [Super Mario](https://wikipedia.org/wiki/Super_Mario). For the computer to play a game, we need it to predict which move to make in each of the game states. While this may seem like a classification problem, it is not - because we do not have a dataset with states and corresponding actions. While we may have some data like existing chess matches or recording of players playing Super Mario, it is likely that that data will not sufficiently cover a large enough number of possible states.\n\nInstead of looking for existing game data, **Reinforcement Learning** (RL) is based on the idea of *making the computer play* many times and observing the result. Thus, to apply Reinforcement Learning, we need two things:\n\n- **An environment** and **a simulator** which allow us to play a game many times. This simulator would define all the game rules as well as possible states and actions.\n\n- **A reward function**, which would tell us how well we did during each move or game.\n\nThe main difference between other types of machine learning and RL is that in RL we typically do not know whether we win or lose until we finish the game. Thus, we cannot say whether a certain move alone is good or not - we only receive a reward at the end of the game. And our goal is to design algorithms that will allow us to train a model under  uncertain conditions. We will learn about one RL algorithm called **Q-learning**.\n\n## Lessons\n\n1. [Introduction to reinforcement learning and Q-Learning](1-QLearning/README.md)\n2. [Using a gym simulation environment](2-Gym/README.md)\n\n## Credits\n\n\"Introduction to Reinforcement Learning\" was written with â™¥ï¸ by [Dmitry Soshnikov](http://soshnikov.com)\n",
          "pdfUrl": "",
          "videoUrl": "",
          "orderIndex": 0
        }
      ],
      "quizzes": [],
      "subModules": [
        {
          "id": 86,
          "title": "QLearning",
          "orderIndex": 0,
          "lessons": [
            {
              "id": 87,
              "title": "QLearning",
              "content": "# A More Realistic World\n\nIn our situation, Peter was able to move around almost without getting tired or hungry. In a more realistic world, we has to sit down and rest from time to time, and also to feed himself. Let's make our world more realistic, by implementing the following rules:\n\n1. By moving from one place to another, Peter loses **energy** and gains some **fatigue**.\n2. Peter can gain more energy by eating apples.\n3. Peter can get rid of fatigue by resting under the tree or on the grass (i.e. walking into a board location with a tree or grass - green field)\n4. Peter needs to find and kill the wolf\n5. In order to kill the wolf, Peter needs to have certain levels of energy and fatigue, otherwise he loses the battle.\n## Instructions\n\nUse the original [notebook.ipynb](notebook.ipynb) notebook as a starting point for your solution.\n\nModify the reward function above according to the rules of the game, run the reinforcement learning algorithm to learn the best strategy for winning the game, and compare the results of random walk with your algorithm in terms of number of games won and lost.\n\n> **Note**: In your new world, the state is more complex, and in addition to human position also includes fatigue and energy levels. You may chose to represent the state as a tuple (Board,energy,fatigue), or define a class for the state (you may also want to derive it from `Board`), or even modify the original `Board` class inside [rlboard.py](rlboard.py).\n\nIn your solution, please keep the code responsible for random walk strategy, and compare the results of your algorithm with random walk at the end.\n\n> **Note**: You may need to adjust hyperparameters to make it work, especially the number of epochs. Because the success of the game (fighting the wolf) is a rare event, you can expect much longer training time.\n## Rubric\n\n| Criteria | Exemplary                                                                                                                                                                                             | Adequate                                                                                                                                                                                | Needs Improvement                                                                                                                          |\n| -------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------ |\n|          | A notebook is presented with the definition of new world rules, Q-Learning algorithm and some textual explanations. Q-Learning is able to significantly improve the results comparing to random walk. | Notebook is presented, Q-Learning is implemented and improves results comparing to random walk, but not significantly; or notebook is poorly documented and code is not well-structured | Some attempt to re-define the rules of the world are made, but Q-Learning algorithm does not work, or reward function is not fully defined |\n",
              "pdfUrl": "",
              "videoUrl": "",
              "orderIndex": 0
            },
            {
              "id": 88,
              "title": "QLearning",
              "content": "# Introduction to Reinforcement Learning and Q-Learning\n\n![Summary of reinforcement in machine learning in a sketchnote](../../sketchnotes/ml-reinforcement.png)\n> Sketchnote by [Tomomi Imura](https://www.twitter.com/girlie_mac)\n\nReinforcement learning involves three important concepts: the agent, some states, and a set of actions per state. By executing an action in a specified state, the agent is given a reward. Again imagine the computer game Super Mario. You are Mario, you are in a game level, standing next to a cliff edge. Above you is a coin. You being Mario, in a game level, at a specific position ... that's your state. Moving one step to the right (an action) will take you over the edge, and that would give you a low numerical score. However, pressing the jump button would let you score a point and you would stay alive. That's a positive outcome and that should award you a positive numerical score.\n\nBy using reinforcement learning and a simulator (the game), you can learn how to play the game to maximize the reward which is staying alive and scoring as many points as possible.\n\n[![Intro to Reinforcement Learning](https://img.youtube.com/vi/lDq_en8RNOo/0.jpg)](https://www.youtube.com/watch?v=lDq_en8RNOo)\n\n> ðŸŽ¥ Click the image above to hear Dmitry discuss Reinforcement Learning\n\n## [Pre-lecture quiz](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/45/)\n\n## Prerequisites and Setup\n\nIn this lesson, we will be experimenting with some code in Python. You should be able to run the Jupyter Notebook code from this lesson, either on your computer or somewhere in the cloud.\n\nYou can open [the lesson notebook](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/1-QLearning/notebook.ipynb) and walk through this lesson to build.\n\n> **Note:** If you are opening this code from the cloud, you also need to fetch the [`rlboard.py`](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/1-QLearning/rlboard.py) file, which is used in the notebook code. Add it to the same directory as the notebook.\n\n## Introduction\n\nIn this lesson, we will explore the world of **[Peter and the Wolf](https://en.wikipedia.org/wiki/Peter_and_the_Wolf)**, inspired by a musical fairy tale by a Russian composer, [Sergei Prokofiev](https://en.wikipedia.org/wiki/Sergei_Prokofiev). We will use **Reinforcement Learning** to let Peter explore his environment, collect tasty apples and avoid meeting the wolf.\n\n**Reinforcement Learning** (RL) is a learning technique that allows us to learn an optimal behavior of an **agent** in some **environment** by running many experiments. An agent in this environment should have some **goal**, defined by a **reward function**.\n\n## The environment\n\nFor simplicity, let's consider Peter's world to be a square board of size `width` x `height`, like this:\n\n![Peter's Environment](images/environment.png)\n\nEach cell in this board can either be:\n\n* **ground**, on which Peter and other creatures can walk.\n* **water**, on which you obviously cannot walk.\n* a **tree** or **grass**, a place where you can rest.\n* an **apple**, which represents something Peter would be glad to find in order to feed himself.\n* a **wolf**, which is dangerous and should be avoided.\n\nThere is a separate Python module, [`rlboard.py`](https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/1-QLearning/rlboard.py), which contains the code to work with this environment. Because this code is not important for understanding our concepts, we will import the module and use it to create the sample board (code block 1):\n\n```python\nfrom rlboard import *\n\nwidth, height = 8,8\nm = Board(width,height)\nm.randomize(seed=13)\nm.plot()\n```\n\nThis code should print a picture of the environment similar to the one above.\n\n## Actions and policy\n\nIn our example, Peter's goal would be able to find an apple, while avoiding the wolf and other obstacles. To do this, he can essentially walk around until he finds an apple.\n\nTherefore, at any position, he can choose between one of the following actions: up, down, left and right.\n\nWe will define those actions as a dictionary, and map them to pairs of corresponding coordinate changes. For example, moving right (`R`) would correspond to a pair `(1,0)`. (code block 2):\n\n```python\nactions = { \"U\" : (0,-1), \"D\" : (0,1), \"L\" : (-1,0), \"R\" : (1,0) }\naction_idx = { a : i for i,a in enumerate(actions.keys()) }\n```\n\nTo sum up, the strategy and goal of this scenario are as follows:\n\n- **The strategy**, of our agent (Peter) is defined by a so-called **policy**. A policy is a function that returns the action at any given state. In our case, the state of the problem is represented by the board, including the current position of the player.\n\n- **The goal**, of reinforcement learning is to eventually learn a good policy that will allow us to solve the problem efficiently. However, as a baseline, let's consider the simplest policy called **random walk**.\n\n## Random walk\n\nLet's first solve our problem by implementing a random walk strategy. With random walk, we will randomly choose the next action from the allowed actions, until we reach the apple (code block 3).\n\n1. Implement the random walk with the below code:\n\n    ```python\n    def random_policy(m):\n        return random.choice(list(actions))\n    \n    def walk(m,policy,start_position=None):\n        n = 0 # number of steps\n        # set initial position\n        if start_position:\n            m.human = start_position \n        else:\n            m.random_start()\n        while True:\n            if m.at() == Board.Cell.apple:\n                return n # success!\n            if m.at() in [Board.Cell.wolf, Board.Cell.water]:\n                return -1 # eaten by wolf or drowned\n            while True:\n                a = actions[policy(m)]\n                new_pos = m.move_pos(m.human,a)\n                if m.is_valid(new_pos) and m.at(new_pos)!=Board.Cell.water:\n                    m.move(a) # do the actual move\n                    break\n            n+=1\n    \n    walk(m,random_policy)\n    ```\n\n    The call to `walk` should return the length of the corresponding path, which can vary from one run to another. \n\n1. Run the walk experiment a number of times (say, 100), and print the resulting statistics (code block 4):\n\n    ```python\n    def print_statistics(policy):\n        s,w,n = 0,0,0\n        for _ in range(100):\n            z = walk(m,policy)\n            if z<0:\n                w+=1\n            else:\n                s += z\n                n += 1\n        print(f\"Average path length = {s/n}, eaten by wolf: {w} times\")\n    \n    print_statistics(random_policy)\n    ```\n\n    Note that the average length of a path is around 30-40 steps, which is quite a lot, given the fact that the average distance to the nearest apple is around 5-6 steps.\n\n    You can also see what Peter's movement looks like during the random walk:\n\n    ![Peter's Random Walk](images/random_walk.gif)\n\n## Reward function\n\nTo make our policy more intelligent, we need to understand which moves are \"better\" than others. To do this, we need to define our goal.\n\nThe goal can be defined in terms of a **reward function**, which will return some score value for each state. The higher the number, the better the reward function. (code block 5)\n\n```python\nmove_reward = -0.1\ngoal_reward = 10\nend_reward = -10\n\ndef reward(m,pos=None):\n    pos = pos or m.human\n    if not m.is_valid(pos):\n        return end_reward\n    x = m.at(pos)\n    if x==Board.Cell.water or x == Board.Cell.wolf:\n        return end_reward\n    if x==Board.Cell.apple:\n        return goal_reward\n    return move_reward\n```\n\nAn interesting thing about reward functions is that in most cases, *we are only given a substantial reward at the end of the game*. This means that our algorithm should somehow remember \"good\" steps that lead to a positive reward at the end, and increase their importance. Similarly, all moves that lead to bad results should be discouraged.\n\n## Q-Learning\n\nAn algorithm that we will discuss here is called **Q-Learning**. In this algorithm, the policy is defined by a function (or a data structure) called a **Q-Table**. It records the \"goodness\" of each of the actions in a given state.\n\nIt is called a Q-Table because it is often convenient to represent it as a table, or multi-dimensional array. Since our board has dimensions `width` x `height`, we can represent the Q-Table using a numpy array with shape `width` x `height` x `len(actions)`: (code block 6)\n\n```python\nQ = np.ones((width,height,len(actions)),dtype=np.float)*1.0/len(actions)\n```\n\nNotice that we initialize all the values of the Q-Table with an equal value, in our case - 0.25. This corresponds to the \"random walk\" policy, because all moves in each state are equally good. We can pass the Q-Table to the `plot` function in order to visualize the table on the board: `m.plot(Q)`.\n\n![Peter's Environment](images/env_init.png)\n\nIn the center of each cell there is an \"arrow\" that indicates the preferred direction of movement. Since all directions are equal, a dot is displayed.\n\nNow we need to run the simulation, explore our environment, and learn a better distribution of Q-Table values, which will allow us to find the path to the apple much faster.\n\n## Essence of Q-Learning: Bellman Equation\n\nOnce we start moving, each action will have a corresponding reward, i.e. we can theoretically select the next action based on the highest immediate reward. However, in most states, the move will not achieve our goal of reaching the apple, and thus we cannot immediately decide which direction is better.\n\n> Remember that it is not the immediate result that matters, but rather the final result, which we will obtain at the end of the simulation.\n\nIn order to account for this delayed reward, we need to use the principles of **[dynamic programming](https://en.wikipedia.org/wiki/Dynamic_programming)**, which allow us to think about out problem recursively.\n\nSuppose we are now at the state *s*, and we want to move to the next state *s'*. By doing so, we will receive the immediate reward *r(s,a)*, defined by the reward function, plus some future reward. If we suppose that our Q-Table correctly reflects the \"attractiveness\" of each action, then at state *s'* we will chose an action *a* that corresponds to maximum value of *Q(s',a')*. Thus, the best possible future reward we could get at state *s* will be defined as `max`<sub>a'</sub>*Q(s',a')* (maximum here is computed over all possible actions *a'* at state *s'*).\n\nThis gives the **Bellman formula** for calculating the value of the Q-Table at state *s*, given action *a*:\n\n<img src=\"images/bellman-equation.png\"/>\n\nHere Î³ is the so-called **discount factor** that determines to which extent you should prefer the current reward over the future reward and vice versa.\n\n## Learning Algorithm\n\nGiven the equation above, we can now write pseudo-code for our learning algorithm:\n\n* Initialize Q-Table Q with equal numbers for all states and actions\n* Set learning rate Î± â† 1\n* Repeat simulation many times\n   1. Start at random position\n   1. Repeat\n        1. Select an action *a* at state *s*\n        2. Execute action by moving to a new state *s'*\n        3. If we encounter end-of-game condition, or total reward is too small - exit simulation  \n        4. Compute reward *r* at the new state\n        5. Update Q-Function according to Bellman equation: *Q(s,a)* â† *(1-Î±)Q(s,a)+Î±(r+Î³ max<sub>a'</sub>Q(s',a'))*\n        6. *s* â† *s'*\n        7. Update the total reward and decrease Î±.\n\n## Exploit vs. explore\n\nIn the algorithm above, we did not specify how exactly we should choose an action at step 2.1. If we are choosing the action randomly, we will randomly **explore** the environment, and we are quite likely to die often as well as explore areas where we would not normally go. An alternative approach would be to **exploit** the Q-Table values that we already know, and thus to choose the best action (with higher Q-Table value) at state *s*. This, however, will prevent us from exploring other states, and it's likely we might not find the optimal solution.\n\nThus, the best approach is to strike a balance between exploration and exploitation. This can be done by choosing the action at state *s* with probabilities proportional to values in the Q-Table. In the beginning, when Q-Table values are all the same, it would correspond to a random selection, but as we learn more about our environment, we would be more likely to follow the optimal route while allowing the agent to choose the unexplored path once in a while.\n\n## Python implementation\n\nWe are now ready to implement the learning algorithm. Before we do that, we also need some function that will convert arbitrary numbers in the Q-Table into a vector of probabilities for corresponding actions.\n\n1. Create a function `probs()`:\n\n    ```python\n    def probs(v,eps=1e-4):\n        v = v-v.min()+eps\n        v = v/v.sum()\n        return v\n    ```\n\n    We add a few `eps` to the original vector in order to avoid division by 0 in the initial case, when all components of the vector are identical.\n\nRun them learning algorithm through 5000 experiments, also called **epochs**: (code block 8)\n```python\n    for epoch in range(5000):\n    \n        # Pick initial point\n        m.random_start()\n        \n        # Start travelling\n        n=0\n        cum_reward = 0\n        while True:\n            x,y = m.human\n            v = probs(Q[x,y])\n            a = random.choices(list(actions),weights=v)[0]\n            dpos = actions[a]\n            m.move(dpos,check_correctness=False) # we allow player to move outside the board, which terminates episode\n            r = reward(m)\n            cum_reward += r\n            if r==end_reward or cum_reward < -1000:\n                lpath.append(n)\n                break\n            alpha = np.exp(-n / 10e5)\n            gamma = 0.5\n            ai = action_idx[a]\n            Q[x,y,ai] = (1 - alpha) * Q[x,y,ai] + alpha * (r + gamma * Q[x+dpos[0], y+dpos[1]].max())\n            n+=1\n```\n\nAfter executing this algorithm, the Q-Table should be updated with values that define the attractiveness of different actions at each step. We can try to visualize the Q-Table by plotting a vector at each cell that will point in the desired direction of movement. For simplicity, we draw a small circle instead of an arrow head.\n\n<img src=\"images/learned.png\"/>\n\n## Checking the policy\n\nSince the Q-Table lists the \"attractiveness\" of each action at each state, it is quite easy to use it to define the efficient navigation in our world. In the simplest case, we can select the action corresponding to the highest Q-Table value: (code block 9)\n\n```python\ndef qpolicy_strict(m):\n        x,y = m.human\n        v = probs(Q[x,y])\n        a = list(actions)[np.argmax(v)]\n        return a\n\nwalk(m,qpolicy_strict)\n```\n\n> If you try the code above several times, you may notice that sometimes it \"hangs\", and you need to press the STOP button in the notebook to interrupt it. This happens because there could be situations when two states \"point\" to each other in terms of optimal Q-Value, in which case the agents ends up moving between those states indefinitely.\n\n## ðŸš€Challenge\n\n> **Task 1:** Modify the `walk` function to limit the maximum length of path by a certain number of steps (say, 100), and watch the code above return this value from time to time.\n\n> **Task 2:** Modify the `walk` function so that it does not go back to the places where it has already been previously. This will prevent `walk` from looping, however, the agent can still end up being \"trapped\" in a location from which it is unable to escape.\n\n## Navigation\n\nA better navigation policy would be the one that we used during training, which combines exploitation and exploration. In this policy, we will select each action with a certain probability, proportional to the values in the Q-Table. This strategy may still result in the agent returning back to a position it has already explored, but, as you can see from the code below, it results in a very short average path to the desired location (remember that `print_statistics` runs the simulation 100 times): (code block 10)\n\n```python\ndef qpolicy(m):\n        x,y = m.human\n        v = probs(Q[x,y])\n        a = random.choices(list(actions),weights=v)[0]\n        return a\n\nprint_statistics(qpolicy)\n```\n\nAfter running this code, you should get a much smaller average path length than before, in the range of 3-6.\n\n## Investigating the learning process\n\nAs we have mentioned, the learning process is a balance between exploration and exploration of gained knowledge about the structure of problem space. We have seen that the results of learning (the ability to help an agent to find a short path to the goal) has improved, but it is also interesting to observe how the average path length behaves during the learning process:\n\n<img src=\"images/lpathlen1.png\"/>\n\nThe learnings can be summarized as:\n\n- **Average path length increases**. What we see here is that at first, the average path length increases. This is probably due to the fact that when we know nothing about the environment, we are likely to get trapped in bad states, water or wolf. As we learn more and start using this knowledge, we can explore the environment for longer, but we still do not know where the apples are very well.\n\n- **Path length decrease, as we learn more**. Once we learn enough, it becomes easier for the agent to achieve the goal, and the path length starts to decrease. However, we are still open to exploration, so we often diverge away from the best path, and explore new options, making the path longer than optimal.\n\n- **Length increase abruptly**. What we also observe on this graph is that at some point, the length increased abruptly. This indicates the stochastic nature of the process, and that we can at some point \"spoil\" the Q-Table coefficients by overwriting them with new values. This ideally should be minimized by decreasing learning rate (for example, towards the end of training, we only adjust Q-Table values by a small value).\n\nOverall, it is important to remember that the success and quality of the learning process significantly depends on parameters, such as learning rate, learning rate decay, and discount factor. Those are often called **hyperparameters**, to distinguish them from **parameters**, which we optimize during training (for example, Q-Table coefficients). The process of finding the best hyperparameter values is called **hyperparameter optimization**, and it deserves a separate topic.\n\n## [Post-lecture quiz](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/46/)\n\n## Assignment \n[A More Realistic World](assignment.md)\n",
              "pdfUrl": "",
              "videoUrl": "",
              "orderIndex": 1
            }
          ],
          "quizzes": [],
          "subModules": []
        },
        {
          "id": 89,
          "title": "Gym",
          "orderIndex": 1,
          "lessons": [
            {
              "id": 90,
              "title": "Gym",
              "content": "# Train Mountain Car\n\n[OpenAI Gym](http://gym.openai.com) has been designed in such a way that all environments provide the same API - i.e. the same methods `reset`, `step` and `render`, and the same abstractions of **action space** and **observation space**. Thus is should be possible to adapt the same reinforcement learning algorithms to different environments with minimal code changes.\n\n## A Mountain Car Environment\n\n[Mountain Car environment](https://gym.openai.com/envs/MountainCar-v0/) contains a car stuck in a valley:\n\n<img src=\"images/mountaincar.png\" width=\"300\"/>\n\nThe goal is to get out of the valley and capture the flag, by doing at each step one of the following actions:\n\n| Value | Meaning |\n|---|---|\n| 0 | Accelerate to the left |\n| 1 | Do not accelerate |\n| 2 | Accelerate to the right |\n\nThe main trick of this problem is, however, that the car's engine is not strong enough to scale the mountain in a single pass. Therefore, the only way to succeed is to drive back and forth to build up momentum.\n\nObservation space consists of just two values:\n\n| Num | Observation  | Min | Max |\n|-----|--------------|-----|-----|\n|  0  | Car Position | -1.2| 0.6 |\n|  1  | Car Velocity | -0.07 | 0.07 |\n\nReward system for the mountain car is rather tricky:\n\n * Reward of 0 is awarded if the agent reached the flag (position = 0.5) on top of the mountain.\n * Reward of -1 is awarded if the position of the agent is less than 0.5.\n\nEpisode terminates if the car position is more than 0.5, or episode length is greater than 200.\n## Instructions\n\nAdapt our reinforcement learning algorithm to solve the mountain car problem. Start with existing [notebook.ipynb](notebook.ipynb) code, substitute new environment, change state discretization functions, and try to make existing algorithm to train with minimal code modifications. Optimize the result by adjusting hyperparameters.\n\n> **Note**: Hyperparameters adjustment is likely to be needed to make algorithm converge. \n## Rubric\n\n| Criteria | Exemplary | Adequate | Needs Improvement |\n| -------- | --------- | -------- | ----------------- |\n|          | Q-Learning algorithm is successfully adapted from CartPole example, with minimal code modifications, which is able to solve the problem of capturing the flag under 200 steps. | A new Q-Learning algorithm has been adopted from the Internet, but is well-documented; or existing algorithm adopted, but does not reach desired results | Student was not able to successfully adopt any algorithm, but has mede substantial steps towards solution (implemented state discretization, Q-Table data structure, etc.) |\n",
              "pdfUrl": "",
              "videoUrl": "",
              "orderIndex": 0
            },
            {
              "id": 91,
              "title": "Gym",
              "content": "# CartPole Skating\n\nThe problem we have been solving in the previous lesson might seem like a toy problem, not really applicable for real life scenarios. This is not the case, because many real world problems also share this scenario - including playing Chess or Go. They are similar, because we also have a board with given rules and a **discrete state**.\n\n## [Pre-lecture quiz](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/47/)\n\n## Introduction\n\nIn this lesson we will apply the same principles of Q-Learning to a problem with **continuous state**, i.e. a state that is given by one or more real numbers. We will deal with the following problem:\n\n> **Problem**: If Peter wants to escape from the wolf, he needs to be able to move faster. We will see how Peter can learn to skate, in particular, to keep balance, using Q-Learning.\n\n![The great escape!](images/escape.png)\n\n> Peter and his friends get creative to escape the wolf! Image by [Jen Looper](https://twitter.com/jenlooper)\n\nWe will use a simplified version of balancing known as a **CartPole** problem. In the cartpole world, we have a horizontal slider that can move left or right, and the goal is to balance a vertical pole on top of the slider.\n\n<img alt=\"a cartpole\" src=\"images/cartpole.png\" width=\"200\"/>\n\n## Prerequisites\n\nIn this lesson, we will be using a library called **OpenAI Gym** to simulate different **environments**. You can run this lesson's code locally (eg. from Visual Studio Code), in which case the simulation will open in a new window. When running the code online, you may need to make some tweaks to the code, as described [here](https://towardsdatascience.com/rendering-openai-gym-envs-on-binder-and-google-colab-536f99391cc7).\n\n## OpenAI Gym\n\nIn the previous lesson, the rules of the game and the state were given by the `Board` class which we defined ourselves. Here we will use a special **simulation environment**, which will simulate the physics behind the balancing pole. One of the most popular simulation environments for training reinforcement learning algorithms is called a [Gym](https://gym.openai.com/), which is maintained by [OpenAI](https://openai.com/). By using this gym we can create difference **environments** from a cartpole simulation to Atari games.\n\n> **Note**: You can see other environments available from OpenAI Gym [here](https://gym.openai.com/envs/#classic_control). \n\nFirst, let's install the gym and import required libraries (code block 1):\n\n```python\nimport sys\n!{sys.executable} -m pip install gym \n\nimport gym\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport random\n```\n\n## Exercise - initialize a cartpole environment\n\nTo work with a cartpole balancing problem, we need to initialize corresponding environment. Each environment is associated with an:\n\n- **Observation space** that defines the structure of information that we receive from the environment. For cartpole problem, we receive position of the pole, velocity and some other values.\n\n- **Action space** that defines possible actions. In our case the action space is discrete, and consists of two actions - **left** and **right**. (code block 2)\n\n1. To initialize, type the following code:\n\n    ```python\n    env = gym.make(\"CartPole-v1\")\n    print(env.action_space)\n    print(env.observation_space)\n    print(env.action_space.sample())\n    ```\n\nTo see how the environment works, let's run a short simulation for 100 steps. At each step, we provide one of the actions to be taken - in this simulation we just randomly select an action from `action_space`. \n\n1. Run the code below and see what it leads to.\n\n    âœ… Remember that it is preferred to run this code on local Python installation! (code block 3)\n\n    ```python\n    env.reset()\n    \n    for i in range(100):\n       env.render()\n       env.step(env.action_space.sample())\n    env.close()\n    ```\n\n    You should be seeing something similar to this image:\n\n    ![non-balancing cartpole](images/cartpole-nobalance.gif)\n\n1. During simulation, we need to get observations in order to decide how to act. In fact, the step function returns current observations, a reward function, and the done flag that indicates whether it makes sense to continue the simulation or not: (code block 4)\n\n    ```python\n    env.reset()\n    \n    done = False\n    while not done:\n       env.render()\n       obs, rew, done, info = env.step(env.action_space.sample())\n       print(f\"{obs} -> {rew}\")\n    env.close()\n    ```\n\n    You will end up seeing something like this in the notebook output:\n\n    ```text\n    [ 0.03403272 -0.24301182  0.02669811  0.2895829 ] -> 1.0\n    [ 0.02917248 -0.04828055  0.03248977  0.00543839] -> 1.0\n    [ 0.02820687  0.14636075  0.03259854 -0.27681916] -> 1.0\n    [ 0.03113408  0.34100283  0.02706215 -0.55904489] -> 1.0\n    [ 0.03795414  0.53573468  0.01588125 -0.84308041] -> 1.0\n    ...\n    [ 0.17299878  0.15868546 -0.20754175 -0.55975453] -> 1.0\n    [ 0.17617249  0.35602306 -0.21873684 -0.90998894] -> 1.0\n    ```\n\n    The observation vector that is returned at each step of the simulation contains the following values:\n    - Position of cart\n    - Velocity of cart\n    - Angle of pole\n    - Rotation rate of pole\n\n1. Get min and max value of those numbers: (code block 5)\n\n    ```python\n    print(env.observation_space.low)\n    print(env.observation_space.high)\n    ```\n\n    You may also notice that reward value on each simulation step is always 1. This is because our goal is to survive as long as possible, i.e. keep the pole to a reasonably vertical position for the longest period of time.\n\n    âœ… In fact, the CartPole simulation is considered solved if we manage to get the average reward of 195 over 100 consecutive trials.\n\n## State discretization\n\nIn Q-Learning, we need to build Q-Table that defines what to do at each state. To be able to do this, we need state to be **discreet**, more precisely, it should contain finite number of discrete values. Thus, we need somehow to **discretize** our observations, mapping them to  a finite set of states.\n\nThere are a few ways we can do this:\n\n- **Divide into bins**. If we know the interval of a certain value, we can divide this interval into a number of **bins**, and then replace the value by the bin number that it belongs to. This can be done using the numpy [`digitize`](https://numpy.org/doc/stable/reference/generated/numpy.digitize.html) method. In this case, we will precisely know the state size, because it will depend on the number of bins we select for digitalization.\n  \nâœ… We can use linear interpolation to bring values to some finite interval (say, from -20 to 20), and then convert numbers to integers by rounding them. This gives us a bit less control on the size of the state, especially if we do not know the exact ranges of input values. For example, in our case 2 out of 4 values do not have upper/lower bounds on their values, which may result in the infinite number of states.\n\nIn our example, we will go with the second approach. As you may notice later, despite undefined upper/lower bounds, those value rarely take values outside of certain finite intervals, thus those states with extreme values will be very rare.\n\n1. Here is the function that will take the observation from our model and produce a tuple of 4 integer values: (code block 6)\n\n    ```python\n    def discretize(x):\n        return tuple((x/np.array([0.25, 0.25, 0.01, 0.1])).astype(np.int))\n    ```\n\n1. Let's also explore another discretization method using bins: (code block 7)\n\n    ```python\n    def create_bins(i,num):\n        return np.arange(num+1)*(i[1]-i[0])/num+i[0]\n    \n    print(\"Sample bins for interval (-5,5) with 10 bins\\n\",create_bins((-5,5),10))\n    \n    ints = [(-5,5),(-2,2),(-0.5,0.5),(-2,2)] # intervals of values for each parameter\n    nbins = [20,20,10,10] # number of bins for each parameter\n    bins = [create_bins(ints[i],nbins[i]) for i in range(4)]\n    \n    def discretize_bins(x):\n        return tuple(np.digitize(x[i],bins[i]) for i in range(4))\n    ```\n\n1. Let's now run a short simulation and observe those discrete environment values. Feel free to try both `discretize` and `discretize_bins` and see if there is a difference.\n\n    âœ… discretize_bins returns the bin number, which is 0-based. Thus for values of input variable around 0 it returns the number from the middle of the interval (10). In discretize, we did not care about the range of output values, allowing them to be negative, thus the state values are not shifted, and 0 corresponds to 0. (code block 8)\n\n    ```python\n    env.reset()\n    \n    done = False\n    while not done:\n       #env.render()\n       obs, rew, done, info = env.step(env.action_space.sample())\n       #print(discretize_bins(obs))\n       print(discretize(obs))\n    env.close()\n    ```\n\n    âœ… Uncomment the line starting with env.render if you want to see how the environment executes. Otherwise you can execute it in the background, which is faster. We will use this \"invisible\" execution during our Q-Learning process.\n\n## The Q-Table structure\n\nIn our previous lesson, the state was a simple pair of numbers from 0 to 8, and thus it was convenient to represent Q-Table by a numpy tensor with a shape of 8x8x2. If we use bins discretization, the size of our state vector is also known, so we can use the same approach and represent state by an array of shape 20x20x10x10x2 (here 2 is the dimension of action space, and first dimensions correspond to the number of bins we have selected to use for each of the parameters in observation space).\n\nHowever, sometimes precise dimensions of the observation space are not known. In case of the `discretize` function, we may never be sure that our state stays within certain limits, because some of the original values are not bound. Thus, we will use a slightly different approach and represent Q-Table by a dictionary. \n\n1. Use the pair *(state,action)* as the dictionary key, and the value would correspond to Q-Table entry value. (code block 9)\n\n    ```python\n    Q = {}\n    actions = (0,1)\n    \n    def qvalues(state):\n        return [Q.get((state,a),0) for a in actions]\n    ```\n\n    Here we also define a function `qvalues()`, which returns a list of Q-Table values for a given state that corresponds to all possible actions. If the entry is not present in the Q-Table, we will return 0 as the default.\n\n## Let's start Q-Learning\n\nNow we are ready to teach Peter to balance!\n\n1. First, let's set some hyperparameters: (code block 10)\n\n    ```python\n    # hyperparameters\n    alpha = 0.3\n    gamma = 0.9\n    epsilon = 0.90\n    ```\n\n    Here, `alpha` is the **learning rate** that defines to which extent we should adjust the current values of Q-Table at each step. In the previous lesson we started with 1, and then decreased `alpha` to lower values during training. In this example we will keep it constant just for simplicity, and you can experiment with adjusting `alpha` values later.\n\n    `gamma` is the **discount factor** that shows to which extent we should prioritize future reward over current reward.\n\n    `epsilon` is the **exploration/exploitation factor** that determines whether we should prefer exploration to exploitation or vice versa. In our algorithm, we will in `epsilon` percent of the cases select the next action according to Q-Table values, and in the remaining number of cases we will execute a random action. This will allow us to explore areas of the search space that we have never seen before. \n\n    âœ… In terms of balancing - choosing random action (exploration) would act as a random punch in the wrong direction, and the pole would have to learn how to recover the balance from those \"mistakes\"\n\n### Improve the algorithm\n\nWe can also make two improvements to our algorithm from the previous lesson:\n\n- **Calculate average cumulative reward**, over a number of simulations. We will print the progress each 5000 iterations, and we will average out our cumulative reward over that period of time. It means that if we get more than 195 point - we can consider the problem solved, with even higher quality than required.\n  \n- **Calculate maximum average cumulative result**, `Qmax`, and we will store the Q-Table corresponding to that result. When you run the training you will notice that sometimes the average cumulative result starts to drop, and we want to keep the values of Q-Table that correspond to the best model observed during training.\n\n1. Collect all cumulative rewards at each simulation at `rewards` vector for further plotting. (code block  11)\n\n    ```python\n    def probs(v,eps=1e-4):\n        v = v-v.min()+eps\n        v = v/v.sum()\n        return v\n    \n    Qmax = 0\n    cum_rewards = []\n    rewards = []\n    for epoch in range(100000):\n        obs = env.reset()\n        done = False\n        cum_reward=0\n        # == do the simulation ==\n        while not done:\n            s = discretize(obs)\n            if random.random()<epsilon:\n                # exploitation - chose the action according to Q-Table probabilities\n                v = probs(np.array(qvalues(s)))\n                a = random.choices(actions,weights=v)[0]\n            else:\n                # exploration - randomly chose the action\n                a = np.random.randint(env.action_space.n)\n    \n            obs, rew, done, info = env.step(a)\n            cum_reward+=rew\n            ns = discretize(obs)\n            Q[(s,a)] = (1 - alpha) * Q.get((s,a),0) + alpha * (rew + gamma * max(qvalues(ns)))\n        cum_rewards.append(cum_reward)\n        rewards.append(cum_reward)\n        # == Periodically print results and calculate average reward ==\n        if epoch%5000==0:\n            print(f\"{epoch}: {np.average(cum_rewards)}, alpha={alpha}, epsilon={epsilon}\")\n            if np.average(cum_rewards) > Qmax:\n                Qmax = np.average(cum_rewards)\n                Qbest = Q\n            cum_rewards=[]\n    ```\n\nWhat you may notice from those results:\n\n- **Close to our goal**. We are very close to achieving the goal of getting 195 cumulative rewards over 100+ consecutive runs of the simulation, or we may have actually achieved it! Even if we get smaller numbers, we still do not know, because we average over 5000 runs, and only 100 runs is required in the formal criteria.\n  \n- **Reward starts to drop**. Sometimes the reward start to drop, which means that we can \"destroy\" already learnt values in the Q-Table with the ones that make the situation worse.\n\nThis observation is more clearly visible if we plot training progress.\n\n## Plotting Training Progress\n\nDuring training, we have collected the cumulative reward value at each of the iterations into `rewards` vector. Here is how it looks when we plot it against the iteration number:\n\n```python\nplt.plot(rewards)\n```\n\n![raw  progress](images/train_progress_raw.png)\n\nFrom this graph, it is not possible to tell anything, because due to the nature of stochastic training process the length of training sessions varies greatly. To make more sense of this graph, we can calculate the **running average** over a series of experiments, let's say 100. This can be done conveniently using `np.convolve`: (code block 12)\n\n```python\ndef running_average(x,window):\n    return np.convolve(x,np.ones(window)/window,mode='valid')\n\nplt.plot(running_average(rewards,100))\n```\n\n![training progress](images/train_progress_runav.png)\n\n## Varying hyperparameters\n\nTo make learning more stable, it makes sense to adjust some of our hyperparameters during training. In particular:\n\n- **For learning rate**, `alpha`, we may start with values close to 1, and then keep decreasing the parameter. With time, we will be getting good probability values in the Q-Table, and thus we should be adjusting them slightly, and not overwriting completely with new values.\n\n- **Increase epsilon**. We may want to increase the `epsilon` slowly, in order to explore less and exploit more. It probably makes sense to start with lower value of `epsilon`, and move up to almost 1.\n\n> **Task 1**: Play with hyperparameter values and see if you can achieve higher cumulative reward. Are you getting above 195?\n\n> **Task 2**: To formally solve the problem, you need to get 195 average reward across 100 consecutive runs. Measure that during training and make sure that you have formally solved the problem!\n\n## Seeing the result in action\n\nIt would be interesting to actually see how the trained model behaves. Let's run the simulation and follow the same action selection strategy as during training, sampling according to the probability distribution in Q-Table: (code block 13)\n\n```python\nobs = env.reset()\ndone = False\nwhile not done:\n   s = discretize(obs)\n   env.render()\n   v = probs(np.array(qvalues(s)))\n   a = random.choices(actions,weights=v)[0]\n   obs,_,done,_ = env.step(a)\nenv.close()\n```\n\nYou should see something like this:\n\n![a balancing cartpole](images/cartpole-balance.gif)\n\n---\n\n## ðŸš€Challenge\n\n> **Task 3**: Here, we were using the final copy of Q-Table, which may not be the best one. Remember that we have stored the best-performing Q-Table into `Qbest` variable! Try the same example with the best-performing Q-Table by copying `Qbest` over to `Q` and see if you notice the difference.\n\n> **Task 4**: Here we were not selecting the best action on each step, but rather sampling with corresponding probability distribution. Would it make more sense to always select the best action, with the highest Q-Table value? This can be done by using `np.argmax` function to find out the action number corresponding to highers Q-Table value. Implement this strategy and see if it improves the balancing.\n\n## [Post-lecture quiz](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/48/)\n\n## Assignment\n[Train a Mountain Car](assignment.md)\n\n## Conclusion\n\nWe have now learned how to train agents to achieve good results just by providing them a reward function that defines the desired state of the game, and by giving them an opportunity to intelligently explore the search space. We have successfully applied the Q-Learning algorithm in the cases of discrete and continuous environments, but with discrete actions.\n\nIt's important to also study situations where action state is also continuous, and when observation space is much more complex, such as the image from the Atari game screen. In those problems we often need to use more powerful machine learning techniques, such as neural networks, in order to achieve good results. Those more advanced topics are the subject of our forthcoming more advanced AI course.\n",
              "pdfUrl": "",
              "videoUrl": "",
              "orderIndex": 1
            }
          ],
          "quizzes": [],
          "subModules": []
        }
      ]
    },
    {
      "id": 92,
      "title": "Real World",
      "orderIndex": 8,
      "lessons": [
        {
          "id": 93,
          "title": "Real World",
          "content": "# Postscript: Real world applications of classic machine learning\n\nIn this section of the curriculum, you will be introduced to some real-world applications of classical ML. We have scoured the internet to find whitepapers and articles about applications that have used these strategies, avoiding neural networks, deep learning and AI as much as possible. Learn about how ML is used in business systems, ecological applications, finance, arts and culture, and more.\n\n![chess](images/chess.jpg)\n\n> Photo by <a href=\"https://unsplash.com/@childeye?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">Alexis Fauvet</a> on <a href=\"https://unsplash.com/s/photos/artificial-intelligence?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">Unsplash</a>\n  \n## Lesson\n\n1. [Real-World Applications for ML](1-Applications/README.md)\n2. [Model Debugging in Machine Learning using Responsible AI dashboard components](2-Debugging-ML-Models/README.md)\n\n## Credits\n\n\"Real-World Applications\" was written by a team of folks, including [Jen Looper](https://twitter.com/jenlooper) and [Ornella Altunyan](https://twitter.com/ornelladotcom).\n\n\"Model Debugging in Machine Learning using Responsible AI dashboard components\" was written by [Ruth Yakubu](https://twitter.com/ruthieyakubu)",
          "pdfUrl": "",
          "videoUrl": "",
          "orderIndex": 0
        }
      ],
      "quizzes": [],
      "subModules": [
        {
          "id": 94,
          "title": "Applications",
          "orderIndex": 0,
          "lessons": [
            {
              "id": 95,
              "title": "Applications",
              "content": "# A ML Scavenger Hunt\n\n## Instructions\n\nIn this lesson, you learned about many real-life use cases that were solved using classical ML. While the use of deep learning, new techniques and tools in AI, and leveraging neural networks has helped speed up the production of tools to help in these sectors, classic ML using the techniques in this curriculum still hold great value.\n\nIn this assignment, imagine that you are participating in a hackathon. Use what you learned in the curriculum to propose a solution using classic ML to solve a problem in one of the sectors discussed in this lesson. Create a presentation where you discuss how you will implement your idea. Bonus points if you can gather sample data and build a ML model to support your concept!\n\n## Rubric\n\n| Criteria | Exemplary                                                           | Adequate                                          | Needs Improvement      |\n| -------- | ------------------------------------------------------------------- | ------------------------------------------------- | ---------------------- |\n|          | A PowerPoint presentation is presented - bonus for building a model | A non-innovative, basic presentation is presented | The work is incomplete |\n",
              "pdfUrl": "",
              "videoUrl": "",
              "orderIndex": 0
            },
            {
              "id": 96,
              "title": "Applications",
              "content": "# Postscript: Machine learning in the real world\n\n\n![Summary of machine learning in the real world in a sketchnote](../../sketchnotes/ml-realworld.png)\n> Sketchnote by [Tomomi Imura](https://www.twitter.com/girlie_mac)\n\nIn this curriculum, you have learned many ways to prepare data for training and create machine learning models. You built a series of classic regression, clustering, classification, natural language processing, and time series models. Congratulations! Now, you might be wondering what it's all for... what are the real world applications for these models?\n\nWhile a lot of interest in industry has been garnered by AI, which usually leverages deep learning, there are still valuable applications for classical machine learning models. You might even use some of these applications today! In this lesson, you'll explore how eight different industries and subject-matter domains use these types of models to make their applications more performant, reliable, intelligent, and valuable to users.\n\n## [Pre-lecture quiz](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/49/)\n\n## ðŸ’° Finance\n\nThe finance sector offers many opportunities for machine learning. Many problems in this area lend themselves to be modeled and solved by using ML.\n\n### Credit card fraud detection\n\nWe learned about [k-means clustering](../../5-Clustering/2-K-Means/README.md) earlier in the course, but how can it be used to solve problems related to credit card fraud?\n\nK-means clustering comes in handy during a credit card fraud detection technique called **outlier detection**. Outliers, or deviations in observations about a set of data, can tell us if a credit card is being used in a normal capacity or if something unusual is going on. As shown in the paper linked below, you can sort credit card data using a k-means clustering algorithm and assign each transaction to a cluster based on how much of an outlier it appears to be. Then, you can evaluate the riskiest clusters for fraudulent versus legitimate transactions.\n[Reference](https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.680.1195&rep=rep1&type=pdf)\n\n### Wealth management\n\nIn wealth management, an individual or firm handles investments on behalf of their clients. Their job is to sustain and grow wealth in the long-term, so it is essential to choose investments that perform well.\n\nOne way to evaluate how a particular investment performs is through statistical regression. [Linear regression](../../2-Regression/1-Tools/README.md) is a valuable tool for understanding how a fund performs relative to some benchmark. We can also deduce whether or not the results of the regression are statistically significant, or how much they would affect a client's investments. You could even further expand your analysis using multiple regression, where additional risk factors can be taken into account. For an example of how this would work for a specific fund, check out the paper below on evaluating fund performance using regression.\n[Reference](http://www.brightwoodventures.com/evaluating-fund-performance-using-regression/)\n\n## ðŸŽ“ Education\n\nThe educational sector is also a very interesting area where ML can be applied. There are interesting problems to be tackled such as detecting cheating on tests or essays or managing bias, unintentional or not, in the correction process.\n\n### Predicting student behavior\n\n[Coursera](https://coursera.com), an online open course provider, has a great tech blog where they discuss many engineering decisions. In this case study, they plotted a regression line to try to explore any correlation between a low NPS (Net Promoter Score) rating and course retention or drop-off.\n[Reference](https://medium.com/coursera-engineering/controlled-regression-quantifying-the-impact-of-course-quality-on-learner-retention-31f956bd592a)\n\n### Mitigating bias\n\n[Grammarly](https://grammarly.com), a writing assistant that checks for spelling and grammar errors, uses sophisticated [natural language processing systems](../../6-NLP/README.md) throughout its products. They published an interesting case study in their tech blog about how they dealt with gender bias in machine learning, which you learned about in our [introductory fairness lesson](../../1-Introduction/3-fairness/README.md).\n[Reference](https://www.grammarly.com/blog/engineering/mitigating-gender-bias-in-autocorrect/)\n\n## ðŸ‘œ Retail\n\nThe retail sector can definitely benefit from the use of ML, with everything from creating a better customer journey to stocking inventory in an optimal way.\n\n### Personalizing the customer journey\n\nAt Wayfair, a company that sells home goods like furniture, helping customers find the right products for their taste and needs is paramount. In this article, engineers from the company describe how they use ML and NLP to \"surface the right results for customers\". Notably, their Query Intent Engine has been built to use entity extraction, classifier training, asset and opinion extraction, and sentiment tagging on customer reviews. This is a classic use case of how NLP works in online retail.\n[Reference](https://www.aboutwayfair.com/tech-innovation/how-we-use-machine-learning-and-natural-language-processing-to-empower-search)\n\n### Inventory management\n\nInnovative, nimble companies like [StitchFix](https://stitchfix.com), a box service that ships clothing to consumers, rely heavily on ML for recommendations and inventory management. Their styling teams work together with their merchandising teams, in fact: \"one of our data scientists tinkered with a genetic algorithm and applied it to apparel to predict what would be a successful piece of clothing that doesn't exist today. We brought that to the merchandise team and now they can use that as a tool.\"\n[Reference](https://www.zdnet.com/article/how-stitch-fix-uses-machine-learning-to-master-the-science-of-styling/)\n\n## ðŸ¥ Health Care\n\nThe health care sector can leverage ML to optimize research tasks and also logistic problems like readmitting patients or stopping diseases from spreading.\n\n### Managing clinical trials\n\nToxicity in clinical trials is a major concern to drug makers. How much toxicity is tolerable? In this study, analyzing various clinical trial methods led to the development of a new approach for predicting the odds of clinical trial outcomes. Specifically, they were able to use random forest to produce a [classifier](../../4-Classification/README.md) that is able to distinguish between groups of drugs.\n[Reference](https://www.sciencedirect.com/science/article/pii/S2451945616302914)\n\n### Hospital readmission management\n\nHospital care is costly, especially when patients have to be readmitted. This paper discusses a company that uses ML to predict readmission potential using [clustering](../../5-Clustering/README.md) algorithms. These clusters help analysts to \"discover groups of readmissions that may share a common cause\".\n[Reference](https://healthmanagement.org/c/healthmanagement/issuearticle/hospital-readmissions-and-machine-learning)\n\n### Disease management\n\nThe recent pandemic has shone a bright light on the ways that machine learning can aid in stopping the spread of disease. In this article, you'll recognize the use of ARIMA, logistic curves, linear regression, and SARIMA. \"This work is an attempt to calculate the rate of spread of this virus and thus to predict the deaths, recoveries, and confirmed cases, so that it may help us to prepare better and survive.\"\n[Reference](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7979218/)\n\n## ðŸŒ² Ecology and Green Tech\n\nNature and ecology consists of many sensitive systems where the interplay between animals and nature come into focus. It's important to be able to measure these systems accurately and act appropriately if something happens, like a forest fire or a drop in the animal population.\n\n### Forest management\n\nYou learned about [Reinforcement Learning](../../8-Reinforcement/README.md) in previous lessons. It can be very useful when trying to predict patterns in nature. In particular, it can be used to track ecological problems like forest fires and the spread of invasive species. In Canada, a group of researchers used Reinforcement Learning to build forest wildfire dynamics models from satellite images. Using an innovative \"spatially spreading process (SSP)\", they envisioned a forest fire as \"the agent at any cell in the landscape.\" \"The set of actions the fire can take from a location at any point in time includes spreading north, south, east, or west or not spreading.\n\nThis approach inverts the usual RL setup since the dynamics of the corresponding Markov Decision Process (MDP) is a known function for immediate wildfire spread.\" Read more about the classic algorithms used by this group at the link below.\n[Reference](https://www.frontiersin.org/articles/10.3389/fict.2018.00006/full)\n\n### Motion sensing of animals\n\nWhile deep learning has created a revolution in visually tracking animal movements (you can build your own [polar bear tracker](https://docs.microsoft.com/learn/modules/build-ml-model-with-azure-stream-analytics/?WT.mc_id=academic-77952-leestott) here), classic ML still has a place in this task.\n\nSensors to track movements of farm animals and IoT make use of this type of visual processing, but more basic ML techniques are useful to preprocess data. For example, in this paper, sheep postures were monitored and analyzed using various classifier algorithms. You might recognize the ROC curve on page 335.\n[Reference](https://druckhaus-hofmann.de/gallery/31-wj-feb-2020.pdf)\n\n### âš¡ï¸ Energy Management\n  \nIn our lessons on [time series forecasting](../../7-TimeSeries/README.md), we invoked the concept of smart parking meters to generate revenue for a town based on understanding supply and demand. This article discusses in detail how clustering, regression and time series forecasting combined to help predict future energy use in Ireland, based off of smart metering.\n[Reference](https://www-cdn.knime.com/sites/default/files/inline-images/knime_bigdata_energy_timeseries_whitepaper.pdf)\n\n## ðŸ’¼ Insurance\n\nThe insurance sector is another sector that uses ML to construct and optimize viable financial and actuarial models. \n\n### Volatility Management\n\nMetLife, a life insurance provider, is forthcoming with the way they analyze and mitigate volatility in their financial models. In this article you'll notice binary and ordinal classification visualizations. You'll also discover forecasting visualizations.\n[Reference](https://investments.metlife.com/content/dam/metlifecom/us/investments/insights/research-topics/macro-strategy/pdf/MetLifeInvestmentManagement_MachineLearnedRanking_070920.pdf)\n\n## ðŸŽ¨ Arts, Culture, and Literature\n\nIn the arts, for example in journalism, there are many interesting problems. Detecting fake news is a huge problem as it has been proven to influence the opinion of people and even to topple democracies. Museums can also benefit from using ML in everything from finding links between artifacts to resource planning.\n\n### Fake news detection\n\nDetecting fake news has become a game of cat and mouse in today's media. In this article, researchers suggest that a system combining several of the ML techniques we have studied can be tested and the best model deployed: \"This system is based on natural language processing to extract features from the data and then these features are used for the training of machine learning classifiers such as Naive Bayes,  Support Vector Machine (SVM), Random Forest (RF), Stochastic Gradient Descent (SGD), and Logistic Regression(LR).\"\n[Reference](https://www.irjet.net/archives/V7/i6/IRJET-V7I6688.pdf)\n\nThis article shows how combining different ML domains can produce interesting results that can help stop fake news from spreading and creating real damage; in this case, the impetus was the spread of rumors about COVID treatments that incited mob violence.\n\n### Museum ML\n\nMuseums are at the cusp of an AI revolution in which cataloging and digitizing collections and finding links between artifacts is becoming easier as technology advances. Projects such as [In Codice Ratio](https://www.sciencedirect.com/science/article/abs/pii/S0306457321001035#:~:text=1.,studies%20over%20large%20historical%20sources.) are helping unlock the mysteries of inaccessible collections such as the Vatican Archives. But, the business aspect of museums benefits from ML models as well.\n\nFor example, the Art Institute of Chicago built models to predict what audiences are interested in and when they will attend expositions. The goal is to create individualized and optimized visitor experiences each time the user visits the museum. \"During fiscal 2017, the model predicted attendance and admissions within 1 percent of accuracy, says Andrew Simnick, senior vice president at the Art Institute.\"\n[Reference](https://www.chicagobusiness.com/article/20180518/ISSUE01/180519840/art-institute-of-chicago-uses-data-to-make-exhibit-choices)\n\n## ðŸ· Marketing\n\n### Customer segmentation\n\nThe most effective marketing strategies target customers in different ways based on various groupings. In this article, the uses of Clustering algorithms are discussed to support differentiated marketing. Differentiated marketing helps companies improve brand recognition, reach more customers, and make more money.\n[Reference](https://ai.inqline.com/machine-learning-for-marketing-customer-segmentation/)\n\n## ðŸš€ Challenge\n\nIdentify another sector that benefits from some of the techniques you learned in this curriculum, and discover how it uses ML.\n\n## [Post-lecture quiz](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/50/)\n\n## Review & Self Study\n\nThe Wayfair data science team has several interesting videos on how they use ML at their company. It's worth [taking a look](https://www.youtube.com/channel/UCe2PjkQXqOuwkW1gw6Ameuw/videos)!\n\n## Assignment\n\n[A ML scavenger hunt](assignment.md)\n",
              "pdfUrl": "",
              "videoUrl": "",
              "orderIndex": 1
            }
          ],
          "quizzes": [],
          "subModules": []
        },
        {
          "id": 97,
          "title": "Debugging ML Models",
          "orderIndex": 1,
          "lessons": [
            {
              "id": 98,
              "title": "Debugging ML Models",
              "content": "# Explore Responsible AI (RAI) dashboard\n\n## Instructions\n\nIn this lesson you learned about the RAI dashboard, a suite of components built on \"open-source\" tools  to help data scientists perform error analysis, data exploration, fairness assessment, model interpretability, counterfact/what-if assesments and causal analysis on AI systems.\" For this assignment, explore some of RAI dashboard's sample [notebooks](https://github.com/Azure/RAI-vNext-Preview/tree/main/examples/notebooks) and report your findings in a paper or presentation.\n\n## Rubric\n\n| Criteria | Exemplary | Adequate | Needs Improvement |\n| -------- | --------- | -------- | ----------------- |\n|          |  A paper or powerpoint presentation is presented discussing RAI dashboard's components, the notebook that was run, and the conclusions drawn from running it        |   A paper is presented without conclusions       |  No paper is presented                 |\n",
              "pdfUrl": "",
              "videoUrl": "",
              "orderIndex": 0
            },
            {
              "id": 99,
              "title": "Debugging ML Models",
              "content": "# Postscript: Model Debugging in Machine Learning using Responsible AI dashboard components\n \n\n## [Pre-lecture quiz](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/5/)\n \n## Introduction\n\nMachine learning impacts our everyday lives. AI is finding its way into some of the most important systems that affect us as individuals as well as our society, from healthcare, finance, education, and employment. For instance, systems and models are involved in daily decision-making tasks, such as health care diagnoses or detecting fraud. Consequentially, the advancements in AI along with the accelerated adoption are being met with evolving societal expectations and growing regulation in response. We constantly see areas where AI systems continue to miss expectations; they expose new challenges; and governments are starting to regulate AI solutions. So, it is important that these models are analyzed to provide fair, reliable, inclusive, transparent, and accountable outcomes for everyone.\n\nIn this curriculum, we will look at practical tools that can be used to assess if a model has responsible AI issues. Traditional machine learning debugging techniques tend to be based on quantitative calculations such as aggregated accuracy or average error loss. Imagine what can happen when the data you are using to build these models lacks certain demographics, such as race, gender, political view, religion, or disproportionally represents such demographics. What about when the model's output is interpreted to favor some demographic? This can introduce an over or under representation of these sensitive feature groups resulting in fairness, inclusiveness, or reliability issues from the model. Another factor is, machine learning models are considered black boxes, which makes it hard to understand and explain what drives a modelâ€™s prediction. All of these are challenges data scientists and AI developers face when they do not have adequate tools to debug and assess the fairness or trustworthiness of a model.\n\nIn this lesson, you will learn about debugging your models using:\n\n-\t**Error Analysis**: identify where in your data distribution the model has high error rates.\n-\t**Model Overview**: perform comparative analysis across different data cohorts to discover disparities in your modelâ€™s performance metrics.\n-\t**Data Analysis**: investigate where there could be over or under representation of your data that can skew your model to favor one data demographic vs another.\n-\t**Feature Importance**: understand which features are driving your modelâ€™s predictions on a global level or local level.\n\n## Prerequisite\n\nAs a prerequisite, please take the review [Responsible AI tools for developers](https://www.microsoft.com/ai/ai-lab-responsible-ai-dashboard)\n\n> ![Gif on Responsible AI Tools](./images/rai-overview.gif)\n\n## Error Analysis\n\nTraditional model performance metrics used for measuring accuracy are mostly calculations based on correct vs incorrect predictions. For example, determining that a model is accurate 89% of time with an error loss of 0.001 can be considered a good performance. Errors are often not distributed uniformly in your underlying dataset. You may get an 89% model accuracy score but discover that there are different regions of your data for which the model is failing 42% of the time. The consequence of these failure patterns with certain data groups can lead to fairness or reliability issues. It is essential to understand areas where the model is performing well or not. The data regions where there are a high number of inaccuracies in your model may turn out to be an important data demographic.  \n\n![Analyze and debug model errors](./images/ea-error-distribution.png)\n\nThe Error Analysis component on the RAI dashboard illustrates how model failure is distributed across various cohorts with a tree visualization. This is useful in identifying features or areas where there is a high error rate with your dataset. By seeing where most of the modelâ€™s inaccuracies are coming from, you can start investigating the root cause. You can also create cohorts of data to perform analysis on. These data cohorts help in the debugging process to determine why the model performance is good in one cohort, but erroneous in another.   \n\n![Error Analysis](./images/ea-error-cohort.png)\n\nThe visual indicators on the tree map help in locating the problem areas quicker. For instance, the darker shade of red color a tree node has, the higher the error rate.  \n\nHeat map is another visualization functionality that users can use in investigating the error rate using one or two features to find a contributor to the model errors across an entire dataset or cohorts.\n\n![Error Analysis Heatmap](./images/ea-heatmap.png)\n\nUse error analysis when you need to:\n\n* Gain a deep understanding of how model failures are distributed across a dataset and across several input and feature dimensions.\n* Break down the aggregate performance metrics to automatically discover erroneous cohorts to inform your targeted mitigation steps.\n\n## Model Overview\n\nEvaluating the performance of a machine learning model requires getting a holistic understanding of its behavior. This can be achieved by reviewing more than one metric such as error rate, accuracy, recall, precision, or MAE (Mean Absolute Err) to find disparities among performance metrics.  One performance metric may look great, but inaccuracies can be exposed in another metric. In addition, comparing the metrics for disparities across the entire dataset or cohorts helps shed light on where the model is performing well or not. This is especially important in seeing the modelâ€™s performance among sensitive vs insensitive features (e.g., patient race, gender, or age) to uncover potential unfairness the model may have. For example, discovering that the model is more erroneous in a cohort that has sensitive features can reveal potential unfairness the model may have.\n\nThe Model Overview component of the RAI dashboard helps not just in analyzing the performance metrics of the data representation in a cohort, but it gives users the ability to compare the modelâ€™s behavior across different cohorts.\n\n![Dataset cohorts - model overview in RAI dashboard](./images/model-overview-dataset-cohorts.png)\n\nThe component's feature-based analysis functionality allows users to narrow down data subgroups within a particular feature to identify anomalies on a granular level. For example, the dashboard has built-in intelligence to automatically generate cohorts for a user-selected feature (eg., *\"time_in_hospital < 3\"* or *\"time_in_hospital >= 7\"*). This enables a user to isolate a particular feature from a larger data group to see if it is a key influencer of the model's erroneous outcomes.\n\n![Feature cohorts - model overview in RAI dashboard](./images/model-overview-feature-cohorts.png)\n\nThe Model Overview component supports two classes of disparity metrics:\n\n**Disparity in model performance**: These sets of metrics calculate the disparity (difference) in the values of the selected performance metric across subgroups of data. Here are a few examples:\n\n* Disparity in accuracy rate\n* Disparity in error rate\n* Disparity in precision\n* Disparity in recall\n* Disparity in mean absolute error (MAE)\n\n**Disparity in selection rate**: This metric contains the difference in selection rate (favorable prediction) among subgroups. An example of this is the disparity in loan approval rates. Selection rate means the fraction of data points in each class classified as 1 (in binary classification) or distribution of prediction values (in regression).\n\n## Data Analysis\n\n> \"If you torture the data long enough, it will confess to anything\" - Ronald Coase\n\nThis statement sounds extreme, but it is true that data can be manipulated to support any conclusion. Such manipulation can sometimes happen unintentionally. As humans, we all have bias, and it is often difficult to consciously know when you are introducing bias in data. Guaranteeing fairness in AI and machine learning remains a complex challenge. \n\nData is a huge blind spot for traditional model performance metrics. You may have high accuracy scores, but this does not always reflect the underlining data bias that could be in your dataset. For example, if a dataset of employees has 27% of women in executive positions in a company and 73% of men at the same level, a job advertising AI model trained on this data may target mostly a male audience for senior level job positions. Having this imbalance in data skewed the modelâ€™s prediction to favor one gender. This reveals a fairness issue where there is a gender bias in the AI model.  \n\nThe Data Analysis component on the RAI dashboard helps to identify areas where thereâ€™s an over- and under-representation in the dataset. It helps users diagnose the root cause of errors and fairness issues introduced from data imbalances or lack of representation of a particular data group. This gives users the ability to visualize datasets based on predicted and actual outcomes, error groups, and specific features. Sometimes discovering an underrepresented data group can also uncover that the model is not learning well, hence the high inaccuracies. Having a model that has data bias is not just a fairness issue but shows that the model is not inclusive or reliable.\n\n![Data Analysis component on RAI Dashboard](./images/dataanalysis-cover.png)\n\n\nUse data analysis when you need to:\n\n* Explore your dataset statistics by selecting different filters to slice your data into different dimensions (also known as cohorts).\n* Understand the distribution of your dataset across different cohorts and feature groups.\n* Determine whether your findings related to fairness, error analysis, and causality (derived from other dashboard components) are a result of your dataset's distribution.\n* Decide in which areas to collect more data to mitigate errors that come from representation issues, label noise, feature noise, label bias, and similar factors.\n\n## Model Interpretability\n\nMachine learning models tend to be black boxes. Understanding which key data features drive a modelâ€™s prediction can be challenging.  It is important to provide transparency as to why a model makes a certain prediction. For example, if an AI system predicts that a diabetic patient is at risk of being readmitted back to a hospital in less than 30 days, it should be able to provide supporting data that led to its prediction. Having supporting data indicators brings transparency to help clinicians or hospitals to be able to make well-informed decisions. In addition, being able to explain why a model made a prediction for an individual patient enables accountability with health regulations. When you are using machine learning models in ways that affect peopleâ€™s lives, it is crucial to understand and explain what influences the behavior of a model. Model explainability and interpretability helps answer questions in scenarios such as:\n\n* Model debugging: Why did my model make this mistake? How can I improve my model?\n* Human-AI collaboration: How can I understand and trust the modelâ€™s decisions?\n* Regulatory compliance: Does my model satisfy legal requirements?\n\nThe Feature Importance component of the RAI dashboard helps you to debug and get a comprehensive understanding of how a model makes predictions. It is also a useful tool for machine learning professionals and decision-makers to explain and show evidence of features influencing a model's behavior for regulatory compliance. Next, users can explore both global and local explanations validate which features drive a modelâ€™s prediction. Global explanations lists the top features that affected a modelâ€™s overall prediction. Local explanations display which features led to a modelâ€™s prediction for an individual case. The ability to evaluate local explanations is also helpful in debugging or auditing a specific case to better understand and interpret why a model made an accurate or inaccurate prediction. \n\n![Feature Importance component of the RAI dashboard](./images/9-feature-importance.png)\n\n* Global explanations: For example, what features affect the overall behavior of a diabetes hospital readmission model?\n* Local explanations: For example, why was a diabetic patient over 60 years old with prior hospitalizations predicted to be readmitted or not readmitted within 30 days back to a hospital?\n\nIn the debugging process of examining a modelâ€™s performance across different cohorts, Feature Importance shows what level of impact a feature has across the cohorts. It helps reveal anomalies when comparing the level of influence the feature has in driving a modelâ€™s erroneous predictions. The Feature Importance component can show which values in a feature positively or negatively influenced the modelâ€™s outcome. For instance, if a model made an inaccurate prediction, the component gives you the ability to drill down and pinpoint what features or feature values drove the prediction. This level of detail helps not just in debugging but provides transparency and accountability in auditing situations. Finally, the component can help you to identify fairness issues. To illustrate, if a sensitive feature such as ethnicity or gender is highly influential in driving a modelâ€™s prediction, this could be a sign of race or gender bias in the model.\n\n![Feature importance](./images/9-features-influence.png)\n\nUse interpretability when you need to:\n\n* Determine how trustworthy your AI systemâ€™s predictions are by understanding what features are most important for the predictions.\n* Approach the debugging of your model by understanding it first and identifying whether the model is using healthy features or merely false correlations.\n* Uncover potential sources of unfairness by understanding whether the model is basing predictions on sensitive features or on features that are highly correlated with them.\n* Build user trust in your modelâ€™s decisions by generating local explanations to illustrate their outcomes.\n* Complete a regulatory audit of an AI system to validate models and monitor the impact of model decisions on humans.\n\n## Conclusion\n\nAll the RAI dashboard components are practical tools to help you build machine learning models that are less harmful and more trustworthy to society. It improves the prevention of treats to human rights; discriminating or excluding certain groups to life opportunities; and the risk of physical or psychological injury. It also helps to build trust in your modelâ€™s decisions by generating local explanations to illustrate their outcomes. Some of the potential harms can be classified as:\n\n- **Allocation**, if a gender or ethnicity for example is favored over another.\n- **Quality of service**. If you train the data for one specific scenario but the reality is much more complex, it leads to a poor performing service.\n- **Stereotyping**. Associating a given group with pre-assigned attributes.\n- **Denigration**. To unfairly criticize and label something or someone.\n- **Over- or under- representation**. The idea is that a certain group is not seen in a certain profession, and any service or function that keeps promoting that is contributing to harm.\n\n### Azure RAI dashboard\n \n[Azure RAI dashboard](https://learn.microsoft.com/en-us/azure/machine-learning/concept-responsible-ai-dashboard?WT.mc_id=aiml-90525-ruyakubu) is built on open-source tools developed by the leading academic institutions and organizations including Microsoft are instrumental for data scientists and AI developers to better understand model behavior, discover and mitigate undesirable issues from AI models.\n\n- Learn how to use the different components by checking out the RAI dashboard [docs.](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-responsible-ai-dashboard?WT.mc_id=aiml-90525-ruyakubu)\n\n- Check out some RAI dashboard [sample notebooks](https://github.com/Azure/RAI-vNext-Preview/tree/main/examples/notebooks) for debugging more responsible AI scenarios in Azure Machine Learning. \n  \n---\n## ðŸš€ Challenge \n \nTo prevent statistical or data biases from being introduced in the first place, we should: \n\n- have a diversity of backgrounds and perspectives among the people working on systems \n- invest in datasets that reflect the diversity of our society \n- develop better methods for detecting and correcting bias when it occurs \n\nThink about real-life scenarios where unfairness is evident in model-building and usage. What else should we consider? \n\n## [Post-lecture quiz](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/6/)\n## Review & Self Study \n \nIn this lesson, you have learned some of the practical tools of incorporating responsible AI in machine learning.  \n\nWatch this workshop to dive deeper into the topics: \n\n- Responsible AI Dashboard: One-stop shop for operationalizing RAI in practice by Besmira Nushi and Mehrnoosh Sameki\n\n[![Responsible AI Dashboard: One-stop shop for operationalizing RAI in practice](https://img.youtube.com/vi/f1oaDNl3djg/0.jpg)](https://www.youtube.com/watch?v=f1oaDNl3djg \"Responsible AI Dashboard: One-stop shop for operationalizing RAI in practice\")\n\n> ðŸŽ¥ Click the image above for a video: Responsible AI Dashboard: One-stop shop for operationalizing RAI in practice by Besmira Nushi and Mehrnoosh Sameki\n \nReference the following materials to learn more about responsible AI and how to build more trustworthy models: \n\n- Microsoftâ€™s RAI dashboard tools for debugging ML models: [Responsible AI tools resources](https://aka.ms/rai-dashboard)\n\n- Explore the Responsible AI toolkit: [Github](https://github.com/microsoft/responsible-ai-toolbox)\n\n- Microsoftâ€™s RAI resource center: [Responsible AI Resources â€“ Microsoft AI](https://www.microsoft.com/ai/responsible-ai-resources?activetab=pivot1%3aprimaryr4) \n\n- Microsoftâ€™s FATE research group: [FATE: Fairness, Accountability, Transparency, and Ethics in AI - Microsoft Research](https://www.microsoft.com/research/theme/fate/) \n\n## Assignment\n\n[Explore RAI dashboard](assignment.md) \n",
              "pdfUrl": "",
              "videoUrl": "",
              "orderIndex": 1
            }
          ],
          "quizzes": [],
          "subModules": []
        }
      ]
    }
  ]
}